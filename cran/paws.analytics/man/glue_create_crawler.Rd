% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/glue_operations.R
\name{glue_create_crawler}
\alias{glue_create_crawler}
\title{Creates a new crawler with specified targets, role, configuration, and
optional schedule}
\usage{
glue_create_crawler(Name, Role, DatabaseName, Description, Targets,
  Schedule, Classifiers, TablePrefix, SchemaChangePolicy, RecrawlPolicy,
  LineageConfiguration, Configuration, CrawlerSecurityConfiguration, Tags)
}
\arguments{
\item{Name}{[required] Name of the new crawler.}

\item{Role}{[required] The IAM role or Amazon Resource Name (ARN) of an IAM role used by the
new crawler to access customer resources.}

\item{DatabaseName}{The AWS Glue database where results are written, such as:
\verb{arn:aws:daylight:us-east-1::database/sometable/*}.}

\item{Description}{A description of the new crawler.}

\item{Targets}{[required] A list of collection of targets to crawl.}

\item{Schedule}{A \code{cron} expression used to specify the schedule (see \href{https://docs.aws.amazon.com/glue/latest/dg/monitor-data-warehouse-schedule.html}{Time-Based Schedules for Jobs and Crawlers}.
For example, to run something every day at 12:15 UTC, you would specify:
\verb{cron(15 12 * * ? *)}.}

\item{Classifiers}{A list of custom classifiers that the user has registered. By default,
all built-in classifiers are included in a crawl, but these custom
classifiers always override the default classifiers for a given
classification.}

\item{TablePrefix}{The table prefix used for catalog tables that are created.}

\item{SchemaChangePolicy}{The policy for the crawler's update and deletion behavior.}

\item{RecrawlPolicy}{A policy that specifies whether to crawl the entire dataset again, or to
crawl only folders that were added since the last crawler run.}

\item{LineageConfiguration}{Specifies data lineage configuration settings for the crawler.}

\item{Configuration}{Crawler configuration information. This versioned JSON string allows
users to specify aspects of a crawler's behavior. For more information,
see \href{https://docs.aws.amazon.com/glue/latest/dg/crawler-configuration.html}{Configuring a Crawler}.}

\item{CrawlerSecurityConfiguration}{The name of the \code{SecurityConfiguration} structure to be used by this
crawler.}

\item{Tags}{The tags to use with this crawler request. You may use tags to limit
access to the crawler. For more information about tags in AWS Glue, see
\href{https://docs.aws.amazon.com/glue/latest/dg/monitor-tags.html}{AWS Tags in AWS Glue} in
the developer guide.}
}
\value{
An empty list.
}
\description{
Creates a new crawler with specified targets, role, configuration, and
optional schedule. At least one crawl target must be specified, in the
\code{s3Targets} field, the \code{jdbcTargets} field, or the \code{DynamoDBTargets}
field.
}
\section{Request syntax}{
\preformatted{svc$create_crawler(
  Name = "string",
  Role = "string",
  DatabaseName = "string",
  Description = "string",
  Targets = list(
    S3Targets = list(
      list(
        Path = "string",
        Exclusions = list(
          "string"
        ),
        ConnectionName = "string"
      )
    ),
    JdbcTargets = list(
      list(
        ConnectionName = "string",
        Path = "string",
        Exclusions = list(
          "string"
        )
      )
    ),
    MongoDBTargets = list(
      list(
        ConnectionName = "string",
        Path = "string",
        ScanAll = TRUE|FALSE
      )
    ),
    DynamoDBTargets = list(
      list(
        Path = "string",
        scanAll = TRUE|FALSE,
        scanRate = 123.0
      )
    ),
    CatalogTargets = list(
      list(
        DatabaseName = "string",
        Tables = list(
          "string"
        )
      )
    )
  ),
  Schedule = "string",
  Classifiers = list(
    "string"
  ),
  TablePrefix = "string",
  SchemaChangePolicy = list(
    UpdateBehavior = "LOG"|"UPDATE_IN_DATABASE",
    DeleteBehavior = "LOG"|"DELETE_FROM_DATABASE"|"DEPRECATE_IN_DATABASE"
  ),
  RecrawlPolicy = list(
    RecrawlBehavior = "CRAWL_EVERYTHING"|"CRAWL_NEW_FOLDERS_ONLY"
  ),
  LineageConfiguration = list(
    CrawlerLineageSettings = "ENABLE"|"DISABLE"
  ),
  Configuration = "string",
  CrawlerSecurityConfiguration = "string",
  Tags = list(
    "string"
  )
)
}
}

\keyword{internal}
