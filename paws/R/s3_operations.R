# This file is generated by make.paws. Please do not edit here.
#' @importFrom paws.common get_config new_operation new_request send_request
#' @include s3_service.R
NULL

#' This operation aborts a multipart upload
#'
#' @description
#' This operation aborts a multipart upload. After a multipart upload is
#' aborted, no additional parts can be uploaded using that upload ID. The
#' storage consumed by any previously uploaded parts will be freed.
#' However, if any part uploads are currently in progress, those part
#' uploads might or might not succeed. As a result, it might be necessary
#' to abort a given multipart upload multiple times in order to completely
#' free all storage consumed by all parts.
#' 
#' To verify that all parts have been removed and prevent getting charged
#' for the part storage, you should call the [`list_parts`][s3_list_parts]
#' API operation and ensure that the parts list is empty.
#' 
#' -   **Directory buckets** - If multipart uploads in a directory bucket
#'     are in progress, you can't delete the bucket until all the
#'     in-progress multipart uploads are aborted or completed. To delete
#'     these in-progress multipart uploads, use the
#'     [`list_multipart_uploads`][s3_list_multipart_uploads] operation to
#'     list the in-progress multipart uploads in the bucket and use the
#'     [`abort_multipart_upload`][s3_abort_multipart_upload] operation to
#'     abort all the in-progress multipart uploads.
#' 
#' -   **Directory buckets** - For directory buckets, you must make
#'     requests for this API operation to the Zonal endpoint. These
#'     endpoints support virtual-hosted-style requests in the format
#'     `https://amzn-s3-demo-bucket.s3express-zone-id.region-code.amazonaws.com/key-name `.
#'     Path-style requests are not supported. For more information about
#'     endpoints in Availability Zones, see [Regional and Zonal endpoints
#'     for directory buckets in Availability
#'     Zones](https://docs.aws.amazon.com/AmazonS3/latest/userguide/endpoint-directory-buckets-AZ.html)
#'     in the *Amazon S3 User Guide*. For more information about endpoints
#'     in Local Zones, see [Concepts for directory buckets in Local
#'     Zones](https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-lzs-for-directory-buckets.html)
#'     in the *Amazon S3 User Guide*.
#' 
#' ### Permissions
#' 
#' -   **General purpose bucket permissions** - For information about
#'     permissions required to use the multipart upload, see [Multipart
#'     Upload and
#'     Permissions](https://docs.aws.amazon.com/AmazonS3/latest/userguide/mpuoverview.html)
#'     in the *Amazon S3 User Guide*.
#' 
#' -   **Directory bucket permissions** - To grant access to this API
#'     operation on a directory bucket, we recommend that you use the
#'     [`create_session`](https://docs.aws.amazon.com/AmazonS3/latest/API/API_CreateSession.html)
#'     API operation for session-based authorization. Specifically, you
#'     grant the `s3express:CreateSession` permission to the directory
#'     bucket in a bucket policy or an IAM identity-based policy. Then, you
#'     make the [`create_session`][s3_create_session] API call on the
#'     bucket to obtain a session token. With the session token in your
#'     request header, you can make API requests to this operation. After
#'     the session token expires, you make another
#'     [`create_session`][s3_create_session] API call to generate a new
#'     session token for use. Amazon Web Services CLI or SDKs create
#'     session and refresh the session token automatically to avoid service
#'     interruptions when a session expires. For more information about
#'     authorization, see
#'     [`create_session`](https://docs.aws.amazon.com/AmazonS3/latest/API/API_CreateSession.html)
#'     .
#' 
#' ### HTTP Host header syntax
#' 
#' **Directory buckets** - The HTTP Host header syntax is
#' ` Bucket-name.s3express-zone-id.region-code.amazonaws.com`.
#' 
#' The following operations are related to
#' [`abort_multipart_upload`][s3_abort_multipart_upload]:
#' 
#' -   [`create_multipart_upload`][s3_create_multipart_upload]
#' 
#' -   [`upload_part`][s3_upload_part]
#' 
#' -   [`complete_multipart_upload`][s3_complete_multipart_upload]
#' 
#' -   [`list_parts`][s3_list_parts]
#' 
#' -   [`list_multipart_uploads`][s3_list_multipart_uploads]
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#'
#' @usage
#' s3_abort_multipart_upload(Bucket, Key, UploadId, RequestPayer,
#'   ExpectedBucketOwner, IfMatchInitiatedTime)
#'
#' @param Bucket &#91;required&#93; The bucket name to which the upload was taking place.
#' 
#' **Directory buckets** - When you use this operation with a directory
#' bucket, you must use virtual-hosted-style requests in the format
#' ` Bucket-name.s3express-zone-id.region-code.amazonaws.com`. Path-style
#' requests are not supported. Directory bucket names must be unique in the
#' chosen Zone (Availability Zone or Local Zone). Bucket names must follow
#' the format ` bucket-base-name--zone-id--x-s3` (for example,
#' ` amzn-s3-demo-bucket--usw2-az1--x-s3`). For information about bucket
#' naming restrictions, see [Directory bucket naming
#' rules](https://docs.aws.amazon.com/AmazonS3/latest/userguide/directory-bucket-naming-rules.html)
#' in the *Amazon S3 User Guide*.
#' 
#' **Access points** - When you use this action with an access point for
#' general purpose buckets, you must provide the alias of the access point
#' in place of the bucket name or specify the access point ARN. When you
#' use this action with an access point for directory buckets, you must
#' provide the access point name in place of the bucket name. When using
#' the access point ARN, you must direct requests to the access point
#' hostname. The access point hostname takes the form
#' *AccessPointName*-*AccountId*.s3-accesspoint.*Region*.amazonaws.com.
#' When using this action with an access point through the Amazon Web
#' Services SDKs, you provide the access point ARN in place of the bucket
#' name. For more information about access point ARNs, see [Using access
#' points](https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-access-points.html)
#' in the *Amazon S3 User Guide*.
#' 
#' Object Lambda access points are not supported by directory buckets.
#' 
#' **S3 on Outposts** - When you use this action with S3 on Outposts, you
#' must direct requests to the S3 on Outposts hostname. The S3 on Outposts
#' hostname takes the form
#' ` AccessPointName-AccountId.outpostID.s3-outposts.Region.amazonaws.com`.
#' When you use this action with S3 on Outposts, the destination bucket
#' must be the Outposts access point ARN or the access point alias. For
#' more information about S3 on Outposts, see [What is S3 on
#' Outposts?](https://docs.aws.amazon.com/AmazonS3/latest/s3-outposts/S3onOutposts.html)
#' in the *Amazon S3 User Guide*.
#' @param Key &#91;required&#93; Key of the object for which the multipart upload was initiated.
#' @param UploadId &#91;required&#93; Upload ID that identifies the multipart upload.
#' @param RequestPayer 
#' @param ExpectedBucketOwner The account ID of the expected bucket owner. If the account ID that you
#' provide does not match the actual owner of the bucket, the request fails
#' with the HTTP status code `403 Forbidden` (access denied).
#' @param IfMatchInitiatedTime If present, this header aborts an in progress multipart upload only if
#' it was initiated on the provided timestamp. If the initiated timestamp
#' of the multipart upload does not match the provided value, the operation
#' returns a `412 Precondition Failed` error. If the initiated timestamp
#' matches or if the multipart upload doesn’t exist, the operation returns
#' a `204 Success (No Content)` response.
#' 
#' This functionality is only supported for directory buckets.
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   RequestCharged = "requester"
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$abort_multipart_upload(
#'   Bucket = "string",
#'   Key = "string",
#'   UploadId = "string",
#'   RequestPayer = "requester",
#'   ExpectedBucketOwner = "string",
#'   IfMatchInitiatedTime = as.POSIXct(
#'     "2015-01-01"
#'   )
#' )
#' ```
#'
#' @examples
#' \dontrun{
#' # The following example aborts a multipart upload.
#' svc$abort_multipart_upload(
#'   Bucket = "examplebucket",
#'   Key = "bigobject",
#'   UploadId = "xadcOB_7YPBOJuoFiQ9cz4P3Pe6FIZwO4f7wN93uHsNBEw97pl5eNwzExg0LA..."
#' )
#' }
#'
#' @keywords internal
#'
#' @rdname s3_abort_multipart_upload
#'
#' @aliases s3_abort_multipart_upload
s3_abort_multipart_upload <- function(Bucket, Key, UploadId, RequestPayer = NULL, ExpectedBucketOwner = NULL, IfMatchInitiatedTime = NULL) {
  op <- new_operation(
    name = "AbortMultipartUpload",
    http_method = "DELETE",
    http_path = "/{Bucket}/{Key+}",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .s3$abort_multipart_upload_input(Bucket = Bucket, Key = Key, UploadId = UploadId, RequestPayer = RequestPayer, ExpectedBucketOwner = ExpectedBucketOwner, IfMatchInitiatedTime = IfMatchInitiatedTime)
  output <- .s3$abort_multipart_upload_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$abort_multipart_upload <- s3_abort_multipart_upload

#' Completes a multipart upload by assembling previously uploaded parts
#'
#' @description
#' Completes a multipart upload by assembling previously uploaded parts.
#' 
#' You first initiate the multipart upload and then upload all parts using
#' the [`upload_part`][s3_upload_part] operation or the
#' [`upload_part_copy`][s3_upload_part_copy] operation. After successfully
#' uploading all relevant parts of an upload, you call this
#' [`complete_multipart_upload`][s3_complete_multipart_upload] operation to
#' complete the upload. Upon receiving this request, Amazon S3 concatenates
#' all the parts in ascending order by part number to create a new object.
#' In the CompleteMultipartUpload request, you must provide the parts list
#' and ensure that the parts list is complete. The CompleteMultipartUpload
#' API operation concatenates the parts that you provide in the list. For
#' each part in the list, you must provide the `PartNumber` value and the
#' `ETag` value that are returned after that part was uploaded.
#' 
#' The processing of a CompleteMultipartUpload request could take several
#' minutes to finalize. After Amazon S3 begins processing the request, it
#' sends an HTTP response header that specifies a `200 OK` response. While
#' processing is in progress, Amazon S3 periodically sends white space
#' characters to keep the connection from timing out. A request could fail
#' after the initial `200 OK` response has been sent. This means that a
#' `200 OK` response can contain either a success or an error. The error
#' response might be embedded in the `200 OK` response. If you call this
#' API operation directly, make sure to design your application to parse
#' the contents of the response and handle it appropriately. If you use
#' Amazon Web Services SDKs, SDKs handle this condition. The SDKs detect
#' the embedded error and apply error handling per your configuration
#' settings (including automatically retrying the request as appropriate).
#' If the condition persists, the SDKs throw an exception (or, for the SDKs
#' that don't use exceptions, they return an error).
#' 
#' Note that if [`complete_multipart_upload`][s3_complete_multipart_upload]
#' fails, applications should be prepared to retry any failed requests
#' (including 500 error responses). For more information, see [Amazon S3
#' Error Best
#' Practices](https://docs.aws.amazon.com/AmazonS3/latest/userguide/).
#' 
#' You can't use `Content-Type: application/x-www-form-urlencoded` for the
#' CompleteMultipartUpload requests. Also, if you don't provide a
#' `Content-Type` header,
#' [`complete_multipart_upload`][s3_complete_multipart_upload] can still
#' return a `200 OK` response.
#' 
#' For more information about multipart uploads, see [Uploading Objects
#' Using Multipart
#' Upload](https://docs.aws.amazon.com/AmazonS3/latest/userguide/mpuoverview.html)
#' in the *Amazon S3 User Guide*.
#' 
#' **Directory buckets** - For directory buckets, you must make requests
#' for this API operation to the Zonal endpoint. These endpoints support
#' virtual-hosted-style requests in the format
#' `https://amzn-s3-demo-bucket.s3express-zone-id.region-code.amazonaws.com/key-name `.
#' Path-style requests are not supported. For more information about
#' endpoints in Availability Zones, see [Regional and Zonal endpoints for
#' directory buckets in Availability
#' Zones](https://docs.aws.amazon.com/AmazonS3/latest/userguide/endpoint-directory-buckets-AZ.html)
#' in the *Amazon S3 User Guide*. For more information about endpoints in
#' Local Zones, see [Concepts for directory buckets in Local
#' Zones](https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-lzs-for-directory-buckets.html)
#' in the *Amazon S3 User Guide*.
#' 
#' ### Permissions
#' 
#' -   **General purpose bucket permissions** - For information about
#'     permissions required to use the multipart upload API, see [Multipart
#'     Upload and
#'     Permissions](https://docs.aws.amazon.com/AmazonS3/latest/userguide/mpuoverview.html)
#'     in the *Amazon S3 User Guide*.
#' 
#'     If you provide an [additional checksum
#'     value](https://docs.aws.amazon.com/AmazonS3/latest/API/API_Checksum.html)
#'     in your `MultipartUpload` requests and the object is encrypted with
#'     Key Management Service, you must have permission to use the
#'     `kms:Decrypt` action for the
#'     [`complete_multipart_upload`][s3_complete_multipart_upload] request
#'     to succeed.
#' 
#' -   **Directory bucket permissions** - To grant access to this API
#'     operation on a directory bucket, we recommend that you use the
#'     [`create_session`](https://docs.aws.amazon.com/AmazonS3/latest/API/API_CreateSession.html)
#'     API operation for session-based authorization. Specifically, you
#'     grant the `s3express:CreateSession` permission to the directory
#'     bucket in a bucket policy or an IAM identity-based policy. Then, you
#'     make the [`create_session`][s3_create_session] API call on the
#'     bucket to obtain a session token. With the session token in your
#'     request header, you can make API requests to this operation. After
#'     the session token expires, you make another
#'     [`create_session`][s3_create_session] API call to generate a new
#'     session token for use. Amazon Web Services CLI or SDKs create
#'     session and refresh the session token automatically to avoid service
#'     interruptions when a session expires. For more information about
#'     authorization, see
#'     [`create_session`](https://docs.aws.amazon.com/AmazonS3/latest/API/API_CreateSession.html)
#'     .
#' 
#'     If the object is encrypted with SSE-KMS, you must also have the
#'     `kms:GenerateDataKey` and `kms:Decrypt` permissions in IAM
#'     identity-based policies and KMS key policies for the KMS key.
#' 
#' ### Special errors
#' 
#' -   Error Code: `EntityTooSmall`
#' 
#'     -   Description: Your proposed upload is smaller than the minimum
#'         allowed object size. Each part must be at least 5 MB in size,
#'         except the last part.
#' 
#'     -   HTTP Status Code: 400 Bad Request
#' 
#' -   Error Code: `InvalidPart`
#' 
#'     -   Description: One or more of the specified parts could not be
#'         found. The part might not have been uploaded, or the specified
#'         ETag might not have matched the uploaded part's ETag.
#' 
#'     -   HTTP Status Code: 400 Bad Request
#' 
#' -   Error Code: `InvalidPartOrder`
#' 
#'     -   Description: The list of parts was not in ascending order. The
#'         parts list must be specified in order by part number.
#' 
#'     -   HTTP Status Code: 400 Bad Request
#' 
#' -   Error Code: `NoSuchUpload`
#' 
#'     -   Description: The specified multipart upload does not exist. The
#'         upload ID might be invalid, or the multipart upload might have
#'         been aborted or completed.
#' 
#'     -   HTTP Status Code: 404 Not Found
#' 
#' ### HTTP Host header syntax
#' 
#' **Directory buckets** - The HTTP Host header syntax is
#' ` Bucket-name.s3express-zone-id.region-code.amazonaws.com`.
#' 
#' The following operations are related to
#' [`complete_multipart_upload`][s3_complete_multipart_upload]:
#' 
#' -   [`create_multipart_upload`][s3_create_multipart_upload]
#' 
#' -   [`upload_part`][s3_upload_part]
#' 
#' -   [`abort_multipart_upload`][s3_abort_multipart_upload]
#' 
#' -   [`list_parts`][s3_list_parts]
#' 
#' -   [`list_multipart_uploads`][s3_list_multipart_uploads]
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#'
#' @usage
#' s3_complete_multipart_upload(Bucket, Key, MultipartUpload, UploadId,
#'   ChecksumCRC32, ChecksumCRC32C, ChecksumCRC64NVME, ChecksumSHA1,
#'   ChecksumSHA256, ChecksumType, MpuObjectSize, RequestPayer,
#'   ExpectedBucketOwner, IfMatch, IfNoneMatch, SSECustomerAlgorithm,
#'   SSECustomerKey, SSECustomerKeyMD5)
#'
#' @param Bucket &#91;required&#93; Name of the bucket to which the multipart upload was initiated.
#' 
#' **Directory buckets** - When you use this operation with a directory
#' bucket, you must use virtual-hosted-style requests in the format
#' ` Bucket-name.s3express-zone-id.region-code.amazonaws.com`. Path-style
#' requests are not supported. Directory bucket names must be unique in the
#' chosen Zone (Availability Zone or Local Zone). Bucket names must follow
#' the format ` bucket-base-name--zone-id--x-s3` (for example,
#' ` amzn-s3-demo-bucket--usw2-az1--x-s3`). For information about bucket
#' naming restrictions, see [Directory bucket naming
#' rules](https://docs.aws.amazon.com/AmazonS3/latest/userguide/directory-bucket-naming-rules.html)
#' in the *Amazon S3 User Guide*.
#' 
#' **Access points** - When you use this action with an access point for
#' general purpose buckets, you must provide the alias of the access point
#' in place of the bucket name or specify the access point ARN. When you
#' use this action with an access point for directory buckets, you must
#' provide the access point name in place of the bucket name. When using
#' the access point ARN, you must direct requests to the access point
#' hostname. The access point hostname takes the form
#' *AccessPointName*-*AccountId*.s3-accesspoint.*Region*.amazonaws.com.
#' When using this action with an access point through the Amazon Web
#' Services SDKs, you provide the access point ARN in place of the bucket
#' name. For more information about access point ARNs, see [Using access
#' points](https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-access-points.html)
#' in the *Amazon S3 User Guide*.
#' 
#' Object Lambda access points are not supported by directory buckets.
#' 
#' **S3 on Outposts** - When you use this action with S3 on Outposts, you
#' must direct requests to the S3 on Outposts hostname. The S3 on Outposts
#' hostname takes the form
#' ` AccessPointName-AccountId.outpostID.s3-outposts.Region.amazonaws.com`.
#' When you use this action with S3 on Outposts, the destination bucket
#' must be the Outposts access point ARN or the access point alias. For
#' more information about S3 on Outposts, see [What is S3 on
#' Outposts?](https://docs.aws.amazon.com/AmazonS3/latest/s3-outposts/S3onOutposts.html)
#' in the *Amazon S3 User Guide*.
#' @param Key &#91;required&#93; Object key for which the multipart upload was initiated.
#' @param MultipartUpload The container for the multipart upload request information.
#' @param UploadId &#91;required&#93; ID for the initiated multipart upload.
#' @param ChecksumCRC32 This header can be used as a data integrity check to verify that the
#' data received is the same data that was originally sent. This header
#' specifies the Base64 encoded, 32-bit `CRC32` checksum of the object. For
#' more information, see [Checking object
#' integrity](https://docs.aws.amazon.com/AmazonS3/latest/userguide/checking-object-integrity.html)
#' in the *Amazon S3 User Guide*.
#' @param ChecksumCRC32C This header can be used as a data integrity check to verify that the
#' data received is the same data that was originally sent. This header
#' specifies the Base64 encoded, 32-bit `CRC32C` checksum of the object.
#' For more information, see [Checking object
#' integrity](https://docs.aws.amazon.com/AmazonS3/latest/userguide/checking-object-integrity.html)
#' in the *Amazon S3 User Guide*.
#' @param ChecksumCRC64NVME This header can be used as a data integrity check to verify that the
#' data received is the same data that was originally sent. This header
#' specifies the Base64 encoded, 64-bit `CRC64NVME` checksum of the object.
#' The `CRC64NVME` checksum is always a full object checksum. For more
#' information, see [Checking object integrity in the Amazon S3 User
#' Guide](https://docs.aws.amazon.com/AmazonS3/latest/userguide/checking-object-integrity.html).
#' @param ChecksumSHA1 This header can be used as a data integrity check to verify that the
#' data received is the same data that was originally sent. This header
#' specifies the Base64 encoded, 160-bit `SHA1` digest of the object. For
#' more information, see [Checking object
#' integrity](https://docs.aws.amazon.com/AmazonS3/latest/userguide/checking-object-integrity.html)
#' in the *Amazon S3 User Guide*.
#' @param ChecksumSHA256 This header can be used as a data integrity check to verify that the
#' data received is the same data that was originally sent. This header
#' specifies the Base64 encoded, 256-bit `SHA256` digest of the object. For
#' more information, see [Checking object
#' integrity](https://docs.aws.amazon.com/AmazonS3/latest/userguide/checking-object-integrity.html)
#' in the *Amazon S3 User Guide*.
#' @param ChecksumType This header specifies the checksum type of the object, which determines
#' how part-level checksums are combined to create an object-level checksum
#' for multipart objects. You can use this header as a data integrity check
#' to verify that the checksum type that is received is the same checksum
#' that was specified. If the checksum type doesn’t match the checksum type
#' that was specified for the object during the
#' [`create_multipart_upload`][s3_create_multipart_upload] request, it’ll
#' result in a `BadDigest` error. For more information, see Checking object
#' integrity in the Amazon S3 User Guide.
#' @param MpuObjectSize The expected total object size of the multipart upload request. If
#' there’s a mismatch between the specified object size value and the
#' actual object size value, it results in an `HTTP 400 InvalidRequest`
#' error.
#' @param RequestPayer 
#' @param ExpectedBucketOwner The account ID of the expected bucket owner. If the account ID that you
#' provide does not match the actual owner of the bucket, the request fails
#' with the HTTP status code `403 Forbidden` (access denied).
#' @param IfMatch Uploads the object only if the ETag (entity tag) value provided during
#' the WRITE operation matches the ETag of the object in S3. If the ETag
#' values do not match, the operation returns a `412 Precondition Failed`
#' error.
#' 
#' If a conflicting operation occurs during the upload S3 returns a
#' `409 ConditionalRequestConflict` response. On a 409 failure you should
#' fetch the object's ETag, re-initiate the multipart upload with
#' [`create_multipart_upload`][s3_create_multipart_upload], and re-upload
#' each part.
#' 
#' Expects the ETag value as a string.
#' 
#' For more information about conditional requests, see [RFC
#' 7232](https://datatracker.ietf.org/doc/html/rfc7232), or [Conditional
#' requests](https://docs.aws.amazon.com/AmazonS3/latest/userguide/conditional-requests.html)
#' in the *Amazon S3 User Guide*.
#' @param IfNoneMatch Uploads the object only if the object key name does not already exist in
#' the bucket specified. Otherwise, Amazon S3 returns a
#' `412 Precondition Failed` error.
#' 
#' If a conflicting operation occurs during the upload S3 returns a
#' `409 ConditionalRequestConflict` response. On a 409 failure you should
#' re-initiate the multipart upload with
#' [`create_multipart_upload`][s3_create_multipart_upload] and re-upload
#' each part.
#' 
#' Expects the '*' (asterisk) character.
#' 
#' For more information about conditional requests, see [RFC
#' 7232](https://datatracker.ietf.org/doc/html/rfc7232), or [Conditional
#' requests](https://docs.aws.amazon.com/AmazonS3/latest/userguide/conditional-requests.html)
#' in the *Amazon S3 User Guide*.
#' @param SSECustomerAlgorithm The server-side encryption (SSE) algorithm used to encrypt the object.
#' This parameter is required only when the object was created using a
#' checksum algorithm or if your bucket policy requires the use of SSE-C.
#' For more information, see [Protecting data using SSE-C
#' keys](https://docs.aws.amazon.com/AmazonS3/latest/userguide/ServerSideEncryptionCustomerKeys.html#ssec-require-condition-key)
#' in the *Amazon S3 User Guide*.
#' 
#' This functionality is not supported for directory buckets.
#' @param SSECustomerKey The server-side encryption (SSE) customer managed key. This parameter is
#' needed only when the object was created using a checksum algorithm. For
#' more information, see [Protecting data using SSE-C
#' keys](https://docs.aws.amazon.com/AmazonS3/latest/userguide/ServerSideEncryptionCustomerKeys.html)
#' in the *Amazon S3 User Guide*.
#' 
#' This functionality is not supported for directory buckets.
#' @param SSECustomerKeyMD5 The MD5 server-side encryption (SSE) customer managed key. This
#' parameter is needed only when the object was created using a checksum
#' algorithm. For more information, see [Protecting data using SSE-C
#' keys](https://docs.aws.amazon.com/AmazonS3/latest/userguide/ServerSideEncryptionCustomerKeys.html)
#' in the *Amazon S3 User Guide*.
#' 
#' This functionality is not supported for directory buckets.
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   Location = "string",
#'   Bucket = "string",
#'   Key = "string",
#'   Expiration = "string",
#'   ETag = "string",
#'   ChecksumCRC32 = "string",
#'   ChecksumCRC32C = "string",
#'   ChecksumCRC64NVME = "string",
#'   ChecksumSHA1 = "string",
#'   ChecksumSHA256 = "string",
#'   ChecksumType = "COMPOSITE"|"FULL_OBJECT",
#'   ServerSideEncryption = "AES256"|"aws:fsx"|"aws:kms"|"aws:kms:dsse",
#'   VersionId = "string",
#'   SSEKMSKeyId = "string",
#'   BucketKeyEnabled = TRUE|FALSE,
#'   RequestCharged = "requester"
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$complete_multipart_upload(
#'   Bucket = "string",
#'   Key = "string",
#'   MultipartUpload = list(
#'     Parts = list(
#'       list(
#'         ETag = "string",
#'         ChecksumCRC32 = "string",
#'         ChecksumCRC32C = "string",
#'         ChecksumCRC64NVME = "string",
#'         ChecksumSHA1 = "string",
#'         ChecksumSHA256 = "string",
#'         PartNumber = 123
#'       )
#'     )
#'   ),
#'   UploadId = "string",
#'   ChecksumCRC32 = "string",
#'   ChecksumCRC32C = "string",
#'   ChecksumCRC64NVME = "string",
#'   ChecksumSHA1 = "string",
#'   ChecksumSHA256 = "string",
#'   ChecksumType = "COMPOSITE"|"FULL_OBJECT",
#'   MpuObjectSize = 123,
#'   RequestPayer = "requester",
#'   ExpectedBucketOwner = "string",
#'   IfMatch = "string",
#'   IfNoneMatch = "string",
#'   SSECustomerAlgorithm = "string",
#'   SSECustomerKey = "string",
#'   SSECustomerKeyMD5 = "string"
#' )
#' ```
#'
#' @examples
#' \dontrun{
#' # The following example completes a multipart upload.
#' svc$complete_multipart_upload(
#'   Bucket = "examplebucket",
#'   Key = "bigobject",
#'   MultipartUpload = list(
#'     Parts = list(
#'       list(
#'         ETag = ""d8c2eafd90c266e19ab9dcacc479f8af"",
#'         PartNumber = "1"
#'       ),
#'       list(
#'         ETag = ""d8c2eafd90c266e19ab9dcacc479f8af"",
#'         PartNumber = "2"
#'       )
#'     )
#'   ),
#'   UploadId = "7YPBOJuoFiQ9cz4P3Pe6FIZwO4f7wN93uHsNBEw97pl5eNwzExg0LAT2dUN91..."
#' )
#' }
#'
#' @keywords internal
#'
#' @rdname s3_complete_multipart_upload
#'
#' @aliases s3_complete_multipart_upload
s3_complete_multipart_upload <- function(Bucket, Key, MultipartUpload = NULL, UploadId, ChecksumCRC32 = NULL, ChecksumCRC32C = NULL, ChecksumCRC64NVME = NULL, ChecksumSHA1 = NULL, ChecksumSHA256 = NULL, ChecksumType = NULL, MpuObjectSize = NULL, RequestPayer = NULL, ExpectedBucketOwner = NULL, IfMatch = NULL, IfNoneMatch = NULL, SSECustomerAlgorithm = NULL, SSECustomerKey = NULL, SSECustomerKeyMD5 = NULL) {
  op <- new_operation(
    name = "CompleteMultipartUpload",
    http_method = "POST",
    http_path = "/{Bucket}/{Key+}",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .s3$complete_multipart_upload_input(Bucket = Bucket, Key = Key, MultipartUpload = MultipartUpload, UploadId = UploadId, ChecksumCRC32 = ChecksumCRC32, ChecksumCRC32C = ChecksumCRC32C, ChecksumCRC64NVME = ChecksumCRC64NVME, ChecksumSHA1 = ChecksumSHA1, ChecksumSHA256 = ChecksumSHA256, ChecksumType = ChecksumType, MpuObjectSize = MpuObjectSize, RequestPayer = RequestPayer, ExpectedBucketOwner = ExpectedBucketOwner, IfMatch = IfMatch, IfNoneMatch = IfNoneMatch, SSECustomerAlgorithm = SSECustomerAlgorithm, SSECustomerKey = SSECustomerKey, SSECustomerKeyMD5 = SSECustomerKeyMD5)
  output <- .s3$complete_multipart_upload_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$complete_multipart_upload <- s3_complete_multipart_upload

#' Creates a copy of an object that is already stored in Amazon S3
#'
#' @description
#' Creates a copy of an object that is already stored in Amazon S3.
#' 
#' End of support notice: As of October 1, 2025, Amazon S3 has discontinued
#' support for Email Grantee Access Control Lists (ACLs). If you attempt to
#' use an Email Grantee ACL in a request after October 1, 2025, the request
#' will receive an `HTTP 405` (Method Not Allowed) error.
#' 
#' This change affects the following Amazon Web Services Regions: US East
#' (N. Virginia), US West (N. California), US West (Oregon), Asia Pacific
#' (Singapore), Asia Pacific (Sydney), Asia Pacific (Tokyo), Europe
#' (Ireland), and South America (São Paulo).
#' 
#' You can store individual objects of up to 50 TB in Amazon S3. You create
#' a copy of your object up to 5 GB in size in a single atomic action using
#' this API. However, to copy an object greater than 5 GB, you must use the
#' multipart upload Upload Part - Copy (UploadPartCopy) API. For more
#' information, see [Copy Object Using the REST Multipart Upload
#' API](https://docs.aws.amazon.com/AmazonS3/latest/userguide/CopyingObjectsMPUapi.html).
#' 
#' You can copy individual objects between general purpose buckets, between
#' directory buckets, and between general purpose buckets and directory
#' buckets.
#' 
#' -   Amazon S3 supports copy operations using Multi-Region Access Points
#'     only as a destination when using the Multi-Region Access Point ARN.
#' 
#' -   **Directory buckets** - For directory buckets, you must make
#'     requests for this API operation to the Zonal endpoint. These
#'     endpoints support virtual-hosted-style requests in the format
#'     `https://amzn-s3-demo-bucket.s3express-zone-id.region-code.amazonaws.com/key-name `.
#'     Path-style requests are not supported. For more information about
#'     endpoints in Availability Zones, see [Regional and Zonal endpoints
#'     for directory buckets in Availability
#'     Zones](https://docs.aws.amazon.com/AmazonS3/latest/userguide/endpoint-directory-buckets-AZ.html)
#'     in the *Amazon S3 User Guide*. For more information about endpoints
#'     in Local Zones, see [Concepts for directory buckets in Local
#'     Zones](https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-lzs-for-directory-buckets.html)
#'     in the *Amazon S3 User Guide*.
#' 
#' -   VPC endpoints don't support cross-Region requests (including
#'     copies). If you're using VPC endpoints, your source and destination
#'     buckets should be in the same Amazon Web Services Region as your VPC
#'     endpoint.
#' 
#' Both the Region that you want to copy the object from and the Region
#' that you want to copy the object to must be enabled for your account.
#' For more information about how to enable a Region for your account, see
#' [Enable or disable a Region for standalone
#' accounts](https://docs.aws.amazon.com/accounts/latest/reference/manage-acct-regions.html#manage-acct-regions-enable-standalone)
#' in the *Amazon Web Services Account Management Guide*.
#' 
#' Amazon S3 transfer acceleration does not support cross-Region copies. If
#' you request a cross-Region copy using a transfer acceleration endpoint,
#' you get a `400 Bad Request` error. For more information, see [Transfer
#' Acceleration](https://docs.aws.amazon.com/AmazonS3/latest/userguide/transfer-acceleration.html).
#' 
#' ### Authentication and authorization
#' 
#' All [`copy_object`][s3_copy_object] requests must be authenticated and
#' signed by using IAM credentials (access key ID and secret access key for
#' the IAM identities). All headers with the `x-amz-` prefix, including
#' `x-amz-copy-source`, must be signed. For more information, see [REST
#' Authentication](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTAuthentication.html).
#' 
#' **Directory buckets** - You must use the IAM credentials to authenticate
#' and authorize your access to the [`copy_object`][s3_copy_object] API
#' operation, instead of using the temporary security credentials through
#' the [`create_session`][s3_create_session] API operation.
#' 
#' Amazon Web Services CLI or SDKs handles authentication and authorization
#' on your behalf.
#' 
#' ### Permissions
#' 
#' You must have *read* access to the source object and *write* access to
#' the destination bucket.
#' 
#' -   **General purpose bucket permissions** - You must have permissions
#'     in an IAM policy based on the source and destination bucket types in
#'     a [`copy_object`][s3_copy_object] operation.
#' 
#'     -   If the source object is in a general purpose bucket, you must
#'         have **`s3:GetObject`** permission to read the source object
#'         that is being copied.
#' 
#'     -   If the destination bucket is a general purpose bucket, you must
#'         have **`s3:PutObject`** permission to write the object copy to
#'         the destination bucket.
#' 
#' -   **Directory bucket permissions** - You must have permissions in a
#'     bucket policy or an IAM identity-based policy based on the source
#'     and destination bucket types in a [`copy_object`][s3_copy_object]
#'     operation.
#' 
#'     -   If the source object that you want to copy is in a directory
#'         bucket, you must have the **`s3express:CreateSession`**
#'         permission in the `Action` element of a policy to read the
#'         object. By default, the session is in the `ReadWrite` mode. If
#'         you want to restrict the access, you can explicitly set the
#'         `s3express:SessionMode` condition key to `ReadOnly` on the copy
#'         source bucket.
#' 
#'     -   If the copy destination is a directory bucket, you must have the
#'         **`s3express:CreateSession`** permission in the `Action` element
#'         of a policy to write the object to the destination. The
#'         `s3express:SessionMode` condition key can't be set to `ReadOnly`
#'         on the copy destination bucket.
#' 
#'     If the object is encrypted with SSE-KMS, you must also have the
#'     `kms:GenerateDataKey` and `kms:Decrypt` permissions in IAM
#'     identity-based policies and KMS key policies for the KMS key.
#' 
#'     For example policies, see [Example bucket policies for S3 Express
#'     One
#'     Zone](https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-express-security-iam-example-bucket-policies.html)
#'     and [Amazon Web Services Identity and Access Management (IAM)
#'     identity-based policies for S3 Express One
#'     Zone](https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-express-security-iam-identity-policies.html)
#'     in the *Amazon S3 User Guide*.
#' 
#' ### Response and special errors
#' 
#' When the request is an HTTP 1.1 request, the response is chunk encoded.
#' When the request is not an HTTP 1.1 request, the response would not
#' contain the `Content-Length`. You always need to read the entire
#' response body to check if the copy succeeds.
#' 
#' -   If the copy is successful, you receive a response with information
#'     about the copied object.
#' 
#' -   A copy request might return an error when Amazon S3 receives the
#'     copy request or while Amazon S3 is copying the files. A `200 OK`
#'     response can contain either a success or an error.
#' 
#'     -   If the error occurs before the copy action starts, you receive a
#'         standard Amazon S3 error.
#' 
#'     -   If the error occurs during the copy operation, the error
#'         response is embedded in the `200 OK` response. For example, in a
#'         cross-region copy, you may encounter throttling and receive a
#'         `200 OK` response. For more information, see Resolve the Error
#'         200 response when copying objects to Amazon S3. The `200 OK`
#'         status code means the copy was accepted, but it doesn't mean the
#'         copy is complete. Another example is when you disconnect from
#'         Amazon S3 before the copy is complete, Amazon S3 might cancel
#'         the copy and you may receive a `200 OK` response. You must stay
#'         connected to Amazon S3 until the entire response is successfully
#'         received and processed.
#' 
#'         If you call this API operation directly, make sure to design
#'         your application to parse the content of the response and handle
#'         it appropriately. If you use Amazon Web Services SDKs, SDKs
#'         handle this condition. The SDKs detect the embedded error and
#'         apply error handling per your configuration settings (including
#'         automatically retrying the request as appropriate). If the
#'         condition persists, the SDKs throw an exception (or, for the
#'         SDKs that don't use exceptions, they return an error).
#' 
#' ### Charge
#' 
#' The copy request charge is based on the storage class and Region that
#' you specify for the destination object. The request can also result in a
#' data retrieval charge for the source if the source storage class bills
#' for data retrieval. If the copy source is in a different region, the
#' data transfer is billed to the copy source account. For pricing
#' information, see [Amazon S3
#' pricing](https://aws.amazon.com/s3/pricing/).
#' 
#' ### HTTP Host header syntax
#' 
#' -   **Directory buckets** - The HTTP Host header syntax is
#'     ` Bucket-name.s3express-zone-id.region-code.amazonaws.com`.
#' 
#' -   **Amazon S3 on Outposts** - When you use this action with S3 on
#'     Outposts through the REST API, you must direct requests to the S3 on
#'     Outposts hostname. The S3 on Outposts hostname takes the form
#'     ` AccessPointName-AccountId.outpostID.s3-outposts.Region.amazonaws.com`.
#'     The hostname isn't required when you use the Amazon Web Services CLI
#'     or SDKs.
#' 
#' The following operations are related to [`copy_object`][s3_copy_object]:
#' 
#' -   [`put_object`][s3_put_object]
#' 
#' -   [`get_object`][s3_get_object]
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#'
#' @usage
#' s3_copy_object(ACL, Bucket, CacheControl, ChecksumAlgorithm,
#'   ContentDisposition, ContentEncoding, ContentLanguage, ContentType,
#'   CopySource, CopySourceIfMatch, CopySourceIfModifiedSince,
#'   CopySourceIfNoneMatch, CopySourceIfUnmodifiedSince, Expires,
#'   GrantFullControl, GrantRead, GrantReadACP, GrantWriteACP, IfMatch,
#'   IfNoneMatch, Key, Metadata, MetadataDirective, TaggingDirective,
#'   ServerSideEncryption, StorageClass, WebsiteRedirectLocation,
#'   SSECustomerAlgorithm, SSECustomerKey, SSECustomerKeyMD5, SSEKMSKeyId,
#'   SSEKMSEncryptionContext, BucketKeyEnabled,
#'   CopySourceSSECustomerAlgorithm, CopySourceSSECustomerKey,
#'   CopySourceSSECustomerKeyMD5, RequestPayer, Tagging, ObjectLockMode,
#'   ObjectLockRetainUntilDate, ObjectLockLegalHoldStatus,
#'   ExpectedBucketOwner, ExpectedSourceBucketOwner)
#'
#' @param ACL The canned access control list (ACL) to apply to the object.
#' 
#' When you copy an object, the ACL metadata is not preserved and is set to
#' `private` by default. Only the owner has full access control. To
#' override the default ACL setting, specify a new ACL when you generate a
#' copy request. For more information, see [Using
#' ACLs](https://docs.aws.amazon.com/AmazonS3/latest/userguide/acl-overview.html).
#' 
#' If the destination bucket that you're copying objects to uses the bucket
#' owner enforced setting for S3 Object Ownership, ACLs are disabled and no
#' longer affect permissions. Buckets that use this setting only accept
#' `PUT` requests that don't specify an ACL or `PUT` requests that specify
#' bucket owner full control ACLs, such as the `bucket-owner-full-control`
#' canned ACL or an equivalent form of this ACL expressed in the XML
#' format. For more information, see [Controlling ownership of objects and
#' disabling
#' ACLs](https://docs.aws.amazon.com/AmazonS3/latest/userguide/about-object-ownership.html)
#' in the *Amazon S3 User Guide*.
#' 
#' -   If your destination bucket uses the bucket owner enforced setting
#'     for Object Ownership, all objects written to the bucket by any
#'     account will be owned by the bucket owner.
#' 
#' -   This functionality is not supported for directory buckets.
#' 
#' -   This functionality is not supported for Amazon S3 on Outposts.
#' @param Bucket &#91;required&#93; The name of the destination bucket.
#' 
#' **Directory buckets** - When you use this operation with a directory
#' bucket, you must use virtual-hosted-style requests in the format
#' ` Bucket-name.s3express-zone-id.region-code.amazonaws.com`. Path-style
#' requests are not supported. Directory bucket names must be unique in the
#' chosen Zone (Availability Zone or Local Zone). Bucket names must follow
#' the format ` bucket-base-name--zone-id--x-s3` (for example,
#' ` amzn-s3-demo-bucket--usw2-az1--x-s3`). For information about bucket
#' naming restrictions, see [Directory bucket naming
#' rules](https://docs.aws.amazon.com/AmazonS3/latest/userguide/directory-bucket-naming-rules.html)
#' in the *Amazon S3 User Guide*.
#' 
#' Copying objects across different Amazon Web Services Regions isn't
#' supported when the source or destination bucket is in Amazon Web
#' Services Local Zones. The source and destination buckets must have the
#' same parent Amazon Web Services Region. Otherwise, you get an HTTP
#' `400 Bad Request` error with the error code `InvalidRequest`.
#' 
#' **Access points** - When you use this action with an access point for
#' general purpose buckets, you must provide the alias of the access point
#' in place of the bucket name or specify the access point ARN. When you
#' use this action with an access point for directory buckets, you must
#' provide the access point name in place of the bucket name. When using
#' the access point ARN, you must direct requests to the access point
#' hostname. The access point hostname takes the form
#' *AccessPointName*-*AccountId*.s3-accesspoint.*Region*.amazonaws.com.
#' When using this action with an access point through the Amazon Web
#' Services SDKs, you provide the access point ARN in place of the bucket
#' name. For more information about access point ARNs, see [Using access
#' points](https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-access-points.html)
#' in the *Amazon S3 User Guide*.
#' 
#' Object Lambda access points are not supported by directory buckets.
#' 
#' **S3 on Outposts** - When you use this action with S3 on Outposts, you
#' must use the Outpost bucket access point ARN or the access point alias
#' for the destination bucket. You can only copy objects within the same
#' Outpost bucket. It's not supported to copy objects across different
#' Amazon Web Services Outposts, between buckets on the same Outposts, or
#' between Outposts buckets and any other bucket types. For more
#' information about S3 on Outposts, see [What is S3 on
#' Outposts?](https://docs.aws.amazon.com/AmazonS3/latest/s3-outposts/S3onOutposts.html)
#' in the *S3 on Outposts guide*. When you use this action with S3 on
#' Outposts through the REST API, you must direct requests to the S3 on
#' Outposts hostname, in the format
#' ` AccessPointName-AccountId.outpostID.s3-outposts.Region.amazonaws.com`.
#' The hostname isn't required when you use the Amazon Web Services CLI or
#' SDKs.
#' @param CacheControl Specifies the caching behavior along the request/reply chain.
#' @param ChecksumAlgorithm Indicates the algorithm that you want Amazon S3 to use to create the
#' checksum for the object. For more information, see [Checking object
#' integrity](https://docs.aws.amazon.com/AmazonS3/latest/userguide/checking-object-integrity.html)
#' in the *Amazon S3 User Guide*.
#' 
#' When you copy an object, if the source object has a checksum, that
#' checksum value will be copied to the new object by default. If the
#' [`copy_object`][s3_copy_object] request does not include this
#' `x-amz-checksum-algorithm` header, the checksum algorithm will be copied
#' from the source object to the destination object (if it's present on the
#' source object). You can optionally specify a different checksum
#' algorithm to use with the `x-amz-checksum-algorithm` header.
#' Unrecognized or unsupported values will respond with the HTTP status
#' code `400 Bad Request`.
#' 
#' For directory buckets, when you use Amazon Web Services SDKs, `CRC32` is
#' the default checksum algorithm that's used for performance.
#' @param ContentDisposition Specifies presentational information for the object. Indicates whether
#' an object should be displayed in a web browser or downloaded as a file.
#' It allows specifying the desired filename for the downloaded file.
#' @param ContentEncoding Specifies what content encodings have been applied to the object and
#' thus what decoding mechanisms must be applied to obtain the media-type
#' referenced by the Content-Type header field.
#' 
#' For directory buckets, only the `aws-chunked` value is supported in this
#' header field.
#' @param ContentLanguage The language the content is in.
#' @param ContentType A standard MIME type that describes the format of the object data.
#' @param CopySource &#91;required&#93; Specifies the source object for the copy operation. The source object
#' can be up to 5 GB. If the source object is an object that was uploaded
#' by using a multipart upload, the object copy will be a single part
#' object after the source object is copied to the destination bucket.
#' 
#' You specify the value of the copy source in one of two formats,
#' depending on whether you want to access the source object through an
#' [access
#' point](https://docs.aws.amazon.com/AmazonS3/latest/userguide/access-points.html):
#' 
#' -   For objects not accessed through an access point, specify the name
#'     of the source bucket and the key of the source object, separated by
#'     a slash (/). For example, to copy the object `reports/january.pdf`
#'     from the general purpose bucket `awsexamplebucket`, use
#'     `awsexamplebucket/reports/january.pdf`. The value must be
#'     URL-encoded. To copy the object `reports/january.pdf` from the
#'     directory bucket `awsexamplebucket--use1-az5--x-s3`, use
#'     `awsexamplebucket--use1-az5--x-s3/reports/january.pdf`. The value
#'     must be URL-encoded.
#' 
#' -   For objects accessed through access points, specify the Amazon
#'     Resource Name (ARN) of the object as accessed through the access
#'     point, in the format
#'     `arn:aws:s3:<Region>:<account-id>:accesspoint/<access-point-name>/object/<key>`.
#'     For example, to copy the object `reports/january.pdf` through access
#'     point `my-access-point` owned by account `123456789012` in Region
#'     `us-west-2`, use the URL encoding of
#'     `arn:aws:s3:us-west-2:123456789012:accesspoint/my-access-point/object/reports/january.pdf`.
#'     The value must be URL encoded.
#' 
#'     -   Amazon S3 supports copy operations using Access points only when
#'         the source and destination buckets are in the same Amazon Web
#'         Services Region.
#' 
#'     -   Access points are not supported by directory buckets.
#' 
#'     Alternatively, for objects accessed through Amazon S3 on Outposts,
#'     specify the ARN of the object as accessed in the format
#'     `arn:aws:s3-outposts:<Region>:<account-id>:outpost/<outpost-id>/object/<key>`.
#'     For example, to copy the object `reports/january.pdf` through
#'     outpost `my-outpost` owned by account `123456789012` in Region
#'     `us-west-2`, use the URL encoding of
#'     `arn:aws:s3-outposts:us-west-2:123456789012:outpost/my-outpost/object/reports/january.pdf`.
#'     The value must be URL-encoded.
#' 
#' If your source bucket versioning is enabled, the `x-amz-copy-source`
#' header by default identifies the current version of an object to copy.
#' If the current version is a delete marker, Amazon S3 behaves as if the
#' object was deleted. To copy a different version, use the `versionId`
#' query parameter. Specifically, append `?versionId=<version-id>` to the
#' value (for example,
#' `awsexamplebucket/reports/january.pdf?versionId=QUpfdndhfd8438MNFDN93jdnJFkdmqnh893`).
#' If you don't specify a version ID, Amazon S3 copies the latest version
#' of the source object.
#' 
#' If you enable versioning on the destination bucket, Amazon S3 generates
#' a unique version ID for the copied object. This version ID is different
#' from the version ID of the source object. Amazon S3 returns the version
#' ID of the copied object in the `x-amz-version-id` response header in the
#' response.
#' 
#' If you do not enable versioning or suspend it on the destination bucket,
#' the version ID that Amazon S3 generates in the `x-amz-version-id`
#' response header is always null.
#' 
#' **Directory buckets** - S3 Versioning isn't enabled and supported for
#' directory buckets.
#' @param CopySourceIfMatch Copies the object if its entity tag (ETag) matches the specified tag.
#' 
#' If both the `x-amz-copy-source-if-match` and
#' `x-amz-copy-source-if-unmodified-since` headers are present in the
#' request and evaluate as follows, Amazon S3 returns `200 OK` and copies
#' the data:
#' 
#' -   `x-amz-copy-source-if-match` condition evaluates to true
#' 
#' -   `x-amz-copy-source-if-unmodified-since` condition evaluates to false
#' @param CopySourceIfModifiedSince Copies the object if it has been modified since the specified time.
#' 
#' If both the `x-amz-copy-source-if-none-match` and
#' `x-amz-copy-source-if-modified-since` headers are present in the request
#' and evaluate as follows, Amazon S3 returns the `412 Precondition Failed`
#' response code:
#' 
#' -   `x-amz-copy-source-if-none-match` condition evaluates to false
#' 
#' -   `x-amz-copy-source-if-modified-since` condition evaluates to true
#' @param CopySourceIfNoneMatch Copies the object if its entity tag (ETag) is different than the
#' specified ETag.
#' 
#' If both the `x-amz-copy-source-if-none-match` and
#' `x-amz-copy-source-if-modified-since` headers are present in the request
#' and evaluate as follows, Amazon S3 returns the `412 Precondition Failed`
#' response code:
#' 
#' -   `x-amz-copy-source-if-none-match` condition evaluates to false
#' 
#' -   `x-amz-copy-source-if-modified-since` condition evaluates to true
#' @param CopySourceIfUnmodifiedSince Copies the object if it hasn't been modified since the specified time.
#' 
#' If both the `x-amz-copy-source-if-match` and
#' `x-amz-copy-source-if-unmodified-since` headers are present in the
#' request and evaluate as follows, Amazon S3 returns `200 OK` and copies
#' the data:
#' 
#' -   `x-amz-copy-source-if-match` condition evaluates to true
#' 
#' -   `x-amz-copy-source-if-unmodified-since` condition evaluates to false
#' @param Expires The date and time at which the object is no longer cacheable.
#' @param GrantFullControl Gives the grantee READ, READ_ACP, and WRITE_ACP permissions on the
#' object.
#' 
#' -   This functionality is not supported for directory buckets.
#' 
#' -   This functionality is not supported for Amazon S3 on Outposts.
#' @param GrantRead Allows grantee to read the object data and its metadata.
#' 
#' -   This functionality is not supported for directory buckets.
#' 
#' -   This functionality is not supported for Amazon S3 on Outposts.
#' @param GrantReadACP Allows grantee to read the object ACL.
#' 
#' -   This functionality is not supported for directory buckets.
#' 
#' -   This functionality is not supported for Amazon S3 on Outposts.
#' @param GrantWriteACP Allows grantee to write the ACL for the applicable object.
#' 
#' -   This functionality is not supported for directory buckets.
#' 
#' -   This functionality is not supported for Amazon S3 on Outposts.
#' @param IfMatch Copies the object if the entity tag (ETag) of the destination object
#' matches the specified tag. If the ETag values do not match, the
#' operation returns a `412 Precondition Failed` error. If a concurrent
#' operation occurs during the upload S3 returns a
#' `409 ConditionalRequestConflict` response. On a 409 failure you should
#' fetch the object's ETag and retry the upload.
#' 
#' Expects the ETag value as a string.
#' 
#' For more information about conditional requests, see [RFC
#' 7232](https://datatracker.ietf.org/doc/html/rfc7232).
#' @param IfNoneMatch Copies the object only if the object key name at the destination does
#' not already exist in the bucket specified. Otherwise, Amazon S3 returns
#' a `412 Precondition Failed` error. If a concurrent operation occurs
#' during the upload S3 returns a `409 ConditionalRequestConflict`
#' response. On a 409 failure you should retry the upload.
#' 
#' Expects the '*' (asterisk) character.
#' 
#' For more information about conditional requests, see [RFC
#' 7232](https://datatracker.ietf.org/doc/html/rfc7232).
#' @param Key &#91;required&#93; The key of the destination object.
#' @param Metadata A map of metadata to store with the object in S3.
#' @param MetadataDirective Specifies whether the metadata is copied from the source object or
#' replaced with metadata that's provided in the request. When copying an
#' object, you can preserve all metadata (the default) or specify new
#' metadata. If this header isn’t specified, `COPY` is the default
#' behavior.
#' 
#' **General purpose bucket** - For general purpose buckets, when you grant
#' permissions, you can use the `s3:x-amz-metadata-directive` condition key
#' to enforce certain metadata behavior when objects are uploaded. For more
#' information, see [Amazon S3 condition key
#' examples](https://docs.aws.amazon.com/AmazonS3/latest/userguide/amazon-s3-policy-keys.html)
#' in the *Amazon S3 User Guide*.
#' 
#' `x-amz-website-redirect-location` is unique to each object and is not
#' copied when using the `x-amz-metadata-directive` header. To copy the
#' value, you must specify `x-amz-website-redirect-location` in the request
#' header.
#' @param TaggingDirective Specifies whether the object tag-set is copied from the source object or
#' replaced with the tag-set that's provided in the request.
#' 
#' The default value is `COPY`.
#' 
#' **Directory buckets** - For directory buckets in a
#' [`copy_object`][s3_copy_object] operation, only the empty tag-set is
#' supported. Any requests that attempt to write non-empty tags into
#' directory buckets will receive a `501 Not Implemented` status code. When
#' the destination bucket is a directory bucket, you will receive a
#' `501 Not Implemented` response in any of the following situations:
#' 
#' -   When you attempt to `COPY` the tag-set from an S3 source object that
#'     has non-empty tags.
#' 
#' -   When you attempt to `REPLACE` the tag-set of a source object and set
#'     a non-empty value to `x-amz-tagging`.
#' 
#' -   When you don't set the `x-amz-tagging-directive` header and the
#'     source object has non-empty tags. This is because the default value
#'     of `x-amz-tagging-directive` is `COPY`.
#' 
#' Because only the empty tag-set is supported for directory buckets in a
#' [`copy_object`][s3_copy_object] operation, the following situations are
#' allowed:
#' 
#' -   When you attempt to `COPY` the tag-set from a directory bucket
#'     source object that has no tags to a general purpose bucket. It
#'     copies an empty tag-set to the destination object.
#' 
#' -   When you attempt to `REPLACE` the tag-set of a directory bucket
#'     source object and set the `x-amz-tagging` value of the directory
#'     bucket destination object to empty.
#' 
#' -   When you attempt to `REPLACE` the tag-set of a general purpose
#'     bucket source object that has non-empty tags and set the
#'     `x-amz-tagging` value of the directory bucket destination object to
#'     empty.
#' 
#' -   When you attempt to `REPLACE` the tag-set of a directory bucket
#'     source object and don't set the `x-amz-tagging` value of the
#'     directory bucket destination object. This is because the default
#'     value of `x-amz-tagging` is the empty value.
#' @param ServerSideEncryption The server-side encryption algorithm used when storing this object in
#' Amazon S3. Unrecognized or unsupported values won’t write a destination
#' object and will receive a `400 Bad Request` response.
#' 
#' Amazon S3 automatically encrypts all new objects that are copied to an
#' S3 bucket. When copying an object, if you don't specify encryption
#' information in your copy request, the encryption setting of the target
#' object is set to the default encryption configuration of the destination
#' bucket. By default, all buckets have a base level of encryption
#' configuration that uses server-side encryption with Amazon S3 managed
#' keys (SSE-S3). If the destination bucket has a different default
#' encryption configuration, Amazon S3 uses the corresponding encryption
#' key to encrypt the target object copy.
#' 
#' With server-side encryption, Amazon S3 encrypts your data as it writes
#' your data to disks in its data centers and decrypts the data when you
#' access it. For more information about server-side encryption, see [Using
#' Server-Side
#' Encryption](https://docs.aws.amazon.com/AmazonS3/latest/userguide/serv-side-encryption.html)
#' in the *Amazon S3 User Guide*.
#' 
#' **General purpose buckets**
#' 
#' -   For general purpose buckets, there are the following supported
#'     options for server-side encryption: server-side encryption with Key
#'     Management Service (KMS) keys (SSE-KMS), dual-layer server-side
#'     encryption with Amazon Web Services KMS keys (DSSE-KMS), and
#'     server-side encryption with customer-provided encryption keys
#'     (SSE-C). Amazon S3 uses the corresponding KMS key, or a
#'     customer-provided key to encrypt the target object copy.
#' 
#' -   When you perform a [`copy_object`][s3_copy_object] operation, if you
#'     want to use a different type of encryption setting for the target
#'     object, you can specify appropriate encryption-related headers to
#'     encrypt the target object with an Amazon S3 managed key, a KMS key,
#'     or a customer-provided key. If the encryption setting in your
#'     request is different from the default encryption configuration of
#'     the destination bucket, the encryption setting in your request takes
#'     precedence.
#' 
#' **Directory buckets**
#' 
#' -   For directory buckets, there are only two supported options for
#'     server-side encryption: server-side encryption with Amazon S3
#'     managed keys (SSE-S3) (`AES256`) and server-side encryption with KMS
#'     keys (SSE-KMS) (`aws:kms`). We recommend that the bucket's default
#'     encryption uses the desired encryption configuration and you don't
#'     override the bucket default encryption in your
#'     [`create_session`][s3_create_session] requests or `PUT` object
#'     requests. Then, new objects are automatically encrypted with the
#'     desired encryption settings. For more information, see [Protecting
#'     data with server-side
#'     encryption](https://docs.aws.amazon.com/AmazonS3/latest/userguide/)
#'     in the *Amazon S3 User Guide*. For more information about the
#'     encryption overriding behaviors in directory buckets, see
#'     [Specifying server-side encryption with KMS for new object
#'     uploads](https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-express-specifying-kms-encryption.html).
#' 
#' -   To encrypt new object copies to a directory bucket with SSE-KMS, we
#'     recommend you specify SSE-KMS as the directory bucket's default
#'     encryption configuration with a KMS key (specifically, a [customer
#'     managed
#'     key](https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#customer-cmk)).
#'     The [Amazon Web Services managed
#'     key](https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#aws-managed-cmk)
#'     (`aws/s3`) isn't supported. Your SSE-KMS configuration can only
#'     support 1 [customer managed
#'     key](https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#customer-cmk)
#'     per directory bucket for the lifetime of the bucket. After you
#'     specify a customer managed key for SSE-KMS, you can't override the
#'     customer managed key for the bucket's SSE-KMS configuration. Then,
#'     when you perform a [`copy_object`][s3_copy_object] operation and
#'     want to specify server-side encryption settings for new object
#'     copies with SSE-KMS in the encryption-related request headers, you
#'     must ensure the encryption key is the same customer managed key that
#'     you specified for the directory bucket's default encryption
#'     configuration.
#' 
#' -   **S3 access points for Amazon FSx** - When accessing data stored in
#'     Amazon FSx file systems using S3 access points, the only valid
#'     server side encryption option is `aws:fsx`. All Amazon FSx file
#'     systems have encryption configured by default and are encrypted at
#'     rest. Data is automatically encrypted before being written to the
#'     file system, and automatically decrypted as it is read. These
#'     processes are handled transparently by Amazon FSx.
#' @param StorageClass If the `x-amz-storage-class` header is not used, the copied object will
#' be stored in the `STANDARD` Storage Class by default. The `STANDARD`
#' storage class provides high durability and high availability. Depending
#' on performance needs, you can specify a different Storage Class.
#' 
#' -   **Directory buckets** - Directory buckets only support
#'     `EXPRESS_ONEZONE` (the S3 Express One Zone storage class) in
#'     Availability Zones and `ONEZONE_IA` (the S3 One Zone-Infrequent
#'     Access storage class) in Dedicated Local Zones. Unsupported storage
#'     class values won't write a destination object and will respond with
#'     the HTTP status code `400 Bad Request`.
#' 
#' -   **Amazon S3 on Outposts** - S3 on Outposts only uses the `OUTPOSTS`
#'     Storage Class.
#' 
#' You can use the [`copy_object`][s3_copy_object] action to change the
#' storage class of an object that is already stored in Amazon S3 by using
#' the `x-amz-storage-class` header. For more information, see [Storage
#' Classes](https://docs.aws.amazon.com/AmazonS3/latest/userguide/storage-class-intro.html)
#' in the *Amazon S3 User Guide*.
#' 
#' Before using an object as a source object for the copy operation, you
#' must restore a copy of it if it meets any of the following conditions:
#' 
#' -   The storage class of the source object is `GLACIER` or
#'     `DEEP_ARCHIVE`.
#' 
#' -   The storage class of the source object is `INTELLIGENT_TIERING` and
#'     it's [S3 Intelligent-Tiering access
#'     tier](https://docs.aws.amazon.com/AmazonS3/latest/userguide/intelligent-tiering-overview.html#intel-tiering-tier-definition)
#'     is `Archive Access` or `Deep Archive Access`.
#' 
#' For more information, see [`restore_object`][s3_restore_object] and
#' [Copying
#' Objects](https://docs.aws.amazon.com/AmazonS3/latest/userguide/copy-object.html)
#' in the *Amazon S3 User Guide*.
#' @param WebsiteRedirectLocation If the destination bucket is configured as a website, redirects requests
#' for this object copy to another object in the same bucket or to an
#' external URL. Amazon S3 stores the value of this header in the object
#' metadata. This value is unique to each object and is not copied when
#' using the `x-amz-metadata-directive` header. Instead, you may opt to
#' provide this header in combination with the `x-amz-metadata-directive`
#' header.
#' 
#' This functionality is not supported for directory buckets.
#' @param SSECustomerAlgorithm Specifies the algorithm to use when encrypting the object (for example,
#' `AES256`).
#' 
#' When you perform a [`copy_object`][s3_copy_object] operation, if you
#' want to use a different type of encryption setting for the target
#' object, you can specify appropriate encryption-related headers to
#' encrypt the target object with an Amazon S3 managed key, a KMS key, or a
#' customer-provided key. If the encryption setting in your request is
#' different from the default encryption configuration of the destination
#' bucket, the encryption setting in your request takes precedence.
#' 
#' This functionality is not supported when the destination bucket is a
#' directory bucket.
#' @param SSECustomerKey Specifies the customer-provided encryption key for Amazon S3 to use in
#' encrypting data. This value is used to store the object and then it is
#' discarded. Amazon S3 does not store the encryption key. The key must be
#' appropriate for use with the algorithm specified in the
#' `x-amz-server-side-encryption-customer-algorithm` header.
#' 
#' This functionality is not supported when the destination bucket is a
#' directory bucket.
#' @param SSECustomerKeyMD5 Specifies the 128-bit MD5 digest of the encryption key according to RFC
#' 1321. Amazon S3 uses this header for a message integrity check to ensure
#' that the encryption key was transmitted without error.
#' 
#' This functionality is not supported when the destination bucket is a
#' directory bucket.
#' @param SSEKMSKeyId Specifies the KMS key ID (Key ID, Key ARN, or Key Alias) to use for
#' object encryption. All GET and PUT requests for an object protected by
#' KMS will fail if they're not made via SSL or using SigV4. For
#' information about configuring any of the officially supported Amazon Web
#' Services SDKs and Amazon Web Services CLI, see [Specifying the Signature
#' Version in Request
#' Authentication](https://docs.aws.amazon.com/AmazonS3/latest/API/#specify-signature-version)
#' in the *Amazon S3 User Guide*.
#' 
#' **Directory buckets** - To encrypt data using SSE-KMS, it's recommended
#' to specify the `x-amz-server-side-encryption` header to `aws:kms`. Then,
#' the `x-amz-server-side-encryption-aws-kms-key-id` header implicitly uses
#' the bucket's default KMS customer managed key ID. If you want to
#' explicitly set the ` x-amz-server-side-encryption-aws-kms-key-id`
#' header, it must match the bucket's default customer managed key (using
#' key ID or ARN, not alias). Your SSE-KMS configuration can only support 1
#' [customer managed
#' key](https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#customer-cmk)
#' per directory bucket's lifetime. The [Amazon Web Services managed
#' key](https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#aws-managed-cmk)
#' (`aws/s3`) isn't supported. Incorrect key specification results in an
#' HTTP `400 Bad Request` error.
#' @param SSEKMSEncryptionContext Specifies the Amazon Web Services KMS Encryption Context as an
#' additional encryption context to use for the destination object
#' encryption. The value of this header is a base64-encoded UTF-8 string
#' holding JSON with the encryption context key-value pairs.
#' 
#' **General purpose buckets** - This value must be explicitly added to
#' specify encryption context for [`copy_object`][s3_copy_object] requests
#' if you want an additional encryption context for your destination
#' object. The additional encryption context of the source object won't be
#' copied to the destination object. For more information, see [Encryption
#' context](https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingKMSEncryption.html#encryption-context)
#' in the *Amazon S3 User Guide*.
#' 
#' **Directory buckets** - You can optionally provide an explicit
#' encryption context value. The value must match the default encryption
#' context - the bucket Amazon Resource Name (ARN). An additional
#' encryption context value is not supported.
#' @param BucketKeyEnabled Specifies whether Amazon S3 should use an S3 Bucket Key for object
#' encryption with server-side encryption using Key Management Service
#' (KMS) keys (SSE-KMS). If a target object uses SSE-KMS, you can enable an
#' S3 Bucket Key for the object.
#' 
#' Setting this header to `true` causes Amazon S3 to use an S3 Bucket Key
#' for object encryption with SSE-KMS. Specifying this header with a COPY
#' action doesn’t affect bucket-level settings for S3 Bucket Key.
#' 
#' For more information, see [Amazon S3 Bucket
#' Keys](https://docs.aws.amazon.com/AmazonS3/latest/userguide/bucket-key.html)
#' in the *Amazon S3 User Guide*.
#' 
#' **Directory buckets** - S3 Bucket Keys aren't supported, when you copy
#' SSE-KMS encrypted objects from general purpose buckets to directory
#' buckets, from directory buckets to general purpose buckets, or between
#' directory buckets, through [`copy_object`][s3_copy_object]. In this
#' case, Amazon S3 makes a call to KMS every time a copy request is made
#' for a KMS-encrypted object.
#' @param CopySourceSSECustomerAlgorithm Specifies the algorithm to use when decrypting the source object (for
#' example, `AES256`).
#' 
#' If the source object for the copy is stored in Amazon S3 using SSE-C,
#' you must provide the necessary encryption information in your request so
#' that Amazon S3 can decrypt the object for copying.
#' 
#' This functionality is not supported when the source object is in a
#' directory bucket.
#' @param CopySourceSSECustomerKey Specifies the customer-provided encryption key for Amazon S3 to use to
#' decrypt the source object. The encryption key provided in this header
#' must be the same one that was used when the source object was created.
#' 
#' If the source object for the copy is stored in Amazon S3 using SSE-C,
#' you must provide the necessary encryption information in your request so
#' that Amazon S3 can decrypt the object for copying.
#' 
#' This functionality is not supported when the source object is in a
#' directory bucket.
#' @param CopySourceSSECustomerKeyMD5 Specifies the 128-bit MD5 digest of the encryption key according to RFC
#' 1321. Amazon S3 uses this header for a message integrity check to ensure
#' that the encryption key was transmitted without error.
#' 
#' If the source object for the copy is stored in Amazon S3 using SSE-C,
#' you must provide the necessary encryption information in your request so
#' that Amazon S3 can decrypt the object for copying.
#' 
#' This functionality is not supported when the source object is in a
#' directory bucket.
#' @param RequestPayer 
#' @param Tagging The tag-set for the object copy in the destination bucket. This value
#' must be used in conjunction with the `x-amz-tagging-directive` if you
#' choose `REPLACE` for the `x-amz-tagging-directive`. If you choose `COPY`
#' for the `x-amz-tagging-directive`, you don't need to set the
#' `x-amz-tagging` header, because the tag-set will be copied from the
#' source object directly. The tag-set must be encoded as URL Query
#' parameters.
#' 
#' The default value is the empty value.
#' 
#' **Directory buckets** - For directory buckets in a
#' [`copy_object`][s3_copy_object] operation, only the empty tag-set is
#' supported. Any requests that attempt to write non-empty tags into
#' directory buckets will receive a `501 Not Implemented` status code. When
#' the destination bucket is a directory bucket, you will receive a
#' `501 Not Implemented` response in any of the following situations:
#' 
#' -   When you attempt to `COPY` the tag-set from an S3 source object that
#'     has non-empty tags.
#' 
#' -   When you attempt to `REPLACE` the tag-set of a source object and set
#'     a non-empty value to `x-amz-tagging`.
#' 
#' -   When you don't set the `x-amz-tagging-directive` header and the
#'     source object has non-empty tags. This is because the default value
#'     of `x-amz-tagging-directive` is `COPY`.
#' 
#' Because only the empty tag-set is supported for directory buckets in a
#' [`copy_object`][s3_copy_object] operation, the following situations are
#' allowed:
#' 
#' -   When you attempt to `COPY` the tag-set from a directory bucket
#'     source object that has no tags to a general purpose bucket. It
#'     copies an empty tag-set to the destination object.
#' 
#' -   When you attempt to `REPLACE` the tag-set of a directory bucket
#'     source object and set the `x-amz-tagging` value of the directory
#'     bucket destination object to empty.
#' 
#' -   When you attempt to `REPLACE` the tag-set of a general purpose
#'     bucket source object that has non-empty tags and set the
#'     `x-amz-tagging` value of the directory bucket destination object to
#'     empty.
#' 
#' -   When you attempt to `REPLACE` the tag-set of a directory bucket
#'     source object and don't set the `x-amz-tagging` value of the
#'     directory bucket destination object. This is because the default
#'     value of `x-amz-tagging` is the empty value.
#' @param ObjectLockMode The Object Lock mode that you want to apply to the object copy.
#' 
#' This functionality is not supported for directory buckets.
#' @param ObjectLockRetainUntilDate The date and time when you want the Object Lock of the object copy to
#' expire.
#' 
#' This functionality is not supported for directory buckets.
#' @param ObjectLockLegalHoldStatus Specifies whether you want to apply a legal hold to the object copy.
#' 
#' This functionality is not supported for directory buckets.
#' @param ExpectedBucketOwner The account ID of the expected destination bucket owner. If the account
#' ID that you provide does not match the actual owner of the destination
#' bucket, the request fails with the HTTP status code `403 Forbidden`
#' (access denied).
#' @param ExpectedSourceBucketOwner The account ID of the expected source bucket owner. If the account ID
#' that you provide does not match the actual owner of the source bucket,
#' the request fails with the HTTP status code `403 Forbidden` (access
#' denied).
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   CopyObjectResult = list(
#'     ETag = "string",
#'     LastModified = as.POSIXct(
#'       "2015-01-01"
#'     ),
#'     ChecksumType = "COMPOSITE"|"FULL_OBJECT",
#'     ChecksumCRC32 = "string",
#'     ChecksumCRC32C = "string",
#'     ChecksumCRC64NVME = "string",
#'     ChecksumSHA1 = "string",
#'     ChecksumSHA256 = "string"
#'   ),
#'   Expiration = "string",
#'   CopySourceVersionId = "string",
#'   VersionId = "string",
#'   ServerSideEncryption = "AES256"|"aws:fsx"|"aws:kms"|"aws:kms:dsse",
#'   SSECustomerAlgorithm = "string",
#'   SSECustomerKeyMD5 = "string",
#'   SSEKMSKeyId = "string",
#'   SSEKMSEncryptionContext = "string",
#'   BucketKeyEnabled = TRUE|FALSE,
#'   RequestCharged = "requester"
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$copy_object(
#'   ACL = "private"|"public-read"|"public-read-write"|"authenticated-read"|"aws-exec-read"|"bucket-owner-read"|"bucket-owner-full-control",
#'   Bucket = "string",
#'   CacheControl = "string",
#'   ChecksumAlgorithm = "CRC32"|"CRC32C"|"SHA1"|"SHA256"|"CRC64NVME",
#'   ContentDisposition = "string",
#'   ContentEncoding = "string",
#'   ContentLanguage = "string",
#'   ContentType = "string",
#'   CopySource = "string",
#'   CopySourceIfMatch = "string",
#'   CopySourceIfModifiedSince = as.POSIXct(
#'     "2015-01-01"
#'   ),
#'   CopySourceIfNoneMatch = "string",
#'   CopySourceIfUnmodifiedSince = as.POSIXct(
#'     "2015-01-01"
#'   ),
#'   Expires = "string",
#'   GrantFullControl = "string",
#'   GrantRead = "string",
#'   GrantReadACP = "string",
#'   GrantWriteACP = "string",
#'   IfMatch = "string",
#'   IfNoneMatch = "string",
#'   Key = "string",
#'   Metadata = list(
#'     "string"
#'   ),
#'   MetadataDirective = "COPY"|"REPLACE",
#'   TaggingDirective = "COPY"|"REPLACE",
#'   ServerSideEncryption = "AES256"|"aws:fsx"|"aws:kms"|"aws:kms:dsse",
#'   StorageClass = "STANDARD"|"REDUCED_REDUNDANCY"|"STANDARD_IA"|"ONEZONE_IA"|"INTELLIGENT_TIERING"|"GLACIER"|"DEEP_ARCHIVE"|"OUTPOSTS"|"GLACIER_IR"|"SNOW"|"EXPRESS_ONEZONE"|"FSX_OPENZFS"|"FSX_ONTAP",
#'   WebsiteRedirectLocation = "string",
#'   SSECustomerAlgorithm = "string",
#'   SSECustomerKey = "string",
#'   SSECustomerKeyMD5 = "string",
#'   SSEKMSKeyId = "string",
#'   SSEKMSEncryptionContext = "string",
#'   BucketKeyEnabled = TRUE|FALSE,
#'   CopySourceSSECustomerAlgorithm = "string",
#'   CopySourceSSECustomerKey = "string",
#'   CopySourceSSECustomerKeyMD5 = "string",
#'   RequestPayer = "requester",
#'   Tagging = "string",
#'   ObjectLockMode = "GOVERNANCE"|"COMPLIANCE",
#'   ObjectLockRetainUntilDate = as.POSIXct(
#'     "2015-01-01"
#'   ),
#'   ObjectLockLegalHoldStatus = "ON"|"OFF",
#'   ExpectedBucketOwner = "string",
#'   ExpectedSourceBucketOwner = "string"
#' )
#' ```
#'
#' @examples
#' \dontrun{
#' # The following example copies an object from one bucket to another.
#' svc$copy_object(
#'   Bucket = "destinationbucket",
#'   CopySource = "/sourcebucket/HappyFacejpg",
#'   Key = "HappyFaceCopyjpg"
#' )
#' }
#'
#' @keywords internal
#'
#' @rdname s3_copy_object
#'
#' @aliases s3_copy_object
s3_copy_object <- function(ACL = NULL, Bucket, CacheControl = NULL, ChecksumAlgorithm = NULL, ContentDisposition = NULL, ContentEncoding = NULL, ContentLanguage = NULL, ContentType = NULL, CopySource, CopySourceIfMatch = NULL, CopySourceIfModifiedSince = NULL, CopySourceIfNoneMatch = NULL, CopySourceIfUnmodifiedSince = NULL, Expires = NULL, GrantFullControl = NULL, GrantRead = NULL, GrantReadACP = NULL, GrantWriteACP = NULL, IfMatch = NULL, IfNoneMatch = NULL, Key, Metadata = NULL, MetadataDirective = NULL, TaggingDirective = NULL, ServerSideEncryption = NULL, StorageClass = NULL, WebsiteRedirectLocation = NULL, SSECustomerAlgorithm = NULL, SSECustomerKey = NULL, SSECustomerKeyMD5 = NULL, SSEKMSKeyId = NULL, SSEKMSEncryptionContext = NULL, BucketKeyEnabled = NULL, CopySourceSSECustomerAlgorithm = NULL, CopySourceSSECustomerKey = NULL, CopySourceSSECustomerKeyMD5 = NULL, RequestPayer = NULL, Tagging = NULL, ObjectLockMode = NULL, ObjectLockRetainUntilDate = NULL, ObjectLockLegalHoldStatus = NULL, ExpectedBucketOwner = NULL, ExpectedSourceBucketOwner = NULL) {
  op <- new_operation(
    name = "CopyObject",
    http_method = "PUT",
    http_path = "/{Bucket}/{Key+}",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .s3$copy_object_input(ACL = ACL, Bucket = Bucket, CacheControl = CacheControl, ChecksumAlgorithm = ChecksumAlgorithm, ContentDisposition = ContentDisposition, ContentEncoding = ContentEncoding, ContentLanguage = ContentLanguage, ContentType = ContentType, CopySource = CopySource, CopySourceIfMatch = CopySourceIfMatch, CopySourceIfModifiedSince = CopySourceIfModifiedSince, CopySourceIfNoneMatch = CopySourceIfNoneMatch, CopySourceIfUnmodifiedSince = CopySourceIfUnmodifiedSince, Expires = Expires, GrantFullControl = GrantFullControl, GrantRead = GrantRead, GrantReadACP = GrantReadACP, GrantWriteACP = GrantWriteACP, IfMatch = IfMatch, IfNoneMatch = IfNoneMatch, Key = Key, Metadata = Metadata, MetadataDirective = MetadataDirective, TaggingDirective = TaggingDirective, ServerSideEncryption = ServerSideEncryption, StorageClass = StorageClass, WebsiteRedirectLocation = WebsiteRedirectLocation, SSECustomerAlgorithm = SSECustomerAlgorithm, SSECustomerKey = SSECustomerKey, SSECustomerKeyMD5 = SSECustomerKeyMD5, SSEKMSKeyId = SSEKMSKeyId, SSEKMSEncryptionContext = SSEKMSEncryptionContext, BucketKeyEnabled = BucketKeyEnabled, CopySourceSSECustomerAlgorithm = CopySourceSSECustomerAlgorithm, CopySourceSSECustomerKey = CopySourceSSECustomerKey, CopySourceSSECustomerKeyMD5 = CopySourceSSECustomerKeyMD5, RequestPayer = RequestPayer, Tagging = Tagging, ObjectLockMode = ObjectLockMode, ObjectLockRetainUntilDate = ObjectLockRetainUntilDate, ObjectLockLegalHoldStatus = ObjectLockLegalHoldStatus, ExpectedBucketOwner = ExpectedBucketOwner, ExpectedSourceBucketOwner = ExpectedSourceBucketOwner)
  output <- .s3$copy_object_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$copy_object <- s3_copy_object

#' This action creates an Amazon S3 bucket
#'
#' @description
#' This action creates an Amazon S3 bucket. To create an Amazon S3 on
#' Outposts bucket, see
#' [`create_bucket`](https://docs.aws.amazon.com/AmazonS3/latest/API/API_control_CreateBucket.html)
#' .
#' 
#' Creates a new S3 bucket. To create a bucket, you must set up Amazon S3
#' and have a valid Amazon Web Services Access Key ID to authenticate
#' requests. Anonymous requests are never allowed to create buckets. By
#' creating the bucket, you become the bucket owner.
#' 
#' There are two types of buckets: general purpose buckets and directory
#' buckets. For more information about these bucket types, see [Creating,
#' configuring, and working with Amazon S3
#' buckets](https://docs.aws.amazon.com/AmazonS3/latest/userguide/creating-buckets-s3.html)
#' in the *Amazon S3 User Guide*.
#' 
#' -   **General purpose buckets** - If you send your
#'     [`create_bucket`][s3_create_bucket] request to the
#'     `s3.amazonaws.com` global endpoint, the request goes to the
#'     `us-east-1` Region. So the signature calculations in Signature
#'     Version 4 must use `us-east-1` as the Region, even if the location
#'     constraint in the request specifies another Region where the bucket
#'     is to be created. If you create a bucket in a Region other than US
#'     East (N. Virginia), your application must be able to handle 307
#'     redirect. For more information, see [Virtual hosting of
#'     buckets](https://docs.aws.amazon.com/AmazonS3/latest/userguide/VirtualHosting.html)
#'     in the *Amazon S3 User Guide*.
#' 
#' -   **Directory buckets** - For directory buckets, you must make
#'     requests for this API operation to the Regional endpoint. These
#'     endpoints support path-style requests in the format
#'     `https://s3express-control.region-code.amazonaws.com/bucket-name `.
#'     Virtual-hosted-style requests aren't supported. For more information
#'     about endpoints in Availability Zones, see [Regional and Zonal
#'     endpoints for directory buckets in Availability
#'     Zones](https://docs.aws.amazon.com/AmazonS3/latest/userguide/endpoint-directory-buckets-AZ.html)
#'     in the *Amazon S3 User Guide*. For more information about endpoints
#'     in Local Zones, see [Concepts for directory buckets in Local
#'     Zones](https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-lzs-for-directory-buckets.html)
#'     in the *Amazon S3 User Guide*.
#' 
#' ### Permissions
#' 
#' -   **General purpose bucket permissions** - In addition to the
#'     `s3:CreateBucket` permission, the following permissions are required
#'     in a policy when your [`create_bucket`][s3_create_bucket] request
#'     includes specific headers:
#' 
#'     -   **Access control lists (ACLs)** - In your
#'         [`create_bucket`][s3_create_bucket] request, if you specify an
#'         access control list (ACL) and set it to `public-read`,
#'         `public-read-write`, `authenticated-read`, or if you explicitly
#'         specify any other custom ACLs, both `s3:CreateBucket` and
#'         `s3:PutBucketAcl` permissions are required. In your
#'         [`create_bucket`][s3_create_bucket] request, if you set the ACL
#'         to `private`, or if you don't specify any ACLs, only the
#'         `s3:CreateBucket` permission is required.
#' 
#'     -   **Object Lock** - In your [`create_bucket`][s3_create_bucket]
#'         request, if you set `x-amz-bucket-object-lock-enabled` to true,
#'         the `s3:PutBucketObjectLockConfiguration` and
#'         `s3:PutBucketVersioning` permissions are required.
#' 
#'     -   **S3 Object Ownership** - If your
#'         [`create_bucket`][s3_create_bucket] request includes the
#'         `x-amz-object-ownership` header, then the
#'         `s3:PutBucketOwnershipControls` permission is required.
#' 
#'         To set an ACL on a bucket as part of a
#'         [`create_bucket`][s3_create_bucket] request, you must explicitly
#'         set S3 Object Ownership for the bucket to a different value than
#'         the default, `BucketOwnerEnforced`. Additionally, if your
#'         desired bucket ACL grants public access, you must first create
#'         the bucket (without the bucket ACL) and then explicitly disable
#'         Block Public Access on the bucket before using
#'         [`put_bucket_acl`][s3_put_bucket_acl] to set the ACL. If you try
#'         to create a bucket with a public ACL, the request will fail.
#' 
#'         For the majority of modern use cases in S3, we recommend that
#'         you keep all Block Public Access settings enabled and keep ACLs
#'         disabled. If you would like to share data with users outside of
#'         your account, you can use bucket policies as needed. For more
#'         information, see [Controlling ownership of objects and disabling
#'         ACLs for your
#'         bucket](https://docs.aws.amazon.com/AmazonS3/latest/userguide/about-object-ownership.html)
#'         and [Blocking public access to your Amazon S3
#'         storage](https://docs.aws.amazon.com/AmazonS3/latest/userguide/access-control-block-public-access.html)
#'         in the *Amazon S3 User Guide*.
#' 
#'     -   **S3 Block Public Access** - If your specific use case requires
#'         granting public access to your S3 resources, you can disable
#'         Block Public Access. Specifically, you can create a new bucket
#'         with Block Public Access enabled, then separately call the
#'         [`delete_public_access_block`](https://docs.aws.amazon.com/AmazonS3/latest/API/API_DeletePublicAccessBlock.html)
#'         API. To use this operation, you must have the
#'         `s3:PutBucketPublicAccessBlock` permission. For more information
#'         about S3 Block Public Access, see [Blocking public access to
#'         your Amazon S3
#'         storage](https://docs.aws.amazon.com/AmazonS3/latest/userguide/access-control-block-public-access.html)
#'         in the *Amazon S3 User Guide*.
#' 
#' -   **Directory bucket permissions** - You must have the
#'     `s3express:CreateBucket` permission in an IAM identity-based policy
#'     instead of a bucket policy. Cross-account access to this API
#'     operation isn't supported. This operation can only be performed by
#'     the Amazon Web Services account that owns the resource. For more
#'     information about directory bucket policies and permissions, see
#'     [Amazon Web Services Identity and Access Management (IAM) for S3
#'     Express One
#'     Zone](https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-express-security-iam.html)
#'     in the *Amazon S3 User Guide*.
#' 
#'     The permissions for ACLs, Object Lock, S3 Object Ownership, and S3
#'     Block Public Access are not supported for directory buckets. For
#'     directory buckets, all Block Public Access settings are enabled at
#'     the bucket level and S3 Object Ownership is set to Bucket owner
#'     enforced (ACLs disabled). These settings can't be modified.
#' 
#'     For more information about permissions for creating and working with
#'     directory buckets, see [Directory
#'     buckets](https://docs.aws.amazon.com/AmazonS3/latest/userguide/directory-buckets-overview.html)
#'     in the *Amazon S3 User Guide*. For more information about supported
#'     S3 features for directory buckets, see [Features of S3 Express One
#'     Zone](https://docs.aws.amazon.com/AmazonS3/latest/userguide/#s3-express-features)
#'     in the *Amazon S3 User Guide*.
#' 
#' ### HTTP Host header syntax
#' 
#' **Directory buckets** - The HTTP Host header syntax is
#' `s3express-control.region-code.amazonaws.com`.
#' 
#' The following operations are related to
#' [`create_bucket`][s3_create_bucket]:
#' 
#' -   [`put_object`][s3_put_object]
#' 
#' -   [`delete_bucket`][s3_delete_bucket]
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#'
#' @usage
#' s3_create_bucket(ACL, Bucket, CreateBucketConfiguration,
#'   GrantFullControl, GrantRead, GrantReadACP, GrantWrite, GrantWriteACP,
#'   ObjectLockEnabledForBucket, ObjectOwnership)
#'
#' @param ACL The canned ACL to apply to the bucket.
#' 
#' This functionality is not supported for directory buckets.
#' @param Bucket &#91;required&#93; The name of the bucket to create.
#' 
#' **General purpose buckets** - For information about bucket naming
#' restrictions, see [Bucket naming
#' rules](https://docs.aws.amazon.com/AmazonS3/latest/userguide/bucketnamingrules.html)
#' in the *Amazon S3 User Guide*.
#' 
#' **Directory buckets** - When you use this operation with a directory
#' bucket, you must use path-style requests in the format
#' `https://s3express-control.region-code.amazonaws.com/bucket-name `.
#' Virtual-hosted-style requests aren't supported. Directory bucket names
#' must be unique in the chosen Zone (Availability Zone or Local Zone).
#' Bucket names must also follow the format
#' ` bucket-base-name--zone-id--x-s3` (for example,
#' ` DOC-EXAMPLE-BUCKET--usw2-az1--x-s3`). For information about bucket
#' naming restrictions, see [Directory bucket naming
#' rules](https://docs.aws.amazon.com/AmazonS3/latest/userguide/directory-bucket-naming-rules.html)
#' in the *Amazon S3 User Guide*
#' @param CreateBucketConfiguration The configuration information for the bucket.
#' @param GrantFullControl Allows grantee the read, write, read ACP, and write ACP permissions on
#' the bucket.
#' 
#' This functionality is not supported for directory buckets.
#' @param GrantRead Allows grantee to list the objects in the bucket.
#' 
#' This functionality is not supported for directory buckets.
#' @param GrantReadACP Allows grantee to read the bucket ACL.
#' 
#' This functionality is not supported for directory buckets.
#' @param GrantWrite Allows grantee to create new objects in the bucket.
#' 
#' For the bucket and object owners of existing objects, also allows
#' deletions and overwrites of those objects.
#' 
#' This functionality is not supported for directory buckets.
#' @param GrantWriteACP Allows grantee to write the ACL for the applicable bucket.
#' 
#' This functionality is not supported for directory buckets.
#' @param ObjectLockEnabledForBucket Specifies whether you want S3 Object Lock to be enabled for the new
#' bucket.
#' 
#' This functionality is not supported for directory buckets.
#' @param ObjectOwnership 
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   Location = "string",
#'   BucketArn = "string"
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$create_bucket(
#'   ACL = "private"|"public-read"|"public-read-write"|"authenticated-read",
#'   Bucket = "string",
#'   CreateBucketConfiguration = list(
#'     LocationConstraint = "af-south-1"|"ap-east-1"|"ap-northeast-1"|"ap-northeast-2"|"ap-northeast-3"|"ap-south-1"|"ap-south-2"|"ap-southeast-1"|"ap-southeast-2"|"ap-southeast-3"|"ap-southeast-4"|"ap-southeast-5"|"ca-central-1"|"cn-north-1"|"cn-northwest-1"|"EU"|"eu-central-1"|"eu-central-2"|"eu-north-1"|"eu-south-1"|"eu-south-2"|"eu-west-1"|"eu-west-2"|"eu-west-3"|"il-central-1"|"me-central-1"|"me-south-1"|"sa-east-1"|"us-east-2"|"us-gov-east-1"|"us-gov-west-1"|"us-west-1"|"us-west-2",
#'     Location = list(
#'       Type = "AvailabilityZone"|"LocalZone",
#'       Name = "string"
#'     ),
#'     Bucket = list(
#'       DataRedundancy = "SingleAvailabilityZone"|"SingleLocalZone",
#'       Type = "Directory"
#'     ),
#'     Tags = list(
#'       list(
#'         Key = "string",
#'         Value = "string"
#'       )
#'     )
#'   ),
#'   GrantFullControl = "string",
#'   GrantRead = "string",
#'   GrantReadACP = "string",
#'   GrantWrite = "string",
#'   GrantWriteACP = "string",
#'   ObjectLockEnabledForBucket = TRUE|FALSE,
#'   ObjectOwnership = "BucketOwnerPreferred"|"ObjectWriter"|"BucketOwnerEnforced"
#' )
#' ```
#'
#' @examples
#' \dontrun{
#' # The following example creates a bucket. The request specifies an AWS
#' # region where to create the bucket.
#' svc$create_bucket(
#'   Bucket = "examplebucket",
#'   CreateBucketConfiguration = list(
#'     LocationConstraint = "eu-west-1"
#'   )
#' )
#' 
#' # The following example creates a bucket.
#' svc$create_bucket(
#'   Bucket = "examplebucket"
#' )
#' }
#'
#' @keywords internal
#'
#' @rdname s3_create_bucket
#'
#' @aliases s3_create_bucket
s3_create_bucket <- function(ACL = NULL, Bucket, CreateBucketConfiguration = NULL, GrantFullControl = NULL, GrantRead = NULL, GrantReadACP = NULL, GrantWrite = NULL, GrantWriteACP = NULL, ObjectLockEnabledForBucket = NULL, ObjectOwnership = NULL) {
  op <- new_operation(
    name = "CreateBucket",
    http_method = "PUT",
    http_path = "/{Bucket}",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .s3$create_bucket_input(ACL = ACL, Bucket = Bucket, CreateBucketConfiguration = CreateBucketConfiguration, GrantFullControl = GrantFullControl, GrantRead = GrantRead, GrantReadACP = GrantReadACP, GrantWrite = GrantWrite, GrantWriteACP = GrantWriteACP, ObjectLockEnabledForBucket = ObjectLockEnabledForBucket, ObjectOwnership = ObjectOwnership)
  output <- .s3$create_bucket_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$create_bucket <- s3_create_bucket

#' Creates an S3 Metadata V2 metadata configuration for a general purpose
#' bucket
#'
#' @description
#' Creates an S3 Metadata V2 metadata configuration for a general purpose
#' bucket. For more information, see [Accelerating data discovery with S3
#' Metadata](https://docs.aws.amazon.com/AmazonS3/latest/userguide/metadata-tables-overview.html)
#' in the *Amazon S3 User Guide*.
#' 
#' ### Permissions
#' 
#' To use this operation, you must have the following permissions. For more
#' information, see [Setting up permissions for configuring metadata
#' tables](https://docs.aws.amazon.com/AmazonS3/latest/userguide/metadata-tables-permissions.html)
#' in the *Amazon S3 User Guide*.
#' 
#' If you want to encrypt your metadata tables with server-side encryption
#' with Key Management Service (KMS) keys (SSE-KMS), you need additional
#' permissions in your KMS key policy. For more information, see [Setting
#' up permissions for configuring metadata
#' tables](https://docs.aws.amazon.com/AmazonS3/latest/userguide/metadata-tables-permissions.html)
#' in the *Amazon S3 User Guide*.
#' 
#' If you also want to integrate your table bucket with Amazon Web Services
#' analytics services so that you can query your metadata table, you need
#' additional permissions. For more information, see [Integrating Amazon S3
#' Tables with Amazon Web Services analytics
#' services](https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-tables-integrating-aws.html)
#' in the *Amazon S3 User Guide*.
#' 
#' To query your metadata tables, you need additional permissions. For more
#' information, see [Permissions for querying metadata
#' tables](https://docs.aws.amazon.com/AmazonS3/latest/userguide/metadata-tables-bucket-query-permissions.html)
#' in the *Amazon S3 User Guide*.
#' 
#' -   `s3:CreateBucketMetadataTableConfiguration`
#' 
#'     The IAM policy action name is the same for the V1 and V2 API
#'     operations.
#' 
#' -   `s3tables:CreateTableBucket`
#' 
#' -   `s3tables:CreateNamespace`
#' 
#' -   `s3tables:GetTable`
#' 
#' -   `s3tables:CreateTable`
#' 
#' -   `s3tables:PutTablePolicy`
#' 
#' -   `s3tables:PutTableEncryption`
#' 
#' -   `kms:DescribeKey`
#' 
#' The following operations are related to
#' [`create_bucket_metadata_configuration`][s3_create_bucket_metadata_configuration]:
#' 
#' -   [`delete_bucket_metadata_configuration`][s3_delete_bucket_metadata_configuration]
#' 
#' -   [`get_bucket_metadata_configuration`][s3_get_bucket_metadata_configuration]
#' 
#' -   [`update_bucket_metadata_inventory_table_configuration`][s3_update_bucket_metadata_inventory_table_configuration]
#' 
#' -   [`update_bucket_metadata_journal_table_configuration`][s3_update_bucket_metadata_journal_table_configuration]
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#'
#' @usage
#' s3_create_bucket_metadata_configuration(Bucket, ContentMD5,
#'   ChecksumAlgorithm, MetadataConfiguration, ExpectedBucketOwner)
#'
#' @param Bucket &#91;required&#93; The general purpose bucket that you want to create the metadata
#' configuration for.
#' @param ContentMD5 The `Content-MD5` header for the metadata configuration.
#' @param ChecksumAlgorithm The checksum algorithm to use with your metadata configuration.
#' @param MetadataConfiguration &#91;required&#93; The contents of your metadata configuration.
#' @param ExpectedBucketOwner The expected owner of the general purpose bucket that corresponds to
#' your metadata configuration.
#'
#' @return
#' An empty list.
#'
#' @section Request syntax:
#' ```
#' svc$create_bucket_metadata_configuration(
#'   Bucket = "string",
#'   ContentMD5 = "string",
#'   ChecksumAlgorithm = "CRC32"|"CRC32C"|"SHA1"|"SHA256"|"CRC64NVME",
#'   MetadataConfiguration = list(
#'     JournalTableConfiguration = list(
#'       RecordExpiration = list(
#'         Expiration = "ENABLED"|"DISABLED",
#'         Days = 123
#'       ),
#'       EncryptionConfiguration = list(
#'         SseAlgorithm = "aws:kms"|"AES256",
#'         KmsKeyArn = "string"
#'       )
#'     ),
#'     InventoryTableConfiguration = list(
#'       ConfigurationState = "ENABLED"|"DISABLED",
#'       EncryptionConfiguration = list(
#'         SseAlgorithm = "aws:kms"|"AES256",
#'         KmsKeyArn = "string"
#'       )
#'     )
#'   ),
#'   ExpectedBucketOwner = "string"
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname s3_create_bucket_metadata_configuration
#'
#' @aliases s3_create_bucket_metadata_configuration
s3_create_bucket_metadata_configuration <- function(Bucket, ContentMD5 = NULL, ChecksumAlgorithm = NULL, MetadataConfiguration, ExpectedBucketOwner = NULL) {
  op <- new_operation(
    name = "CreateBucketMetadataConfiguration",
    http_method = "POST",
    http_path = "/{Bucket}?metadataConfiguration",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .s3$create_bucket_metadata_configuration_input(Bucket = Bucket, ContentMD5 = ContentMD5, ChecksumAlgorithm = ChecksumAlgorithm, MetadataConfiguration = MetadataConfiguration, ExpectedBucketOwner = ExpectedBucketOwner)
  output <- .s3$create_bucket_metadata_configuration_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$create_bucket_metadata_configuration <- s3_create_bucket_metadata_configuration

#' We recommend that you create your S3 Metadata configurations by using
#' the V2 CreateBucketMetadataConfiguration API operation
#'
#' @description
#' We recommend that you create your S3 Metadata configurations by using
#' the V2
#' [`create_bucket_metadata_configuration`][s3_create_bucket_metadata_configuration]
#' API operation. We no longer recommend using the V1
#' [`create_bucket_metadata_table_configuration`][s3_create_bucket_metadata_table_configuration]
#' API operation.
#' 
#' If you created your S3 Metadata configuration before July 15, 2025, we
#' recommend that you delete and re-create your configuration by using
#' [`create_bucket_metadata_configuration`][s3_create_bucket_metadata_configuration]
#' so that you can expire journal table records and create a live inventory
#' table.
#' 
#' Creates a V1 S3 Metadata configuration for a general purpose bucket. For
#' more information, see [Accelerating data discovery with S3
#' Metadata](https://docs.aws.amazon.com/AmazonS3/latest/userguide/metadata-tables-overview.html)
#' in the *Amazon S3 User Guide*.
#' 
#' ### Permissions
#' 
#' To use this operation, you must have the following permissions. For more
#' information, see [Setting up permissions for configuring metadata
#' tables](https://docs.aws.amazon.com/AmazonS3/latest/userguide/metadata-tables-permissions.html)
#' in the *Amazon S3 User Guide*.
#' 
#' If you want to encrypt your metadata tables with server-side encryption
#' with Key Management Service (KMS) keys (SSE-KMS), you need additional
#' permissions. For more information, see [Setting up permissions for
#' configuring metadata
#' tables](https://docs.aws.amazon.com/AmazonS3/latest/userguide/metadata-tables-permissions.html)
#' in the *Amazon S3 User Guide*.
#' 
#' If you also want to integrate your table bucket with Amazon Web Services
#' analytics services so that you can query your metadata table, you need
#' additional permissions. For more information, see [Integrating Amazon S3
#' Tables with Amazon Web Services analytics
#' services](https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-tables-integrating-aws.html)
#' in the *Amazon S3 User Guide*.
#' 
#' -   `s3:CreateBucketMetadataTableConfiguration`
#' 
#' -   `s3tables:CreateNamespace`
#' 
#' -   `s3tables:GetTable`
#' 
#' -   `s3tables:CreateTable`
#' 
#' -   `s3tables:PutTablePolicy`
#' 
#' The following operations are related to
#' [`create_bucket_metadata_table_configuration`][s3_create_bucket_metadata_table_configuration]:
#' 
#' -   [`delete_bucket_metadata_table_configuration`][s3_delete_bucket_metadata_table_configuration]
#' 
#' -   [`get_bucket_metadata_table_configuration`][s3_get_bucket_metadata_table_configuration]
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#'
#' @usage
#' s3_create_bucket_metadata_table_configuration(Bucket, ContentMD5,
#'   ChecksumAlgorithm, MetadataTableConfiguration, ExpectedBucketOwner)
#'
#' @param Bucket &#91;required&#93; The general purpose bucket that you want to create the metadata table
#' configuration for.
#' @param ContentMD5 The `Content-MD5` header for the metadata table configuration.
#' @param ChecksumAlgorithm The checksum algorithm to use with your metadata table configuration.
#' @param MetadataTableConfiguration &#91;required&#93; The contents of your metadata table configuration.
#' @param ExpectedBucketOwner The expected owner of the general purpose bucket that corresponds to
#' your metadata table configuration.
#'
#' @return
#' An empty list.
#'
#' @section Request syntax:
#' ```
#' svc$create_bucket_metadata_table_configuration(
#'   Bucket = "string",
#'   ContentMD5 = "string",
#'   ChecksumAlgorithm = "CRC32"|"CRC32C"|"SHA1"|"SHA256"|"CRC64NVME",
#'   MetadataTableConfiguration = list(
#'     S3TablesDestination = list(
#'       TableBucketArn = "string",
#'       TableName = "string"
#'     )
#'   ),
#'   ExpectedBucketOwner = "string"
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname s3_create_bucket_metadata_table_configuration
#'
#' @aliases s3_create_bucket_metadata_table_configuration
s3_create_bucket_metadata_table_configuration <- function(Bucket, ContentMD5 = NULL, ChecksumAlgorithm = NULL, MetadataTableConfiguration, ExpectedBucketOwner = NULL) {
  op <- new_operation(
    name = "CreateBucketMetadataTableConfiguration",
    http_method = "POST",
    http_path = "/{Bucket}?metadataTable",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .s3$create_bucket_metadata_table_configuration_input(Bucket = Bucket, ContentMD5 = ContentMD5, ChecksumAlgorithm = ChecksumAlgorithm, MetadataTableConfiguration = MetadataTableConfiguration, ExpectedBucketOwner = ExpectedBucketOwner)
  output <- .s3$create_bucket_metadata_table_configuration_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$create_bucket_metadata_table_configuration <- s3_create_bucket_metadata_table_configuration

#' End of support notice: As of October 1, 2025, Amazon S3 has discontinued
#' support for Email Grantee Access Control Lists (ACLs)
#'
#' @description
#' End of support notice: As of October 1, 2025, Amazon S3 has discontinued
#' support for Email Grantee Access Control Lists (ACLs). If you attempt to
#' use an Email Grantee ACL in a request after October 1, 2025, the request
#' will receive an `HTTP 405` (Method Not Allowed) error.
#' 
#' This change affects the following Amazon Web Services Regions: US East
#' (N. Virginia), US West (N. California), US West (Oregon), Asia Pacific
#' (Singapore), Asia Pacific (Sydney), Asia Pacific (Tokyo), Europe
#' (Ireland), and South America (São Paulo).
#' 
#' This action initiates a multipart upload and returns an upload ID. This
#' upload ID is used to associate all of the parts in the specific
#' multipart upload. You specify this upload ID in each of your subsequent
#' upload part requests (see [`upload_part`][s3_upload_part]). You also
#' include this upload ID in the final request to either complete or abort
#' the multipart upload request. For more information about multipart
#' uploads, see [Multipart Upload
#' Overview](https://docs.aws.amazon.com/AmazonS3/latest/userguide/mpuoverview.html)
#' in the *Amazon S3 User Guide*.
#' 
#' After you initiate a multipart upload and upload one or more parts, to
#' stop being charged for storing the uploaded parts, you must either
#' complete or abort the multipart upload. Amazon S3 frees up the space
#' used to store the parts and stops charging you for storing them only
#' after you either complete or abort a multipart upload.
#' 
#' If you have configured a lifecycle rule to abort incomplete multipart
#' uploads, the created multipart upload must be completed within the
#' number of days specified in the bucket lifecycle configuration.
#' Otherwise, the incomplete multipart upload becomes eligible for an abort
#' action and Amazon S3 aborts the multipart upload. For more information,
#' see [Aborting Incomplete Multipart Uploads Using a Bucket Lifecycle
#' Configuration](https://docs.aws.amazon.com/AmazonS3/latest/userguide/mpuoverview.html#mpu-abort-incomplete-mpu-lifecycle-config).
#' 
#' -   **Directory buckets** - S3 Lifecycle is not supported by directory
#'     buckets.
#' 
#' -   **Directory buckets** - For directory buckets, you must make
#'     requests for this API operation to the Zonal endpoint. These
#'     endpoints support virtual-hosted-style requests in the format
#'     `https://amzn-s3-demo-bucket.s3express-zone-id.region-code.amazonaws.com/key-name `.
#'     Path-style requests are not supported. For more information about
#'     endpoints in Availability Zones, see [Regional and Zonal endpoints
#'     for directory buckets in Availability
#'     Zones](https://docs.aws.amazon.com/AmazonS3/latest/userguide/endpoint-directory-buckets-AZ.html)
#'     in the *Amazon S3 User Guide*. For more information about endpoints
#'     in Local Zones, see [Concepts for directory buckets in Local
#'     Zones](https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-lzs-for-directory-buckets.html)
#'     in the *Amazon S3 User Guide*.
#' 
#' ### Request signing
#' 
#' For request signing, multipart upload is just a series of regular
#' requests. You initiate a multipart upload, send one or more requests to
#' upload parts, and then complete the multipart upload process. You sign
#' each request individually. There is nothing special about signing
#' multipart upload requests. For more information about signing, see
#' [Authenticating Requests (Amazon Web Services Signature Version
#' 4)](https://docs.aws.amazon.com/AmazonS3/latest/API/sig-v4-authenticating-requests.html)
#' in the *Amazon S3 User Guide*.
#' 
#' ### Permissions
#' 
#' -   **General purpose bucket permissions** - To perform a multipart
#'     upload with encryption using an Key Management Service (KMS) KMS
#'     key, the requester must have permission to the `kms:Decrypt` and
#'     `kms:GenerateDataKey` actions on the key. The requester must also
#'     have permissions for the `kms:GenerateDataKey` action for the
#'     [`create_multipart_upload`][s3_create_multipart_upload] API. Then,
#'     the requester needs permissions for the `kms:Decrypt` action on the
#'     [`upload_part`][s3_upload_part] and
#'     [`upload_part_copy`][s3_upload_part_copy] APIs. These permissions
#'     are required because Amazon S3 must decrypt and read data from the
#'     encrypted file parts before it completes the multipart upload. For
#'     more information, see [Multipart upload API and
#'     permissions](https://docs.aws.amazon.com/AmazonS3/latest/userguide/mpuoverview.html#mpuAndPermissions)
#'     and [Protecting data using server-side encryption with Amazon Web
#'     Services
#'     KMS](https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingKMSEncryption.html)
#'     in the *Amazon S3 User Guide*.
#' 
#' -   **Directory bucket permissions** - To grant access to this API
#'     operation on a directory bucket, we recommend that you use the
#'     [`create_session`](https://docs.aws.amazon.com/AmazonS3/latest/API/API_CreateSession.html)
#'     API operation for session-based authorization. Specifically, you
#'     grant the `s3express:CreateSession` permission to the directory
#'     bucket in a bucket policy or an IAM identity-based policy. Then, you
#'     make the [`create_session`][s3_create_session] API call on the
#'     bucket to obtain a session token. With the session token in your
#'     request header, you can make API requests to this operation. After
#'     the session token expires, you make another
#'     [`create_session`][s3_create_session] API call to generate a new
#'     session token for use. Amazon Web Services CLI or SDKs create
#'     session and refresh the session token automatically to avoid service
#'     interruptions when a session expires. For more information about
#'     authorization, see
#'     [`create_session`](https://docs.aws.amazon.com/AmazonS3/latest/API/API_CreateSession.html)
#'     .
#' 
#' ### Encryption
#' 
#' -   **General purpose buckets** - Server-side encryption is for data
#'     encryption at rest. Amazon S3 encrypts your data as it writes it to
#'     disks in its data centers and decrypts it when you access it. Amazon
#'     S3 automatically encrypts all new objects that are uploaded to an S3
#'     bucket. When doing a multipart upload, if you don't specify
#'     encryption information in your request, the encryption setting of
#'     the uploaded parts is set to the default encryption configuration of
#'     the destination bucket. By default, all buckets have a base level of
#'     encryption configuration that uses server-side encryption with
#'     Amazon S3 managed keys (SSE-S3). If the destination bucket has a
#'     default encryption configuration that uses server-side encryption
#'     with an Key Management Service (KMS) key (SSE-KMS), or a
#'     customer-provided encryption key (SSE-C), Amazon S3 uses the
#'     corresponding KMS key, or a customer-provided key to encrypt the
#'     uploaded parts. When you perform a CreateMultipartUpload operation,
#'     if you want to use a different type of encryption setting for the
#'     uploaded parts, you can request that Amazon S3 encrypts the object
#'     with a different encryption key (such as an Amazon S3 managed key, a
#'     KMS key, or a customer-provided key). When the encryption setting in
#'     your request is different from the default encryption configuration
#'     of the destination bucket, the encryption setting in your request
#'     takes precedence. If you choose to provide your own encryption key,
#'     the request headers you provide in [`upload_part`][s3_upload_part]
#'     and [`upload_part_copy`][s3_upload_part_copy] requests must match
#'     the headers you used in the
#'     [`create_multipart_upload`][s3_create_multipart_upload] request.
#' 
#'     -   Use KMS keys (SSE-KMS) that include the Amazon Web Services
#'         managed key (`aws/s3`) and KMS customer managed keys stored in
#'         Key Management Service (KMS) – If you want Amazon Web Services
#'         to manage the keys used to encrypt data, specify the following
#'         headers in the request.
#' 
#'         -   `x-amz-server-side-encryption`
#' 
#'         -   `x-amz-server-side-encryption-aws-kms-key-id`
#' 
#'         -   `x-amz-server-side-encryption-context`
#' 
#'         <!-- -->
#' 
#'         -   If you specify `x-amz-server-side-encryption:aws:kms`, but
#'             don't provide `x-amz-server-side-encryption-aws-kms-key-id`,
#'             Amazon S3 uses the Amazon Web Services managed key (`aws/s3`
#'             key) in KMS to protect the data.
#' 
#'         -   To perform a multipart upload with encryption by using an
#'             Amazon Web Services KMS key, the requester must have
#'             permission to the `kms:Decrypt` and `kms:GenerateDataKey*`
#'             actions on the key. These permissions are required because
#'             Amazon S3 must decrypt and read data from the encrypted file
#'             parts before it completes the multipart upload. For more
#'             information, see [Multipart upload API and
#'             permissions](https://docs.aws.amazon.com/AmazonS3/latest/userguide/mpuoverview.html#mpuAndPermissions)
#'             and [Protecting data using server-side encryption with
#'             Amazon Web Services
#'             KMS](https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingKMSEncryption.html)
#'             in the *Amazon S3 User Guide*.
#' 
#'         -   If your Identity and Access Management (IAM) user or role is
#'             in the same Amazon Web Services account as the KMS key, then
#'             you must have these permissions on the key policy. If your
#'             IAM user or role is in a different account from the key,
#'             then you must have the permissions on both the key policy
#'             and your IAM user or role.
#' 
#'         -   All `GET` and `PUT` requests for an object protected by KMS
#'             fail if you don't make them by using Secure Sockets Layer
#'             (SSL), Transport Layer Security (TLS), or Signature
#'             Version 4. For information about configuring any of the
#'             officially supported Amazon Web Services SDKs and Amazon Web
#'             Services CLI, see [Specifying the Signature Version in
#'             Request
#'             Authentication](https://docs.aws.amazon.com/AmazonS3/latest/API/#specify-signature-version)
#'             in the *Amazon S3 User Guide*.
#' 
#'         For more information about server-side encryption with KMS keys
#'         (SSE-KMS), see [Protecting Data Using Server-Side Encryption
#'         with KMS
#'         keys](https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingKMSEncryption.html)
#'         in the *Amazon S3 User Guide*.
#' 
#'     -   Use customer-provided encryption keys (SSE-C) – If you want to
#'         manage your own encryption keys, provide all the following
#'         headers in the request.
#' 
#'         -   `x-amz-server-side-encryption-customer-algorithm`
#' 
#'         -   `x-amz-server-side-encryption-customer-key`
#' 
#'         -   `x-amz-server-side-encryption-customer-key-MD5`
#' 
#'         For more information about server-side encryption with
#'         customer-provided encryption keys (SSE-C), see [Protecting data
#'         using server-side encryption with customer-provided encryption
#'         keys
#'         (SSE-C)](https://docs.aws.amazon.com/AmazonS3/latest/userguide/ServerSideEncryptionCustomerKeys.html)
#'         in the *Amazon S3 User Guide*.
#' 
#' -   **Directory buckets** - For directory buckets, there are only two
#'     supported options for server-side encryption: server-side encryption
#'     with Amazon S3 managed keys (SSE-S3) (`AES256`) and server-side
#'     encryption with KMS keys (SSE-KMS) (`aws:kms`). We recommend that
#'     the bucket's default encryption uses the desired encryption
#'     configuration and you don't override the bucket default encryption
#'     in your [`create_session`][s3_create_session] requests or `PUT`
#'     object requests. Then, new objects are automatically encrypted with
#'     the desired encryption settings. For more information, see
#'     [Protecting data with server-side
#'     encryption](https://docs.aws.amazon.com/AmazonS3/latest/userguide/)
#'     in the *Amazon S3 User Guide*. For more information about the
#'     encryption overriding behaviors in directory buckets, see
#'     [Specifying server-side encryption with KMS for new object
#'     uploads](https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-express-specifying-kms-encryption.html).
#' 
#'     In the Zonal endpoint API calls (except
#'     [`copy_object`][s3_copy_object] and
#'     [`upload_part_copy`][s3_upload_part_copy]) using the REST API, the
#'     encryption request headers must match the encryption settings that
#'     are specified in the [`create_session`][s3_create_session] request.
#'     You can't override the values of the encryption settings
#'     (`x-amz-server-side-encryption`,
#'     `x-amz-server-side-encryption-aws-kms-key-id`,
#'     `x-amz-server-side-encryption-context`, and
#'     `x-amz-server-side-encryption-bucket-key-enabled`) that are
#'     specified in the [`create_session`][s3_create_session] request. You
#'     don't need to explicitly specify these encryption settings values in
#'     Zonal endpoint API calls, and Amazon S3 will use the encryption
#'     settings values from the [`create_session`][s3_create_session]
#'     request to protect new objects in the directory bucket.
#' 
#'     When you use the CLI or the Amazon Web Services SDKs, for
#'     [`create_session`][s3_create_session], the session token refreshes
#'     automatically to avoid service interruptions when a session expires.
#'     The CLI or the Amazon Web Services SDKs use the bucket's default
#'     encryption configuration for the
#'     [`create_session`][s3_create_session] request. It's not supported to
#'     override the encryption settings values in the
#'     [`create_session`][s3_create_session] request. So in the Zonal
#'     endpoint API calls (except [`copy_object`][s3_copy_object] and
#'     [`upload_part_copy`][s3_upload_part_copy]), the encryption request
#'     headers must match the default encryption configuration of the
#'     directory bucket.
#' 
#'     For directory buckets, when you perform a
#'     [`create_multipart_upload`][s3_create_multipart_upload] operation
#'     and an [`upload_part_copy`][s3_upload_part_copy] operation, the
#'     request headers you provide in the
#'     [`create_multipart_upload`][s3_create_multipart_upload] request must
#'     match the default encryption configuration of the destination
#'     bucket.
#' 
#' ### HTTP Host header syntax
#' 
#' **Directory buckets** - The HTTP Host header syntax is
#' ` Bucket-name.s3express-zone-id.region-code.amazonaws.com`.
#' 
#' The following operations are related to
#' [`create_multipart_upload`][s3_create_multipart_upload]:
#' 
#' -   [`upload_part`][s3_upload_part]
#' 
#' -   [`complete_multipart_upload`][s3_complete_multipart_upload]
#' 
#' -   [`abort_multipart_upload`][s3_abort_multipart_upload]
#' 
#' -   [`list_parts`][s3_list_parts]
#' 
#' -   [`list_multipart_uploads`][s3_list_multipart_uploads]
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#'
#' @usage
#' s3_create_multipart_upload(ACL, Bucket, CacheControl,
#'   ContentDisposition, ContentEncoding, ContentLanguage, ContentType,
#'   Expires, GrantFullControl, GrantRead, GrantReadACP, GrantWriteACP, Key,
#'   Metadata, ServerSideEncryption, StorageClass, WebsiteRedirectLocation,
#'   SSECustomerAlgorithm, SSECustomerKey, SSECustomerKeyMD5, SSEKMSKeyId,
#'   SSEKMSEncryptionContext, BucketKeyEnabled, RequestPayer, Tagging,
#'   ObjectLockMode, ObjectLockRetainUntilDate, ObjectLockLegalHoldStatus,
#'   ExpectedBucketOwner, ChecksumAlgorithm, ChecksumType)
#'
#' @param ACL The canned ACL to apply to the object. Amazon S3 supports a set of
#' predefined ACLs, known as *canned ACLs*. Each canned ACL has a
#' predefined set of grantees and permissions. For more information, see
#' [Canned
#' ACL](https://docs.aws.amazon.com/AmazonS3/latest/userguide/acl-overview.html#CannedACL)
#' in the *Amazon S3 User Guide*.
#' 
#' By default, all objects are private. Only the owner has full access
#' control. When uploading an object, you can grant access permissions to
#' individual Amazon Web Services accounts or to predefined groups defined
#' by Amazon S3. These permissions are then added to the access control
#' list (ACL) on the new object. For more information, see [Using
#' ACLs](https://docs.aws.amazon.com/AmazonS3/latest/userguide/acl-overview.html).
#' One way to grant the permissions using the request headers is to specify
#' a canned ACL with the `x-amz-acl` request header.
#' 
#' -   This functionality is not supported for directory buckets.
#' 
#' -   This functionality is not supported for Amazon S3 on Outposts.
#' @param Bucket &#91;required&#93; The name of the bucket where the multipart upload is initiated and where
#' the object is uploaded.
#' 
#' **Directory buckets** - When you use this operation with a directory
#' bucket, you must use virtual-hosted-style requests in the format
#' ` Bucket-name.s3express-zone-id.region-code.amazonaws.com`. Path-style
#' requests are not supported. Directory bucket names must be unique in the
#' chosen Zone (Availability Zone or Local Zone). Bucket names must follow
#' the format ` bucket-base-name--zone-id--x-s3` (for example,
#' ` amzn-s3-demo-bucket--usw2-az1--x-s3`). For information about bucket
#' naming restrictions, see [Directory bucket naming
#' rules](https://docs.aws.amazon.com/AmazonS3/latest/userguide/directory-bucket-naming-rules.html)
#' in the *Amazon S3 User Guide*.
#' 
#' **Access points** - When you use this action with an access point for
#' general purpose buckets, you must provide the alias of the access point
#' in place of the bucket name or specify the access point ARN. When you
#' use this action with an access point for directory buckets, you must
#' provide the access point name in place of the bucket name. When using
#' the access point ARN, you must direct requests to the access point
#' hostname. The access point hostname takes the form
#' *AccessPointName*-*AccountId*.s3-accesspoint.*Region*.amazonaws.com.
#' When using this action with an access point through the Amazon Web
#' Services SDKs, you provide the access point ARN in place of the bucket
#' name. For more information about access point ARNs, see [Using access
#' points](https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-access-points.html)
#' in the *Amazon S3 User Guide*.
#' 
#' Object Lambda access points are not supported by directory buckets.
#' 
#' **S3 on Outposts** - When you use this action with S3 on Outposts, you
#' must direct requests to the S3 on Outposts hostname. The S3 on Outposts
#' hostname takes the form
#' ` AccessPointName-AccountId.outpostID.s3-outposts.Region.amazonaws.com`.
#' When you use this action with S3 on Outposts, the destination bucket
#' must be the Outposts access point ARN or the access point alias. For
#' more information about S3 on Outposts, see [What is S3 on
#' Outposts?](https://docs.aws.amazon.com/AmazonS3/latest/s3-outposts/S3onOutposts.html)
#' in the *Amazon S3 User Guide*.
#' @param CacheControl Specifies caching behavior along the request/reply chain.
#' @param ContentDisposition Specifies presentational information for the object.
#' @param ContentEncoding Specifies what content encodings have been applied to the object and
#' thus what decoding mechanisms must be applied to obtain the media-type
#' referenced by the Content-Type header field.
#' 
#' For directory buckets, only the `aws-chunked` value is supported in this
#' header field.
#' @param ContentLanguage The language that the content is in.
#' @param ContentType A standard MIME type describing the format of the object data.
#' @param Expires The date and time at which the object is no longer cacheable.
#' @param GrantFullControl Specify access permissions explicitly to give the grantee READ,
#' READ_ACP, and WRITE_ACP permissions on the object.
#' 
#' By default, all objects are private. Only the owner has full access
#' control. When uploading an object, you can use this header to explicitly
#' grant access permissions to specific Amazon Web Services accounts or
#' groups. This header maps to specific permissions that Amazon S3 supports
#' in an ACL. For more information, see [Access Control List (ACL)
#' Overview](https://docs.aws.amazon.com/AmazonS3/latest/userguide/acl-overview.html)
#' in the *Amazon S3 User Guide*.
#' 
#' You specify each grantee as a type=value pair, where the type is one of
#' the following:
#' 
#' -   `id` – if the value specified is the canonical user ID of an Amazon
#'     Web Services account
#' 
#' -   `uri` – if you are granting permissions to a predefined group
#' 
#' -   `emailAddress` – if the value specified is the email address of an
#'     Amazon Web Services account
#' 
#'     Using email addresses to specify a grantee is only supported in the
#'     following Amazon Web Services Regions:
#' 
#'     -   US East (N. Virginia)
#' 
#'     -   US West (N. California)
#' 
#'     -   US West (Oregon)
#' 
#'     -   Asia Pacific (Singapore)
#' 
#'     -   Asia Pacific (Sydney)
#' 
#'     -   Asia Pacific (Tokyo)
#' 
#'     -   Europe (Ireland)
#' 
#'     -   South America (São Paulo)
#' 
#'     For a list of all the Amazon S3 supported Regions and endpoints, see
#'     [Regions and
#'     Endpoints](https://docs.aws.amazon.com/general/latest/gr/rande.html#s3_region)
#'     in the Amazon Web Services General Reference.
#' 
#' For example, the following `x-amz-grant-read` header grants the Amazon
#' Web Services accounts identified by account IDs permissions to read
#' object data and its metadata:
#' 
#' `x-amz-grant-read: id="11112222333", id="444455556666" `
#' 
#' -   This functionality is not supported for directory buckets.
#' 
#' -   This functionality is not supported for Amazon S3 on Outposts.
#' @param GrantRead Specify access permissions explicitly to allow grantee to read the
#' object data and its metadata.
#' 
#' By default, all objects are private. Only the owner has full access
#' control. When uploading an object, you can use this header to explicitly
#' grant access permissions to specific Amazon Web Services accounts or
#' groups. This header maps to specific permissions that Amazon S3 supports
#' in an ACL. For more information, see [Access Control List (ACL)
#' Overview](https://docs.aws.amazon.com/AmazonS3/latest/userguide/acl-overview.html)
#' in the *Amazon S3 User Guide*.
#' 
#' You specify each grantee as a type=value pair, where the type is one of
#' the following:
#' 
#' -   `id` – if the value specified is the canonical user ID of an Amazon
#'     Web Services account
#' 
#' -   `uri` – if you are granting permissions to a predefined group
#' 
#' -   `emailAddress` – if the value specified is the email address of an
#'     Amazon Web Services account
#' 
#'     Using email addresses to specify a grantee is only supported in the
#'     following Amazon Web Services Regions:
#' 
#'     -   US East (N. Virginia)
#' 
#'     -   US West (N. California)
#' 
#'     -   US West (Oregon)
#' 
#'     -   Asia Pacific (Singapore)
#' 
#'     -   Asia Pacific (Sydney)
#' 
#'     -   Asia Pacific (Tokyo)
#' 
#'     -   Europe (Ireland)
#' 
#'     -   South America (São Paulo)
#' 
#'     For a list of all the Amazon S3 supported Regions and endpoints, see
#'     [Regions and
#'     Endpoints](https://docs.aws.amazon.com/general/latest/gr/rande.html#s3_region)
#'     in the Amazon Web Services General Reference.
#' 
#' For example, the following `x-amz-grant-read` header grants the Amazon
#' Web Services accounts identified by account IDs permissions to read
#' object data and its metadata:
#' 
#' `x-amz-grant-read: id="11112222333", id="444455556666" `
#' 
#' -   This functionality is not supported for directory buckets.
#' 
#' -   This functionality is not supported for Amazon S3 on Outposts.
#' @param GrantReadACP Specify access permissions explicitly to allows grantee to read the
#' object ACL.
#' 
#' By default, all objects are private. Only the owner has full access
#' control. When uploading an object, you can use this header to explicitly
#' grant access permissions to specific Amazon Web Services accounts or
#' groups. This header maps to specific permissions that Amazon S3 supports
#' in an ACL. For more information, see [Access Control List (ACL)
#' Overview](https://docs.aws.amazon.com/AmazonS3/latest/userguide/acl-overview.html)
#' in the *Amazon S3 User Guide*.
#' 
#' You specify each grantee as a type=value pair, where the type is one of
#' the following:
#' 
#' -   `id` – if the value specified is the canonical user ID of an Amazon
#'     Web Services account
#' 
#' -   `uri` – if you are granting permissions to a predefined group
#' 
#' -   `emailAddress` – if the value specified is the email address of an
#'     Amazon Web Services account
#' 
#'     Using email addresses to specify a grantee is only supported in the
#'     following Amazon Web Services Regions:
#' 
#'     -   US East (N. Virginia)
#' 
#'     -   US West (N. California)
#' 
#'     -   US West (Oregon)
#' 
#'     -   Asia Pacific (Singapore)
#' 
#'     -   Asia Pacific (Sydney)
#' 
#'     -   Asia Pacific (Tokyo)
#' 
#'     -   Europe (Ireland)
#' 
#'     -   South America (São Paulo)
#' 
#'     For a list of all the Amazon S3 supported Regions and endpoints, see
#'     [Regions and
#'     Endpoints](https://docs.aws.amazon.com/general/latest/gr/rande.html#s3_region)
#'     in the Amazon Web Services General Reference.
#' 
#' For example, the following `x-amz-grant-read` header grants the Amazon
#' Web Services accounts identified by account IDs permissions to read
#' object data and its metadata:
#' 
#' `x-amz-grant-read: id="11112222333", id="444455556666" `
#' 
#' -   This functionality is not supported for directory buckets.
#' 
#' -   This functionality is not supported for Amazon S3 on Outposts.
#' @param GrantWriteACP Specify access permissions explicitly to allows grantee to allow grantee
#' to write the ACL for the applicable object.
#' 
#' By default, all objects are private. Only the owner has full access
#' control. When uploading an object, you can use this header to explicitly
#' grant access permissions to specific Amazon Web Services accounts or
#' groups. This header maps to specific permissions that Amazon S3 supports
#' in an ACL. For more information, see [Access Control List (ACL)
#' Overview](https://docs.aws.amazon.com/AmazonS3/latest/userguide/acl-overview.html)
#' in the *Amazon S3 User Guide*.
#' 
#' You specify each grantee as a type=value pair, where the type is one of
#' the following:
#' 
#' -   `id` – if the value specified is the canonical user ID of an Amazon
#'     Web Services account
#' 
#' -   `uri` – if you are granting permissions to a predefined group
#' 
#' -   `emailAddress` – if the value specified is the email address of an
#'     Amazon Web Services account
#' 
#'     Using email addresses to specify a grantee is only supported in the
#'     following Amazon Web Services Regions:
#' 
#'     -   US East (N. Virginia)
#' 
#'     -   US West (N. California)
#' 
#'     -   US West (Oregon)
#' 
#'     -   Asia Pacific (Singapore)
#' 
#'     -   Asia Pacific (Sydney)
#' 
#'     -   Asia Pacific (Tokyo)
#' 
#'     -   Europe (Ireland)
#' 
#'     -   South America (São Paulo)
#' 
#'     For a list of all the Amazon S3 supported Regions and endpoints, see
#'     [Regions and
#'     Endpoints](https://docs.aws.amazon.com/general/latest/gr/rande.html#s3_region)
#'     in the Amazon Web Services General Reference.
#' 
#' For example, the following `x-amz-grant-read` header grants the Amazon
#' Web Services accounts identified by account IDs permissions to read
#' object data and its metadata:
#' 
#' `x-amz-grant-read: id="11112222333", id="444455556666" `
#' 
#' -   This functionality is not supported for directory buckets.
#' 
#' -   This functionality is not supported for Amazon S3 on Outposts.
#' @param Key &#91;required&#93; Object key for which the multipart upload is to be initiated.
#' @param Metadata A map of metadata to store with the object in S3.
#' @param ServerSideEncryption The server-side encryption algorithm used when you store this object in
#' Amazon S3 or Amazon FSx.
#' 
#' -   **Directory buckets** - For directory buckets, there are only two
#'     supported options for server-side encryption: server-side encryption
#'     with Amazon S3 managed keys (SSE-S3) (`AES256`) and server-side
#'     encryption with KMS keys (SSE-KMS) (`aws:kms`). We recommend that
#'     the bucket's default encryption uses the desired encryption
#'     configuration and you don't override the bucket default encryption
#'     in your [`create_session`][s3_create_session] requests or `PUT`
#'     object requests. Then, new objects are automatically encrypted with
#'     the desired encryption settings. For more information, see
#'     [Protecting data with server-side
#'     encryption](https://docs.aws.amazon.com/AmazonS3/latest/userguide/)
#'     in the *Amazon S3 User Guide*. For more information about the
#'     encryption overriding behaviors in directory buckets, see
#'     [Specifying server-side encryption with KMS for new object
#'     uploads](https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-express-specifying-kms-encryption.html).
#' 
#'     In the Zonal endpoint API calls (except
#'     [`copy_object`][s3_copy_object] and
#'     [`upload_part_copy`][s3_upload_part_copy]) using the REST API, the
#'     encryption request headers must match the encryption settings that
#'     are specified in the [`create_session`][s3_create_session] request.
#'     You can't override the values of the encryption settings
#'     (`x-amz-server-side-encryption`,
#'     `x-amz-server-side-encryption-aws-kms-key-id`,
#'     `x-amz-server-side-encryption-context`, and
#'     `x-amz-server-side-encryption-bucket-key-enabled`) that are
#'     specified in the [`create_session`][s3_create_session] request. You
#'     don't need to explicitly specify these encryption settings values in
#'     Zonal endpoint API calls, and Amazon S3 will use the encryption
#'     settings values from the [`create_session`][s3_create_session]
#'     request to protect new objects in the directory bucket.
#' 
#'     When you use the CLI or the Amazon Web Services SDKs, for
#'     [`create_session`][s3_create_session], the session token refreshes
#'     automatically to avoid service interruptions when a session expires.
#'     The CLI or the Amazon Web Services SDKs use the bucket's default
#'     encryption configuration for the
#'     [`create_session`][s3_create_session] request. It's not supported to
#'     override the encryption settings values in the
#'     [`create_session`][s3_create_session] request. So in the Zonal
#'     endpoint API calls (except [`copy_object`][s3_copy_object] and
#'     [`upload_part_copy`][s3_upload_part_copy]), the encryption request
#'     headers must match the default encryption configuration of the
#'     directory bucket.
#' 
#' -   **S3 access points for Amazon FSx** - When accessing data stored in
#'     Amazon FSx file systems using S3 access points, the only valid
#'     server side encryption option is `aws:fsx`. All Amazon FSx file
#'     systems have encryption configured by default and are encrypted at
#'     rest. Data is automatically encrypted before being written to the
#'     file system, and automatically decrypted as it is read. These
#'     processes are handled transparently by Amazon FSx.
#' @param StorageClass By default, Amazon S3 uses the STANDARD Storage Class to store newly
#' created objects. The STANDARD storage class provides high durability and
#' high availability. Depending on performance needs, you can specify a
#' different Storage Class. For more information, see [Storage
#' Classes](https://docs.aws.amazon.com/AmazonS3/latest/userguide/storage-class-intro.html)
#' in the *Amazon S3 User Guide*.
#' 
#' -   Directory buckets only support `EXPRESS_ONEZONE` (the S3 Express One
#'     Zone storage class) in Availability Zones and `ONEZONE_IA` (the S3
#'     One Zone-Infrequent Access storage class) in Dedicated Local Zones.
#' 
#' -   Amazon S3 on Outposts only uses the OUTPOSTS Storage Class.
#' @param WebsiteRedirectLocation If the bucket is configured as a website, redirects requests for this
#' object to another object in the same bucket or to an external URL.
#' Amazon S3 stores the value of this header in the object metadata.
#' 
#' This functionality is not supported for directory buckets.
#' @param SSECustomerAlgorithm Specifies the algorithm to use when encrypting the object (for example,
#' AES256).
#' 
#' This functionality is not supported for directory buckets.
#' @param SSECustomerKey Specifies the customer-provided encryption key for Amazon S3 to use in
#' encrypting data. This value is used to store the object and then it is
#' discarded; Amazon S3 does not store the encryption key. The key must be
#' appropriate for use with the algorithm specified in the
#' `x-amz-server-side-encryption-customer-algorithm` header.
#' 
#' This functionality is not supported for directory buckets.
#' @param SSECustomerKeyMD5 Specifies the 128-bit MD5 digest of the customer-provided encryption key
#' according to RFC 1321. Amazon S3 uses this header for a message
#' integrity check to ensure that the encryption key was transmitted
#' without error.
#' 
#' This functionality is not supported for directory buckets.
#' @param SSEKMSKeyId Specifies the KMS key ID (Key ID, Key ARN, or Key Alias) to use for
#' object encryption. If the KMS key doesn't exist in the same account
#' that's issuing the command, you must use the full Key ARN not the Key
#' ID.
#' 
#' **General purpose buckets** - If you specify
#' `x-amz-server-side-encryption` with `aws:kms` or `aws:kms:dsse`, this
#' header specifies the ID (Key ID, Key ARN, or Key Alias) of the KMS key
#' to use. If you specify `x-amz-server-side-encryption:aws:kms` or
#' `x-amz-server-side-encryption:aws:kms:dsse`, but do not provide
#' `x-amz-server-side-encryption-aws-kms-key-id`, Amazon S3 uses the Amazon
#' Web Services managed key (`aws/s3`) to protect the data.
#' 
#' **Directory buckets** - To encrypt data using SSE-KMS, it's recommended
#' to specify the `x-amz-server-side-encryption` header to `aws:kms`. Then,
#' the `x-amz-server-side-encryption-aws-kms-key-id` header implicitly uses
#' the bucket's default KMS customer managed key ID. If you want to
#' explicitly set the ` x-amz-server-side-encryption-aws-kms-key-id`
#' header, it must match the bucket's default customer managed key (using
#' key ID or ARN, not alias). Your SSE-KMS configuration can only support 1
#' [customer managed
#' key](https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#customer-cmk)
#' per directory bucket's lifetime. The [Amazon Web Services managed
#' key](https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#aws-managed-cmk)
#' (`aws/s3`) isn't supported. Incorrect key specification results in an
#' HTTP `400 Bad Request` error.
#' @param SSEKMSEncryptionContext Specifies the Amazon Web Services KMS Encryption Context to use for
#' object encryption. The value of this header is a Base64 encoded string
#' of a UTF-8 encoded JSON, which contains the encryption context as
#' key-value pairs.
#' 
#' **Directory buckets** - You can optionally provide an explicit
#' encryption context value. The value must match the default encryption
#' context - the bucket Amazon Resource Name (ARN). An additional
#' encryption context value is not supported.
#' @param BucketKeyEnabled Specifies whether Amazon S3 should use an S3 Bucket Key for object
#' encryption with server-side encryption using Key Management Service
#' (KMS) keys (SSE-KMS).
#' 
#' **General purpose buckets** - Setting this header to `true` causes
#' Amazon S3 to use an S3 Bucket Key for object encryption with SSE-KMS.
#' Also, specifying this header with a PUT action doesn't affect
#' bucket-level settings for S3 Bucket Key.
#' 
#' **Directory buckets** - S3 Bucket Keys are always enabled for `GET` and
#' `PUT` operations in a directory bucket and can’t be disabled. S3 Bucket
#' Keys aren't supported, when you copy SSE-KMS encrypted objects from
#' general purpose buckets to directory buckets, from directory buckets to
#' general purpose buckets, or between directory buckets, through
#' [`copy_object`][s3_copy_object],
#' [`upload_part_copy`][s3_upload_part_copy], [the Copy operation in Batch
#' Operations](https://docs.aws.amazon.com/AmazonS3/latest/userguide/directory-buckets-objects-Batch-Ops.html),
#' or [the import
#' jobs](https://docs.aws.amazon.com/AmazonS3/latest/userguide/create-import-job.html).
#' In this case, Amazon S3 makes a call to KMS every time a copy request is
#' made for a KMS-encrypted object.
#' @param RequestPayer 
#' @param Tagging The tag-set for the object. The tag-set must be encoded as URL Query
#' parameters.
#' 
#' This functionality is not supported for directory buckets.
#' @param ObjectLockMode Specifies the Object Lock mode that you want to apply to the uploaded
#' object.
#' 
#' This functionality is not supported for directory buckets.
#' @param ObjectLockRetainUntilDate Specifies the date and time when you want the Object Lock to expire.
#' 
#' This functionality is not supported for directory buckets.
#' @param ObjectLockLegalHoldStatus Specifies whether you want to apply a legal hold to the uploaded object.
#' 
#' This functionality is not supported for directory buckets.
#' @param ExpectedBucketOwner The account ID of the expected bucket owner. If the account ID that you
#' provide does not match the actual owner of the bucket, the request fails
#' with the HTTP status code `403 Forbidden` (access denied).
#' @param ChecksumAlgorithm Indicates the algorithm that you want Amazon S3 to use to create the
#' checksum for the object. For more information, see [Checking object
#' integrity](https://docs.aws.amazon.com/AmazonS3/latest/userguide/checking-object-integrity.html)
#' in the *Amazon S3 User Guide*.
#' @param ChecksumType Indicates the checksum type that you want Amazon S3 to use to calculate
#' the object’s checksum value. For more information, see [Checking object
#' integrity in the Amazon S3 User
#' Guide](https://docs.aws.amazon.com/AmazonS3/latest/userguide/checking-object-integrity.html).
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   AbortDate = as.POSIXct(
#'     "2015-01-01"
#'   ),
#'   AbortRuleId = "string",
#'   Bucket = "string",
#'   Key = "string",
#'   UploadId = "string",
#'   ServerSideEncryption = "AES256"|"aws:fsx"|"aws:kms"|"aws:kms:dsse",
#'   SSECustomerAlgorithm = "string",
#'   SSECustomerKeyMD5 = "string",
#'   SSEKMSKeyId = "string",
#'   SSEKMSEncryptionContext = "string",
#'   BucketKeyEnabled = TRUE|FALSE,
#'   RequestCharged = "requester",
#'   ChecksumAlgorithm = "CRC32"|"CRC32C"|"SHA1"|"SHA256"|"CRC64NVME",
#'   ChecksumType = "COMPOSITE"|"FULL_OBJECT"
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$create_multipart_upload(
#'   ACL = "private"|"public-read"|"public-read-write"|"authenticated-read"|"aws-exec-read"|"bucket-owner-read"|"bucket-owner-full-control",
#'   Bucket = "string",
#'   CacheControl = "string",
#'   ContentDisposition = "string",
#'   ContentEncoding = "string",
#'   ContentLanguage = "string",
#'   ContentType = "string",
#'   Expires = "string",
#'   GrantFullControl = "string",
#'   GrantRead = "string",
#'   GrantReadACP = "string",
#'   GrantWriteACP = "string",
#'   Key = "string",
#'   Metadata = list(
#'     "string"
#'   ),
#'   ServerSideEncryption = "AES256"|"aws:fsx"|"aws:kms"|"aws:kms:dsse",
#'   StorageClass = "STANDARD"|"REDUCED_REDUNDANCY"|"STANDARD_IA"|"ONEZONE_IA"|"INTELLIGENT_TIERING"|"GLACIER"|"DEEP_ARCHIVE"|"OUTPOSTS"|"GLACIER_IR"|"SNOW"|"EXPRESS_ONEZONE"|"FSX_OPENZFS"|"FSX_ONTAP",
#'   WebsiteRedirectLocation = "string",
#'   SSECustomerAlgorithm = "string",
#'   SSECustomerKey = "string",
#'   SSECustomerKeyMD5 = "string",
#'   SSEKMSKeyId = "string",
#'   SSEKMSEncryptionContext = "string",
#'   BucketKeyEnabled = TRUE|FALSE,
#'   RequestPayer = "requester",
#'   Tagging = "string",
#'   ObjectLockMode = "GOVERNANCE"|"COMPLIANCE",
#'   ObjectLockRetainUntilDate = as.POSIXct(
#'     "2015-01-01"
#'   ),
#'   ObjectLockLegalHoldStatus = "ON"|"OFF",
#'   ExpectedBucketOwner = "string",
#'   ChecksumAlgorithm = "CRC32"|"CRC32C"|"SHA1"|"SHA256"|"CRC64NVME",
#'   ChecksumType = "COMPOSITE"|"FULL_OBJECT"
#' )
#' ```
#'
#' @examples
#' \dontrun{
#' # The following example initiates a multipart upload.
#' svc$create_multipart_upload(
#'   Bucket = "examplebucket",
#'   Key = "largeobject"
#' )
#' }
#'
#' @keywords internal
#'
#' @rdname s3_create_multipart_upload
#'
#' @aliases s3_create_multipart_upload
s3_create_multipart_upload <- function(ACL = NULL, Bucket, CacheControl = NULL, ContentDisposition = NULL, ContentEncoding = NULL, ContentLanguage = NULL, ContentType = NULL, Expires = NULL, GrantFullControl = NULL, GrantRead = NULL, GrantReadACP = NULL, GrantWriteACP = NULL, Key, Metadata = NULL, ServerSideEncryption = NULL, StorageClass = NULL, WebsiteRedirectLocation = NULL, SSECustomerAlgorithm = NULL, SSECustomerKey = NULL, SSECustomerKeyMD5 = NULL, SSEKMSKeyId = NULL, SSEKMSEncryptionContext = NULL, BucketKeyEnabled = NULL, RequestPayer = NULL, Tagging = NULL, ObjectLockMode = NULL, ObjectLockRetainUntilDate = NULL, ObjectLockLegalHoldStatus = NULL, ExpectedBucketOwner = NULL, ChecksumAlgorithm = NULL, ChecksumType = NULL) {
  op <- new_operation(
    name = "CreateMultipartUpload",
    http_method = "POST",
    http_path = "/{Bucket}/{Key+}?uploads",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .s3$create_multipart_upload_input(ACL = ACL, Bucket = Bucket, CacheControl = CacheControl, ContentDisposition = ContentDisposition, ContentEncoding = ContentEncoding, ContentLanguage = ContentLanguage, ContentType = ContentType, Expires = Expires, GrantFullControl = GrantFullControl, GrantRead = GrantRead, GrantReadACP = GrantReadACP, GrantWriteACP = GrantWriteACP, Key = Key, Metadata = Metadata, ServerSideEncryption = ServerSideEncryption, StorageClass = StorageClass, WebsiteRedirectLocation = WebsiteRedirectLocation, SSECustomerAlgorithm = SSECustomerAlgorithm, SSECustomerKey = SSECustomerKey, SSECustomerKeyMD5 = SSECustomerKeyMD5, SSEKMSKeyId = SSEKMSKeyId, SSEKMSEncryptionContext = SSEKMSEncryptionContext, BucketKeyEnabled = BucketKeyEnabled, RequestPayer = RequestPayer, Tagging = Tagging, ObjectLockMode = ObjectLockMode, ObjectLockRetainUntilDate = ObjectLockRetainUntilDate, ObjectLockLegalHoldStatus = ObjectLockLegalHoldStatus, ExpectedBucketOwner = ExpectedBucketOwner, ChecksumAlgorithm = ChecksumAlgorithm, ChecksumType = ChecksumType)
  output <- .s3$create_multipart_upload_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$create_multipart_upload <- s3_create_multipart_upload

#' Creates a session that establishes temporary security credentials to
#' support fast authentication and authorization for the Zonal endpoint API
#' operations on directory buckets
#'
#' @description
#' Creates a session that establishes temporary security credentials to
#' support fast authentication and authorization for the Zonal endpoint API
#' operations on directory buckets. For more information about Zonal
#' endpoint API operations that include the Availability Zone in the
#' request endpoint, see [S3 Express One Zone
#' APIs](https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-express-APIs.html)
#' in the *Amazon S3 User Guide*.
#' 
#' To make Zonal endpoint API requests on a directory bucket, use the
#' [`create_session`][s3_create_session] API operation. Specifically, you
#' grant `s3express:CreateSession` permission to a bucket in a bucket
#' policy or an IAM identity-based policy. Then, you use IAM credentials to
#' make the [`create_session`][s3_create_session] API request on the
#' bucket, which returns temporary security credentials that include the
#' access key ID, secret access key, session token, and expiration. These
#' credentials have associated permissions to access the Zonal endpoint API
#' operations. After the session is created, you don’t need to use other
#' policies to grant permissions to each Zonal endpoint API individually.
#' Instead, in your Zonal endpoint API requests, you sign your requests by
#' applying the temporary security credentials of the session to the
#' request headers and following the SigV4 protocol for authentication. You
#' also apply the session token to the `x-amz-s3session-token` request
#' header for authorization. Temporary security credentials are scoped to
#' the bucket and expire after 5 minutes. After the expiration time, any
#' calls that you make with those credentials will fail. You must use IAM
#' credentials again to make a [`create_session`][s3_create_session] API
#' request that generates a new set of temporary credentials for use.
#' Temporary credentials cannot be extended or refreshed beyond the
#' original specified interval.
#' 
#' If you use Amazon Web Services SDKs, SDKs handle the session token
#' refreshes automatically to avoid service interruptions when a session
#' expires. We recommend that you use the Amazon Web Services SDKs to
#' initiate and manage requests to the CreateSession API. For more
#' information, see [Performance guidelines and design
#' patterns](https://docs.aws.amazon.com/AmazonS3/latest/userguide/#s3-express-optimizing-performance-session-authentication)
#' in the *Amazon S3 User Guide*.
#' 
#' -   You must make requests for this API operation to the Zonal endpoint.
#'     These endpoints support virtual-hosted-style requests in the format
#'     `https://bucket-name.s3express-zone-id.region-code.amazonaws.com`.
#'     Path-style requests are not supported. For more information about
#'     endpoints in Availability Zones, see [Regional and Zonal endpoints
#'     for directory buckets in Availability
#'     Zones](https://docs.aws.amazon.com/AmazonS3/latest/userguide/endpoint-directory-buckets-AZ.html)
#'     in the *Amazon S3 User Guide*. For more information about endpoints
#'     in Local Zones, see [Concepts for directory buckets in Local
#'     Zones](https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-lzs-for-directory-buckets.html)
#'     in the *Amazon S3 User Guide*.
#' 
#' -   **[`copy_object`][s3_copy_object] API operation** - Unlike other
#'     Zonal endpoint API operations, the [`copy_object`][s3_copy_object]
#'     API operation doesn't use the temporary security credentials
#'     returned from the [`create_session`][s3_create_session] API
#'     operation for authentication and authorization. For information
#'     about authentication and authorization of the
#'     [`copy_object`][s3_copy_object] API operation on directory buckets,
#'     see [`copy_object`][s3_copy_object].
#' 
#' -   **[`head_bucket`][s3_head_bucket] API operation** - Unlike other
#'     Zonal endpoint API operations, the [`head_bucket`][s3_head_bucket]
#'     API operation doesn't use the temporary security credentials
#'     returned from the [`create_session`][s3_create_session] API
#'     operation for authentication and authorization. For information
#'     about authentication and authorization of the
#'     [`head_bucket`][s3_head_bucket] API operation on directory buckets,
#'     see [`head_bucket`][s3_head_bucket].
#' 
#' ### Permissions
#' 
#' To obtain temporary security credentials, you must create a bucket
#' policy or an IAM identity-based policy that grants
#' `s3express:CreateSession` permission to the bucket. In a policy, you can
#' have the `s3express:SessionMode` condition key to control who can create
#' a `ReadWrite` or `ReadOnly` session. For more information about
#' `ReadWrite` or `ReadOnly` sessions, see
#' [`x-amz-create-session-mode`](https://docs.aws.amazon.com/AmazonS3/latest/API/API_CreateSession.html#API_CreateSession_RequestParameters)
#' . For example policies, see [Example bucket policies for S3 Express One
#' Zone](https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-express-security-iam-example-bucket-policies.html)
#' and [Amazon Web Services Identity and Access Management (IAM)
#' identity-based policies for S3 Express One
#' Zone](https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-express-security-iam-identity-policies.html)
#' in the *Amazon S3 User Guide*.
#' 
#' To grant cross-account access to Zonal endpoint API operations, the
#' bucket policy should also grant both accounts the
#' `s3express:CreateSession` permission.
#' 
#' If you want to encrypt objects with SSE-KMS, you must also have the
#' `kms:GenerateDataKey` and the `kms:Decrypt` permissions in IAM
#' identity-based policies and KMS key policies for the target KMS key.
#' 
#' ### Encryption
#' 
#' For directory buckets, there are only two supported options for
#' server-side encryption: server-side encryption with Amazon S3 managed
#' keys (SSE-S3) (`AES256`) and server-side encryption with KMS keys
#' (SSE-KMS) (`aws:kms`). We recommend that the bucket's default encryption
#' uses the desired encryption configuration and you don't override the
#' bucket default encryption in your [`create_session`][s3_create_session]
#' requests or `PUT` object requests. Then, new objects are automatically
#' encrypted with the desired encryption settings. For more information,
#' see [Protecting data with server-side
#' encryption](https://docs.aws.amazon.com/AmazonS3/latest/userguide/) in
#' the *Amazon S3 User Guide*. For more information about the encryption
#' overriding behaviors in directory buckets, see [Specifying server-side
#' encryption with KMS for new object
#' uploads](https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-express-specifying-kms-encryption.html).
#' 
#' For [Zonal endpoint (object-level) API
#' operations](https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-express-differences.html#s3-express-differences-api-operations)
#' except [`copy_object`][s3_copy_object] and
#' [`upload_part_copy`][s3_upload_part_copy], you authenticate and
#' authorize requests through [`create_session`][s3_create_session] for low
#' latency. To encrypt new objects in a directory bucket with SSE-KMS, you
#' must specify SSE-KMS as the directory bucket's default encryption
#' configuration with a KMS key (specifically, a [customer managed
#' key](https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#customer-cmk)).
#' Then, when a session is created for Zonal endpoint API operations, new
#' objects are automatically encrypted and decrypted with SSE-KMS and S3
#' Bucket Keys during the session.
#' 
#' Only 1 [customer managed
#' key](https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#customer-cmk)
#' is supported per directory bucket for the lifetime of the bucket. The
#' [Amazon Web Services managed
#' key](https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#aws-managed-cmk)
#' (`aws/s3`) isn't supported. After you specify SSE-KMS as your bucket's
#' default encryption configuration with a customer managed key, you can't
#' change the customer managed key for the bucket's SSE-KMS configuration.
#' 
#' In the Zonal endpoint API calls (except [`copy_object`][s3_copy_object]
#' and [`upload_part_copy`][s3_upload_part_copy]) using the REST API, you
#' can't override the values of the encryption settings
#' (`x-amz-server-side-encryption`,
#' `x-amz-server-side-encryption-aws-kms-key-id`,
#' `x-amz-server-side-encryption-context`, and
#' `x-amz-server-side-encryption-bucket-key-enabled`) from the
#' [`create_session`][s3_create_session] request. You don't need to
#' explicitly specify these encryption settings values in Zonal endpoint
#' API calls, and Amazon S3 will use the encryption settings values from
#' the [`create_session`][s3_create_session] request to protect new objects
#' in the directory bucket.
#' 
#' When you use the CLI or the Amazon Web Services SDKs, for
#' [`create_session`][s3_create_session], the session token refreshes
#' automatically to avoid service interruptions when a session expires. The
#' CLI or the Amazon Web Services SDKs use the bucket's default encryption
#' configuration for the [`create_session`][s3_create_session] request.
#' It's not supported to override the encryption settings values in the
#' [`create_session`][s3_create_session] request. Also, in the Zonal
#' endpoint API calls (except [`copy_object`][s3_copy_object] and
#' [`upload_part_copy`][s3_upload_part_copy]), it's not supported to
#' override the values of the encryption settings from the
#' [`create_session`][s3_create_session] request.
#' 
#' ### HTTP Host header syntax
#' 
#' **Directory buckets** - The HTTP Host header syntax is
#' ` Bucket-name.s3express-zone-id.region-code.amazonaws.com`.
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#'
#' @usage
#' s3_create_session(SessionMode, Bucket, ServerSideEncryption,
#'   SSEKMSKeyId, SSEKMSEncryptionContext, BucketKeyEnabled)
#'
#' @param SessionMode Specifies the mode of the session that will be created, either
#' `ReadWrite` or `ReadOnly`. By default, a `ReadWrite` session is created.
#' A `ReadWrite` session is capable of executing all the Zonal endpoint API
#' operations on a directory bucket. A `ReadOnly` session is constrained to
#' execute the following Zonal endpoint API operations:
#' [`get_object`][s3_get_object], [`head_object`][s3_head_object],
#' [`list_objects_v2`][s3_list_objects_v2],
#' [`get_object_attributes`][s3_get_object_attributes],
#' [`list_parts`][s3_list_parts], and
#' [`list_multipart_uploads`][s3_list_multipart_uploads].
#' @param Bucket &#91;required&#93; The name of the bucket that you create a session for.
#' @param ServerSideEncryption The server-side encryption algorithm to use when you store objects in
#' the directory bucket.
#' 
#' For directory buckets, there are only two supported options for
#' server-side encryption: server-side encryption with Amazon S3 managed
#' keys (SSE-S3) (`AES256`) and server-side encryption with KMS keys
#' (SSE-KMS) (`aws:kms`). By default, Amazon S3 encrypts data with SSE-S3.
#' For more information, see [Protecting data with server-side
#' encryption](https://docs.aws.amazon.com/AmazonS3/latest/userguide/serv-side-encryption.html)
#' in the *Amazon S3 User Guide*.
#' 
#' **S3 access points for Amazon FSx** - When accessing data stored in
#' Amazon FSx file systems using S3 access points, the only valid server
#' side encryption option is `aws:fsx`. All Amazon FSx file systems have
#' encryption configured by default and are encrypted at rest. Data is
#' automatically encrypted before being written to the file system, and
#' automatically decrypted as it is read. These processes are handled
#' transparently by Amazon FSx.
#' @param SSEKMSKeyId If you specify `x-amz-server-side-encryption` with `aws:kms`, you must
#' specify the ` x-amz-server-side-encryption-aws-kms-key-id` header with
#' the ID (Key ID or Key ARN) of the KMS symmetric encryption customer
#' managed key to use. Otherwise, you get an HTTP `400 Bad Request` error.
#' Only use the key ID or key ARN. The key alias format of the KMS key
#' isn't supported. Also, if the KMS key doesn't exist in the same account
#' that't issuing the command, you must use the full Key ARN not the Key
#' ID.
#' 
#' Your SSE-KMS configuration can only support 1 [customer managed
#' key](https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#customer-cmk)
#' per directory bucket's lifetime. The [Amazon Web Services managed
#' key](https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#aws-managed-cmk)
#' (`aws/s3`) isn't supported.
#' @param SSEKMSEncryptionContext Specifies the Amazon Web Services KMS Encryption Context as an
#' additional encryption context to use for object encryption. The value of
#' this header is a Base64 encoded string of a UTF-8 encoded JSON, which
#' contains the encryption context as key-value pairs. This value is stored
#' as object metadata and automatically gets passed on to Amazon Web
#' Services KMS for future [`get_object`][s3_get_object] operations on this
#' object.
#' 
#' **General purpose buckets** - This value must be explicitly added during
#' [`copy_object`][s3_copy_object] operations if you want an additional
#' encryption context for your object. For more information, see
#' [Encryption
#' context](https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingKMSEncryption.html#encryption-context)
#' in the *Amazon S3 User Guide*.
#' 
#' **Directory buckets** - You can optionally provide an explicit
#' encryption context value. The value must match the default encryption
#' context - the bucket Amazon Resource Name (ARN). An additional
#' encryption context value is not supported.
#' @param BucketKeyEnabled Specifies whether Amazon S3 should use an S3 Bucket Key for object
#' encryption with server-side encryption using KMS keys (SSE-KMS).
#' 
#' S3 Bucket Keys are always enabled for `GET` and `PUT` operations in a
#' directory bucket and can’t be disabled. S3 Bucket Keys aren't supported,
#' when you copy SSE-KMS encrypted objects from general purpose buckets to
#' directory buckets, from directory buckets to general purpose buckets, or
#' between directory buckets, through [`copy_object`][s3_copy_object],
#' [`upload_part_copy`][s3_upload_part_copy], [the Copy operation in Batch
#' Operations](https://docs.aws.amazon.com/AmazonS3/latest/userguide/directory-buckets-objects-Batch-Ops.html),
#' or [the import
#' jobs](https://docs.aws.amazon.com/AmazonS3/latest/userguide/create-import-job.html).
#' In this case, Amazon S3 makes a call to KMS every time a copy request is
#' made for a KMS-encrypted object.
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   ServerSideEncryption = "AES256"|"aws:fsx"|"aws:kms"|"aws:kms:dsse",
#'   SSEKMSKeyId = "string",
#'   SSEKMSEncryptionContext = "string",
#'   BucketKeyEnabled = TRUE|FALSE,
#'   Credentials = list(
#'     AccessKeyId = "string",
#'     SecretAccessKey = "string",
#'     SessionToken = "string",
#'     Expiration = as.POSIXct(
#'       "2015-01-01"
#'     )
#'   )
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$create_session(
#'   SessionMode = "ReadOnly"|"ReadWrite",
#'   Bucket = "string",
#'   ServerSideEncryption = "AES256"|"aws:fsx"|"aws:kms"|"aws:kms:dsse",
#'   SSEKMSKeyId = "string",
#'   SSEKMSEncryptionContext = "string",
#'   BucketKeyEnabled = TRUE|FALSE
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname s3_create_session
#'
#' @aliases s3_create_session
s3_create_session <- function(SessionMode = NULL, Bucket, ServerSideEncryption = NULL, SSEKMSKeyId = NULL, SSEKMSEncryptionContext = NULL, BucketKeyEnabled = NULL) {
  op <- new_operation(
    name = "CreateSession",
    http_method = "GET",
    http_path = "/{Bucket}?session",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .s3$create_session_input(SessionMode = SessionMode, Bucket = Bucket, ServerSideEncryption = ServerSideEncryption, SSEKMSKeyId = SSEKMSKeyId, SSEKMSEncryptionContext = SSEKMSEncryptionContext, BucketKeyEnabled = BucketKeyEnabled)
  output <- .s3$create_session_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$create_session <- s3_create_session

#' Deletes the S3 bucket
#'
#' @description
#' Deletes the S3 bucket. All objects (including all object versions and
#' delete markers) in the bucket must be deleted before the bucket itself
#' can be deleted.
#' 
#' -   **Directory buckets** - If multipart uploads in a directory bucket
#'     are in progress, you can't delete the bucket until all the
#'     in-progress multipart uploads are aborted or completed.
#' 
#' -   **Directory buckets** - For directory buckets, you must make
#'     requests for this API operation to the Regional endpoint. These
#'     endpoints support path-style requests in the format
#'     `https://s3express-control.region-code.amazonaws.com/bucket-name `.
#'     Virtual-hosted-style requests aren't supported. For more information
#'     about endpoints in Availability Zones, see [Regional and Zonal
#'     endpoints for directory buckets in Availability
#'     Zones](https://docs.aws.amazon.com/AmazonS3/latest/userguide/endpoint-directory-buckets-AZ.html)
#'     in the *Amazon S3 User Guide*. For more information about endpoints
#'     in Local Zones, see [Concepts for directory buckets in Local
#'     Zones](https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-lzs-for-directory-buckets.html)
#'     in the *Amazon S3 User Guide*.
#' 
#' ### Permissions
#' 
#' -   **General purpose bucket permissions** - You must have the
#'     `s3:DeleteBucket` permission on the specified bucket in a policy.
#' 
#' -   **Directory bucket permissions** - You must have the
#'     `s3express:DeleteBucket` permission in an IAM identity-based policy
#'     instead of a bucket policy. Cross-account access to this API
#'     operation isn't supported. This operation can only be performed by
#'     the Amazon Web Services account that owns the resource. For more
#'     information about directory bucket policies and permissions, see
#'     [Amazon Web Services Identity and Access Management (IAM) for S3
#'     Express One
#'     Zone](https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-express-security-iam.html)
#'     in the *Amazon S3 User Guide*.
#' 
#' ### HTTP Host header syntax
#' 
#' **Directory buckets** - The HTTP Host header syntax is
#' `s3express-control.region-code.amazonaws.com`.
#' 
#' The following operations are related to
#' [`delete_bucket`][s3_delete_bucket]:
#' 
#' -   [`create_bucket`][s3_create_bucket]
#' 
#' -   [`delete_object`][s3_delete_object]
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#'
#' @usage
#' s3_delete_bucket(Bucket, ExpectedBucketOwner)
#'
#' @param Bucket &#91;required&#93; Specifies the bucket being deleted.
#' 
#' **Directory buckets** - When you use this operation with a directory
#' bucket, you must use path-style requests in the format
#' `https://s3express-control.region-code.amazonaws.com/bucket-name `.
#' Virtual-hosted-style requests aren't supported. Directory bucket names
#' must be unique in the chosen Zone (Availability Zone or Local Zone).
#' Bucket names must also follow the format
#' ` bucket-base-name--zone-id--x-s3` (for example,
#' ` DOC-EXAMPLE-BUCKET--usw2-az1--x-s3`). For information about bucket
#' naming restrictions, see [Directory bucket naming
#' rules](https://docs.aws.amazon.com/AmazonS3/latest/userguide/directory-bucket-naming-rules.html)
#' in the *Amazon S3 User Guide*
#' @param ExpectedBucketOwner The account ID of the expected bucket owner. If the account ID that you
#' provide does not match the actual owner of the bucket, the request fails
#' with the HTTP status code `403 Forbidden` (access denied).
#' 
#' For directory buckets, this header is not supported in this API
#' operation. If you specify this header, the request fails with the HTTP
#' status code `501 Not Implemented`.
#'
#' @return
#' An empty list.
#'
#' @section Request syntax:
#' ```
#' svc$delete_bucket(
#'   Bucket = "string",
#'   ExpectedBucketOwner = "string"
#' )
#' ```
#'
#' @examples
#' \dontrun{
#' # The following example deletes the specified bucket.
#' svc$delete_bucket(
#'   Bucket = "forrandall2"
#' )
#' }
#'
#' @keywords internal
#'
#' @rdname s3_delete_bucket
#'
#' @aliases s3_delete_bucket
s3_delete_bucket <- function(Bucket, ExpectedBucketOwner = NULL) {
  op <- new_operation(
    name = "DeleteBucket",
    http_method = "DELETE",
    http_path = "/{Bucket}",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .s3$delete_bucket_input(Bucket = Bucket, ExpectedBucketOwner = ExpectedBucketOwner)
  output <- .s3$delete_bucket_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$delete_bucket <- s3_delete_bucket

#' This operation is not supported for directory buckets
#'
#' @description
#' This operation is not supported for directory buckets.
#' 
#' Deletes an analytics configuration for the bucket (specified by the
#' analytics configuration ID).
#' 
#' To use this operation, you must have permissions to perform the
#' `s3:PutAnalyticsConfiguration` action. The bucket owner has this
#' permission by default. The bucket owner can grant this permission to
#' others. For more information about permissions, see [Permissions Related
#' to Bucket Subresource
#' Operations](https://docs.aws.amazon.com/AmazonS3/latest/userguide/security_iam_service-with-iam.html#security_iam_service-with-iam-id-based-policies-actions)
#' and [Managing Access Permissions to Your Amazon S3
#' Resources](https://docs.aws.amazon.com/AmazonS3/latest/userguide/security-iam.html).
#' 
#' For information about the Amazon S3 analytics feature, see [Amazon S3
#' Analytics – Storage Class
#' Analysis](https://docs.aws.amazon.com/AmazonS3/latest/userguide/analytics-storage-class.html).
#' 
#' The following operations are related to
#' [`delete_bucket_analytics_configuration`][s3_delete_bucket_analytics_configuration]:
#' 
#' -   [`get_bucket_analytics_configuration`][s3_get_bucket_analytics_configuration]
#' 
#' -   [`list_bucket_analytics_configurations`][s3_list_bucket_analytics_configurations]
#' 
#' -   [`put_bucket_analytics_configuration`][s3_put_bucket_analytics_configuration]
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#'
#' @usage
#' s3_delete_bucket_analytics_configuration(Bucket, Id,
#'   ExpectedBucketOwner)
#'
#' @param Bucket &#91;required&#93; The name of the bucket from which an analytics configuration is deleted.
#' @param Id &#91;required&#93; The ID that identifies the analytics configuration.
#' @param ExpectedBucketOwner The account ID of the expected bucket owner. If the account ID that you
#' provide does not match the actual owner of the bucket, the request fails
#' with the HTTP status code `403 Forbidden` (access denied).
#'
#' @return
#' An empty list.
#'
#' @section Request syntax:
#' ```
#' svc$delete_bucket_analytics_configuration(
#'   Bucket = "string",
#'   Id = "string",
#'   ExpectedBucketOwner = "string"
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname s3_delete_bucket_analytics_configuration
#'
#' @aliases s3_delete_bucket_analytics_configuration
s3_delete_bucket_analytics_configuration <- function(Bucket, Id, ExpectedBucketOwner = NULL) {
  op <- new_operation(
    name = "DeleteBucketAnalyticsConfiguration",
    http_method = "DELETE",
    http_path = "/{Bucket}?analytics",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .s3$delete_bucket_analytics_configuration_input(Bucket = Bucket, Id = Id, ExpectedBucketOwner = ExpectedBucketOwner)
  output <- .s3$delete_bucket_analytics_configuration_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$delete_bucket_analytics_configuration <- s3_delete_bucket_analytics_configuration

#' This operation is not supported for directory buckets
#'
#' @description
#' This operation is not supported for directory buckets.
#' 
#' Deletes the `cors` configuration information set for the bucket.
#' 
#' To use this operation, you must have permission to perform the
#' `s3:PutBucketCORS` action. The bucket owner has this permission by
#' default and can grant this permission to others.
#' 
#' For information about `cors`, see [Enabling Cross-Origin Resource
#' Sharing](https://docs.aws.amazon.com/AmazonS3/latest/userguide/cors.html)
#' in the *Amazon S3 User Guide*.
#' 
#' **Related Resources**
#' 
#' -   [`put_bucket_cors`][s3_put_bucket_cors]
#' 
#' -   [RESTOPTIONSobject](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTOPTIONSobject.html)
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#'
#' @usage
#' s3_delete_bucket_cors(Bucket, ExpectedBucketOwner)
#'
#' @param Bucket &#91;required&#93; Specifies the bucket whose `cors` configuration is being deleted.
#' @param ExpectedBucketOwner The account ID of the expected bucket owner. If the account ID that you
#' provide does not match the actual owner of the bucket, the request fails
#' with the HTTP status code `403 Forbidden` (access denied).
#'
#' @return
#' An empty list.
#'
#' @section Request syntax:
#' ```
#' svc$delete_bucket_cors(
#'   Bucket = "string",
#'   ExpectedBucketOwner = "string"
#' )
#' ```
#'
#' @examples
#' \dontrun{
#' # The following example deletes CORS configuration on a bucket.
#' svc$delete_bucket_cors(
#'   Bucket = "examplebucket"
#' )
#' }
#'
#' @keywords internal
#'
#' @rdname s3_delete_bucket_cors
#'
#' @aliases s3_delete_bucket_cors
s3_delete_bucket_cors <- function(Bucket, ExpectedBucketOwner = NULL) {
  op <- new_operation(
    name = "DeleteBucketCors",
    http_method = "DELETE",
    http_path = "/{Bucket}?cors",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .s3$delete_bucket_cors_input(Bucket = Bucket, ExpectedBucketOwner = ExpectedBucketOwner)
  output <- .s3$delete_bucket_cors_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$delete_bucket_cors <- s3_delete_bucket_cors

#' This implementation of the DELETE action resets the default encryption
#' for the bucket as server-side encryption with Amazon S3 managed keys
#' (SSE-S3)
#'
#' @description
#' This implementation of the DELETE action resets the default encryption
#' for the bucket as server-side encryption with Amazon S3 managed keys
#' (SSE-S3).
#' 
#' -   **General purpose buckets** - For information about the bucket
#'     default encryption feature, see [Amazon S3 Bucket Default
#'     Encryption](https://docs.aws.amazon.com/AmazonS3/latest/userguide/bucket-encryption.html)
#'     in the *Amazon S3 User Guide*.
#' 
#' -   **Directory buckets** - For directory buckets, there are only two
#'     supported options for server-side encryption: SSE-S3 and SSE-KMS.
#'     For information about the default encryption configuration in
#'     directory buckets, see [Setting default server-side encryption
#'     behavior for directory
#'     buckets](https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-express-bucket-encryption.html).
#' 
#' ### Permissions
#' 
#' -   **General purpose bucket permissions** - The
#'     `s3:PutEncryptionConfiguration` permission is required in a policy.
#'     The bucket owner has this permission by default. The bucket owner
#'     can grant this permission to others. For more information about
#'     permissions, see [Permissions Related to Bucket
#'     Operations](https://docs.aws.amazon.com/AmazonS3/latest/userguide/security_iam_service-with-iam.html#security_iam_service-with-iam-id-based-policies-actions)
#'     and [Managing Access Permissions to Your Amazon S3
#'     Resources](https://docs.aws.amazon.com/AmazonS3/latest/userguide/security-iam.html).
#' 
#' -   **Directory bucket permissions** - To grant access to this API
#'     operation, you must have the `s3express:PutEncryptionConfiguration`
#'     permission in an IAM identity-based policy instead of a bucket
#'     policy. Cross-account access to this API operation isn't supported.
#'     This operation can only be performed by the Amazon Web Services
#'     account that owns the resource. For more information about directory
#'     bucket policies and permissions, see [Amazon Web Services Identity
#'     and Access Management (IAM) for S3 Express One
#'     Zone](https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-express-security-iam.html)
#'     in the *Amazon S3 User Guide*.
#' 
#' ### HTTP Host header syntax
#' 
#' **Directory buckets** - The HTTP Host header syntax is
#' `s3express-control.region-code.amazonaws.com`.
#' 
#' The following operations are related to
#' [`delete_bucket_encryption`][s3_delete_bucket_encryption]:
#' 
#' -   [`put_bucket_encryption`][s3_put_bucket_encryption]
#' 
#' -   [`get_bucket_encryption`][s3_get_bucket_encryption]
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#'
#' @usage
#' s3_delete_bucket_encryption(Bucket, ExpectedBucketOwner)
#'
#' @param Bucket &#91;required&#93; The name of the bucket containing the server-side encryption
#' configuration to delete.
#' 
#' **Directory buckets** - When you use this operation with a directory
#' bucket, you must use path-style requests in the format
#' `https://s3express-control.region-code.amazonaws.com/bucket-name `.
#' Virtual-hosted-style requests aren't supported. Directory bucket names
#' must be unique in the chosen Zone (Availability Zone or Local Zone).
#' Bucket names must also follow the format
#' ` bucket-base-name--zone-id--x-s3` (for example,
#' ` DOC-EXAMPLE-BUCKET--usw2-az1--x-s3`). For information about bucket
#' naming restrictions, see [Directory bucket naming
#' rules](https://docs.aws.amazon.com/AmazonS3/latest/userguide/directory-bucket-naming-rules.html)
#' in the *Amazon S3 User Guide*
#' @param ExpectedBucketOwner The account ID of the expected bucket owner. If the account ID that you
#' provide does not match the actual owner of the bucket, the request fails
#' with the HTTP status code `403 Forbidden` (access denied).
#' 
#' For directory buckets, this header is not supported in this API
#' operation. If you specify this header, the request fails with the HTTP
#' status code `501 Not Implemented`.
#'
#' @return
#' An empty list.
#'
#' @section Request syntax:
#' ```
#' svc$delete_bucket_encryption(
#'   Bucket = "string",
#'   ExpectedBucketOwner = "string"
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname s3_delete_bucket_encryption
#'
#' @aliases s3_delete_bucket_encryption
s3_delete_bucket_encryption <- function(Bucket, ExpectedBucketOwner = NULL) {
  op <- new_operation(
    name = "DeleteBucketEncryption",
    http_method = "DELETE",
    http_path = "/{Bucket}?encryption",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .s3$delete_bucket_encryption_input(Bucket = Bucket, ExpectedBucketOwner = ExpectedBucketOwner)
  output <- .s3$delete_bucket_encryption_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$delete_bucket_encryption <- s3_delete_bucket_encryption

#' This operation is not supported for directory buckets
#'
#' @description
#' This operation is not supported for directory buckets.
#' 
#' Deletes the S3 Intelligent-Tiering configuration from the specified
#' bucket.
#' 
#' The S3 Intelligent-Tiering storage class is designed to optimize storage
#' costs by automatically moving data to the most cost-effective storage
#' access tier, without performance impact or operational overhead. S3
#' Intelligent-Tiering delivers automatic cost savings in three low latency
#' and high throughput access tiers. To get the lowest storage cost on data
#' that can be accessed in minutes to hours, you can choose to activate
#' additional archiving capabilities.
#' 
#' The S3 Intelligent-Tiering storage class is the ideal storage class for
#' data with unknown, changing, or unpredictable access patterns,
#' independent of object size or retention period. If the size of an object
#' is less than 128 KB, it is not monitored and not eligible for
#' auto-tiering. Smaller objects can be stored, but they are always charged
#' at the Frequent Access tier rates in the S3 Intelligent-Tiering storage
#' class.
#' 
#' For more information, see [Storage class for automatically optimizing
#' frequently and infrequently accessed
#' objects](https://docs.aws.amazon.com/AmazonS3/latest/userguide/storage-class-intro.html#sc-dynamic-data-access).
#' 
#' Operations related to
#' [`delete_bucket_intelligent_tiering_configuration`][s3_delete_bucket_intelligent_tiering_configuration]
#' include:
#' 
#' -   [`get_bucket_intelligent_tiering_configuration`][s3_get_bucket_intelligent_tiering_configuration]
#' 
#' -   [`put_bucket_intelligent_tiering_configuration`][s3_put_bucket_intelligent_tiering_configuration]
#' 
#' -   [`list_bucket_intelligent_tiering_configurations`][s3_list_bucket_intelligent_tiering_configurations]
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#'
#' @usage
#' s3_delete_bucket_intelligent_tiering_configuration(Bucket, Id,
#'   ExpectedBucketOwner)
#'
#' @param Bucket &#91;required&#93; The name of the Amazon S3 bucket whose configuration you want to modify
#' or retrieve.
#' @param Id &#91;required&#93; The ID used to identify the S3 Intelligent-Tiering configuration.
#' @param ExpectedBucketOwner The account ID of the expected bucket owner. If the account ID that you
#' provide does not match the actual owner of the bucket, the request fails
#' with the HTTP status code `403 Forbidden` (access denied).
#'
#' @return
#' An empty list.
#'
#' @section Request syntax:
#' ```
#' svc$delete_bucket_intelligent_tiering_configuration(
#'   Bucket = "string",
#'   Id = "string",
#'   ExpectedBucketOwner = "string"
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname s3_delete_bucket_intelligent_tiering_configuration
#'
#' @aliases s3_delete_bucket_intelligent_tiering_configuration
s3_delete_bucket_intelligent_tiering_configuration <- function(Bucket, Id, ExpectedBucketOwner = NULL) {
  op <- new_operation(
    name = "DeleteBucketIntelligentTieringConfiguration",
    http_method = "DELETE",
    http_path = "/{Bucket}?intelligent-tiering",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .s3$delete_bucket_intelligent_tiering_configuration_input(Bucket = Bucket, Id = Id, ExpectedBucketOwner = ExpectedBucketOwner)
  output <- .s3$delete_bucket_intelligent_tiering_configuration_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$delete_bucket_intelligent_tiering_configuration <- s3_delete_bucket_intelligent_tiering_configuration

#' This operation is not supported for directory buckets
#'
#' @description
#' This operation is not supported for directory buckets.
#' 
#' Deletes an S3 Inventory configuration (identified by the inventory ID)
#' from the bucket.
#' 
#' To use this operation, you must have permissions to perform the
#' `s3:PutInventoryConfiguration` action. The bucket owner has this
#' permission by default. The bucket owner can grant this permission to
#' others. For more information about permissions, see [Permissions Related
#' to Bucket Subresource
#' Operations](https://docs.aws.amazon.com/AmazonS3/latest/userguide/security_iam_service-with-iam.html#security_iam_service-with-iam-id-based-policies-actions)
#' and [Managing Access Permissions to Your Amazon S3
#' Resources](https://docs.aws.amazon.com/AmazonS3/latest/userguide/security-iam.html).
#' 
#' For information about the Amazon S3 inventory feature, see [Amazon S3
#' Inventory](https://docs.aws.amazon.com/AmazonS3/latest/userguide/storage-inventory.html).
#' 
#' Operations related to
#' [`delete_bucket_inventory_configuration`][s3_delete_bucket_inventory_configuration]
#' include:
#' 
#' -   [`get_bucket_inventory_configuration`][s3_get_bucket_inventory_configuration]
#' 
#' -   [`put_bucket_inventory_configuration`][s3_put_bucket_inventory_configuration]
#' 
#' -   [`list_bucket_inventory_configurations`][s3_list_bucket_inventory_configurations]
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#'
#' @usage
#' s3_delete_bucket_inventory_configuration(Bucket, Id,
#'   ExpectedBucketOwner)
#'
#' @param Bucket &#91;required&#93; The name of the bucket containing the inventory configuration to delete.
#' @param Id &#91;required&#93; The ID used to identify the inventory configuration.
#' @param ExpectedBucketOwner The account ID of the expected bucket owner. If the account ID that you
#' provide does not match the actual owner of the bucket, the request fails
#' with the HTTP status code `403 Forbidden` (access denied).
#'
#' @return
#' An empty list.
#'
#' @section Request syntax:
#' ```
#' svc$delete_bucket_inventory_configuration(
#'   Bucket = "string",
#'   Id = "string",
#'   ExpectedBucketOwner = "string"
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname s3_delete_bucket_inventory_configuration
#'
#' @aliases s3_delete_bucket_inventory_configuration
s3_delete_bucket_inventory_configuration <- function(Bucket, Id, ExpectedBucketOwner = NULL) {
  op <- new_operation(
    name = "DeleteBucketInventoryConfiguration",
    http_method = "DELETE",
    http_path = "/{Bucket}?inventory",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .s3$delete_bucket_inventory_configuration_input(Bucket = Bucket, Id = Id, ExpectedBucketOwner = ExpectedBucketOwner)
  output <- .s3$delete_bucket_inventory_configuration_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$delete_bucket_inventory_configuration <- s3_delete_bucket_inventory_configuration

#' Deletes the lifecycle configuration from the specified bucket
#'
#' @description
#' Deletes the lifecycle configuration from the specified bucket. Amazon S3
#' removes all the lifecycle configuration rules in the lifecycle
#' subresource associated with the bucket. Your objects never expire, and
#' Amazon S3 no longer automatically deletes any objects on the basis of
#' rules contained in the deleted lifecycle configuration.
#' 
#' ### Permissions
#' 
#' -   **General purpose bucket permissions** - By default, all Amazon S3
#'     resources are private, including buckets, objects, and related
#'     subresources (for example, lifecycle configuration and website
#'     configuration). Only the resource owner (that is, the Amazon Web
#'     Services account that created it) can access the resource. The
#'     resource owner can optionally grant access permissions to others by
#'     writing an access policy. For this operation, a user must have the
#'     `s3:PutLifecycleConfiguration` permission.
#' 
#'     For more information about permissions, see [Managing Access
#'     Permissions to Your Amazon S3
#'     Resources](https://docs.aws.amazon.com/AmazonS3/latest/userguide/security-iam.html).
#' 
#' 
#' -   **Directory bucket permissions** - You must have the
#'     `s3express:PutLifecycleConfiguration` permission in an IAM
#'     identity-based policy to use this operation. Cross-account access to
#'     this API operation isn't supported. The resource owner can
#'     optionally grant access permissions to others by creating a role or
#'     user for them as long as they are within the same account as the
#'     owner and resource.
#' 
#'     For more information about directory bucket policies and
#'     permissions, see [Authorizing Regional endpoint APIs with
#'     IAM](https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-express-security-iam.html)
#'     in the *Amazon S3 User Guide*.
#' 
#'     **Directory buckets** - For directory buckets, you must make
#'     requests for this API operation to the Regional endpoint. These
#'     endpoints support path-style requests in the format
#'     `https://s3express-control.region-code.amazonaws.com/bucket-name `.
#'     Virtual-hosted-style requests aren't supported. For more information
#'     about endpoints in Availability Zones, see [Regional and Zonal
#'     endpoints for directory buckets in Availability
#'     Zones](https://docs.aws.amazon.com/AmazonS3/latest/userguide/endpoint-directory-buckets-AZ.html)
#'     in the *Amazon S3 User Guide*. For more information about endpoints
#'     in Local Zones, see [Concepts for directory buckets in Local
#'     Zones](https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-lzs-for-directory-buckets.html)
#'     in the *Amazon S3 User Guide*.
#' 
#' ### HTTP Host header syntax
#' 
#' **Directory buckets** - The HTTP Host header syntax is
#' `s3express-control.region.amazonaws.com`.
#' 
#' For more information about the object expiration, see [Elements to
#' Describe Lifecycle
#' Actions](https://docs.aws.amazon.com/AmazonS3/latest/userguide/intro-lifecycle-rules.html#intro-lifecycle-rules-actions).
#' 
#' Related actions include:
#' 
#' -   [`put_bucket_lifecycle_configuration`][s3_put_bucket_lifecycle_configuration]
#' 
#' -   [`get_bucket_lifecycle_configuration`][s3_get_bucket_lifecycle_configuration]
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#'
#' @usage
#' s3_delete_bucket_lifecycle(Bucket, ExpectedBucketOwner)
#'
#' @param Bucket &#91;required&#93; The bucket name of the lifecycle to delete.
#' @param ExpectedBucketOwner The account ID of the expected bucket owner. If the account ID that you
#' provide does not match the actual owner of the bucket, the request fails
#' with the HTTP status code `403 Forbidden` (access denied).
#' 
#' This parameter applies to general purpose buckets only. It is not
#' supported for directory bucket lifecycle configurations.
#'
#' @return
#' An empty list.
#'
#' @section Request syntax:
#' ```
#' svc$delete_bucket_lifecycle(
#'   Bucket = "string",
#'   ExpectedBucketOwner = "string"
#' )
#' ```
#'
#' @examples
#' \dontrun{
#' # The following example deletes lifecycle configuration on a bucket.
#' svc$delete_bucket_lifecycle(
#'   Bucket = "examplebucket"
#' )
#' }
#'
#' @keywords internal
#'
#' @rdname s3_delete_bucket_lifecycle
#'
#' @aliases s3_delete_bucket_lifecycle
s3_delete_bucket_lifecycle <- function(Bucket, ExpectedBucketOwner = NULL) {
  op <- new_operation(
    name = "DeleteBucketLifecycle",
    http_method = "DELETE",
    http_path = "/{Bucket}?lifecycle",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .s3$delete_bucket_lifecycle_input(Bucket = Bucket, ExpectedBucketOwner = ExpectedBucketOwner)
  output <- .s3$delete_bucket_lifecycle_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$delete_bucket_lifecycle <- s3_delete_bucket_lifecycle

#' Deletes an S3 Metadata configuration from a general purpose bucket
#'
#' @description
#' Deletes an S3 Metadata configuration from a general purpose bucket. For
#' more information, see [Accelerating data discovery with S3
#' Metadata](https://docs.aws.amazon.com/AmazonS3/latest/userguide/metadata-tables-overview.html)
#' in the *Amazon S3 User Guide*.
#' 
#' You can use the V2
#' [`delete_bucket_metadata_configuration`][s3_delete_bucket_metadata_configuration]
#' API operation with V1 or V2 metadata configurations. However, if you try
#' to use the V1
#' [`delete_bucket_metadata_table_configuration`][s3_delete_bucket_metadata_table_configuration]
#' API operation with V2 configurations, you will receive an HTTP
#' `405 Method Not Allowed` error.
#' 
#' ### Permissions
#' 
#' To use this operation, you must have the
#' `s3:DeleteBucketMetadataTableConfiguration` permission. For more
#' information, see [Setting up permissions for configuring metadata
#' tables](https://docs.aws.amazon.com/AmazonS3/latest/userguide/metadata-tables-permissions.html)
#' in the *Amazon S3 User Guide*.
#' 
#' The IAM policy action name is the same for the V1 and V2 API operations.
#' 
#' The following operations are related to
#' [`delete_bucket_metadata_configuration`][s3_delete_bucket_metadata_configuration]:
#' 
#' -   [`create_bucket_metadata_configuration`][s3_create_bucket_metadata_configuration]
#' 
#' -   [`get_bucket_metadata_configuration`][s3_get_bucket_metadata_configuration]
#' 
#' -   [`update_bucket_metadata_inventory_table_configuration`][s3_update_bucket_metadata_inventory_table_configuration]
#' 
#' -   [`update_bucket_metadata_journal_table_configuration`][s3_update_bucket_metadata_journal_table_configuration]
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#'
#' @usage
#' s3_delete_bucket_metadata_configuration(Bucket, ExpectedBucketOwner)
#'
#' @param Bucket &#91;required&#93; The general purpose bucket that you want to remove the metadata
#' configuration from.
#' @param ExpectedBucketOwner The expected bucket owner of the general purpose bucket that you want to
#' remove the metadata table configuration from.
#'
#' @return
#' An empty list.
#'
#' @section Request syntax:
#' ```
#' svc$delete_bucket_metadata_configuration(
#'   Bucket = "string",
#'   ExpectedBucketOwner = "string"
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname s3_delete_bucket_metadata_configuration
#'
#' @aliases s3_delete_bucket_metadata_configuration
s3_delete_bucket_metadata_configuration <- function(Bucket, ExpectedBucketOwner = NULL) {
  op <- new_operation(
    name = "DeleteBucketMetadataConfiguration",
    http_method = "DELETE",
    http_path = "/{Bucket}?metadataConfiguration",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .s3$delete_bucket_metadata_configuration_input(Bucket = Bucket, ExpectedBucketOwner = ExpectedBucketOwner)
  output <- .s3$delete_bucket_metadata_configuration_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$delete_bucket_metadata_configuration <- s3_delete_bucket_metadata_configuration

#' We recommend that you delete your S3 Metadata configurations by using
#' the V2 DeleteBucketMetadataTableConfiguration API operation
#'
#' @description
#' We recommend that you delete your S3 Metadata configurations by using
#' the V2
#' [`delete_bucket_metadata_table_configuration`][s3_delete_bucket_metadata_table_configuration]
#' API operation. We no longer recommend using the V1
#' [`delete_bucket_metadata_table_configuration`][s3_delete_bucket_metadata_table_configuration]
#' API operation.
#' 
#' If you created your S3 Metadata configuration before July 15, 2025, we
#' recommend that you delete and re-create your configuration by using
#' [`create_bucket_metadata_configuration`][s3_create_bucket_metadata_configuration]
#' so that you can expire journal table records and create a live inventory
#' table.
#' 
#' Deletes a V1 S3 Metadata configuration from a general purpose bucket.
#' For more information, see [Accelerating data discovery with S3
#' Metadata](https://docs.aws.amazon.com/AmazonS3/latest/userguide/metadata-tables-overview.html)
#' in the *Amazon S3 User Guide*.
#' 
#' You can use the V2
#' [`delete_bucket_metadata_configuration`][s3_delete_bucket_metadata_configuration]
#' API operation with V1 or V2 metadata table configurations. However, if
#' you try to use the V1
#' [`delete_bucket_metadata_table_configuration`][s3_delete_bucket_metadata_table_configuration]
#' API operation with V2 configurations, you will receive an HTTP
#' `405 Method Not Allowed` error.
#' 
#' Make sure that you update your processes to use the new V2 API
#' operations
#' ([`create_bucket_metadata_configuration`][s3_create_bucket_metadata_configuration],
#' [`get_bucket_metadata_configuration`][s3_get_bucket_metadata_configuration],
#' and
#' [`delete_bucket_metadata_configuration`][s3_delete_bucket_metadata_configuration])
#' instead of the V1 API operations.
#' 
#' ### Permissions
#' 
#' To use this operation, you must have the
#' `s3:DeleteBucketMetadataTableConfiguration` permission. For more
#' information, see [Setting up permissions for configuring metadata
#' tables](https://docs.aws.amazon.com/AmazonS3/latest/userguide/metadata-tables-permissions.html)
#' in the *Amazon S3 User Guide*.
#' 
#' The following operations are related to
#' [`delete_bucket_metadata_table_configuration`][s3_delete_bucket_metadata_table_configuration]:
#' 
#' -   [`create_bucket_metadata_table_configuration`][s3_create_bucket_metadata_table_configuration]
#' 
#' -   [`get_bucket_metadata_table_configuration`][s3_get_bucket_metadata_table_configuration]
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#'
#' @usage
#' s3_delete_bucket_metadata_table_configuration(Bucket,
#'   ExpectedBucketOwner)
#'
#' @param Bucket &#91;required&#93; The general purpose bucket that you want to remove the metadata table
#' configuration from.
#' @param ExpectedBucketOwner The expected bucket owner of the general purpose bucket that you want to
#' remove the metadata table configuration from.
#'
#' @return
#' An empty list.
#'
#' @section Request syntax:
#' ```
#' svc$delete_bucket_metadata_table_configuration(
#'   Bucket = "string",
#'   ExpectedBucketOwner = "string"
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname s3_delete_bucket_metadata_table_configuration
#'
#' @aliases s3_delete_bucket_metadata_table_configuration
s3_delete_bucket_metadata_table_configuration <- function(Bucket, ExpectedBucketOwner = NULL) {
  op <- new_operation(
    name = "DeleteBucketMetadataTableConfiguration",
    http_method = "DELETE",
    http_path = "/{Bucket}?metadataTable",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .s3$delete_bucket_metadata_table_configuration_input(Bucket = Bucket, ExpectedBucketOwner = ExpectedBucketOwner)
  output <- .s3$delete_bucket_metadata_table_configuration_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$delete_bucket_metadata_table_configuration <- s3_delete_bucket_metadata_table_configuration

#' This operation is not supported for directory buckets
#'
#' @description
#' This operation is not supported for directory buckets.
#' 
#' Deletes a metrics configuration for the Amazon CloudWatch request
#' metrics (specified by the metrics configuration ID) from the bucket.
#' Note that this doesn't include the daily storage metrics.
#' 
#' To use this operation, you must have permissions to perform the
#' `s3:PutMetricsConfiguration` action. The bucket owner has this
#' permission by default. The bucket owner can grant this permission to
#' others. For more information about permissions, see [Permissions Related
#' to Bucket Subresource
#' Operations](https://docs.aws.amazon.com/AmazonS3/latest/userguide/security_iam_service-with-iam.html#security_iam_service-with-iam-id-based-policies-actions)
#' and [Managing Access Permissions to Your Amazon S3
#' Resources](https://docs.aws.amazon.com/AmazonS3/latest/userguide/security-iam.html).
#' 
#' For information about CloudWatch request metrics for Amazon S3, see
#' [Monitoring Metrics with Amazon
#' CloudWatch](https://docs.aws.amazon.com/AmazonS3/latest/userguide/cloudwatch-monitoring.html).
#' 
#' The following operations are related to
#' [`delete_bucket_metrics_configuration`][s3_delete_bucket_metrics_configuration]:
#' 
#' -   [`get_bucket_metrics_configuration`][s3_get_bucket_metrics_configuration]
#' 
#' -   [`put_bucket_metrics_configuration`][s3_put_bucket_metrics_configuration]
#' 
#' -   [`list_bucket_metrics_configurations`][s3_list_bucket_metrics_configurations]
#' 
#' -   [Monitoring Metrics with Amazon
#'     CloudWatch](https://docs.aws.amazon.com/AmazonS3/latest/userguide/cloudwatch-monitoring.html)
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#'
#' @usage
#' s3_delete_bucket_metrics_configuration(Bucket, Id, ExpectedBucketOwner)
#'
#' @param Bucket &#91;required&#93; The name of the bucket containing the metrics configuration to delete.
#' @param Id &#91;required&#93; The ID used to identify the metrics configuration. The ID has a 64
#' character limit and can only contain letters, numbers, periods, dashes,
#' and underscores.
#' @param ExpectedBucketOwner The account ID of the expected bucket owner. If the account ID that you
#' provide does not match the actual owner of the bucket, the request fails
#' with the HTTP status code `403 Forbidden` (access denied).
#'
#' @return
#' An empty list.
#'
#' @section Request syntax:
#' ```
#' svc$delete_bucket_metrics_configuration(
#'   Bucket = "string",
#'   Id = "string",
#'   ExpectedBucketOwner = "string"
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname s3_delete_bucket_metrics_configuration
#'
#' @aliases s3_delete_bucket_metrics_configuration
s3_delete_bucket_metrics_configuration <- function(Bucket, Id, ExpectedBucketOwner = NULL) {
  op <- new_operation(
    name = "DeleteBucketMetricsConfiguration",
    http_method = "DELETE",
    http_path = "/{Bucket}?metrics",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .s3$delete_bucket_metrics_configuration_input(Bucket = Bucket, Id = Id, ExpectedBucketOwner = ExpectedBucketOwner)
  output <- .s3$delete_bucket_metrics_configuration_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$delete_bucket_metrics_configuration <- s3_delete_bucket_metrics_configuration

#' This operation is not supported for directory buckets
#'
#' @description
#' This operation is not supported for directory buckets.
#' 
#' Removes `OwnershipControls` for an Amazon S3 bucket. To use this
#' operation, you must have the `s3:PutBucketOwnershipControls` permission.
#' For more information about Amazon S3 permissions, see [Specifying
#' Permissions in a
#' Policy](https://docs.aws.amazon.com/AmazonS3/latest/userguide/security_iam_service-with-iam.html#security_iam_service-with-iam-id-based-policies-actions).
#' 
#' For information about Amazon S3 Object Ownership, see [Using Object
#' Ownership](https://docs.aws.amazon.com/AmazonS3/latest/userguide/about-object-ownership.html).
#' 
#' The following operations are related to
#' [`delete_bucket_ownership_controls`][s3_delete_bucket_ownership_controls]:
#' 
#' -   [`get_bucket_ownership_controls`][s3_get_bucket_ownership_controls]
#' 
#' -   [`put_bucket_ownership_controls`][s3_put_bucket_ownership_controls]
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#'
#' @usage
#' s3_delete_bucket_ownership_controls(Bucket, ExpectedBucketOwner)
#'
#' @param Bucket &#91;required&#93; The Amazon S3 bucket whose `OwnershipControls` you want to delete.
#' @param ExpectedBucketOwner The account ID of the expected bucket owner. If the account ID that you
#' provide does not match the actual owner of the bucket, the request fails
#' with the HTTP status code `403 Forbidden` (access denied).
#'
#' @return
#' An empty list.
#'
#' @section Request syntax:
#' ```
#' svc$delete_bucket_ownership_controls(
#'   Bucket = "string",
#'   ExpectedBucketOwner = "string"
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname s3_delete_bucket_ownership_controls
#'
#' @aliases s3_delete_bucket_ownership_controls
s3_delete_bucket_ownership_controls <- function(Bucket, ExpectedBucketOwner = NULL) {
  op <- new_operation(
    name = "DeleteBucketOwnershipControls",
    http_method = "DELETE",
    http_path = "/{Bucket}?ownershipControls",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .s3$delete_bucket_ownership_controls_input(Bucket = Bucket, ExpectedBucketOwner = ExpectedBucketOwner)
  output <- .s3$delete_bucket_ownership_controls_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$delete_bucket_ownership_controls <- s3_delete_bucket_ownership_controls

#' Deletes the policy of a specified bucket
#'
#' @description
#' Deletes the policy of a specified bucket.
#' 
#' **Directory buckets** - For directory buckets, you must make requests
#' for this API operation to the Regional endpoint. These endpoints support
#' path-style requests in the format
#' `https://s3express-control.region-code.amazonaws.com/bucket-name `.
#' Virtual-hosted-style requests aren't supported. For more information
#' about endpoints in Availability Zones, see [Regional and Zonal endpoints
#' for directory buckets in Availability
#' Zones](https://docs.aws.amazon.com/AmazonS3/latest/userguide/endpoint-directory-buckets-AZ.html)
#' in the *Amazon S3 User Guide*. For more information about endpoints in
#' Local Zones, see [Concepts for directory buckets in Local
#' Zones](https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-lzs-for-directory-buckets.html)
#' in the *Amazon S3 User Guide*.
#' 
#' ### Permissions
#' 
#' If you are using an identity other than the root user of the Amazon Web
#' Services account that owns the bucket, the calling identity must both
#' have the [`delete_bucket_policy`][s3_delete_bucket_policy] permissions
#' on the specified bucket and belong to the bucket owner's account in
#' order to use this operation.
#' 
#' If you don't have [`delete_bucket_policy`][s3_delete_bucket_policy]
#' permissions, Amazon S3 returns a `403 Access Denied` error. If you have
#' the correct permissions, but you're not using an identity that belongs
#' to the bucket owner's account, Amazon S3 returns a
#' `405 Method Not Allowed` error.
#' 
#' To ensure that bucket owners don't inadvertently lock themselves out of
#' their own buckets, the root principal in a bucket owner's Amazon Web
#' Services account can perform the
#' [`get_bucket_policy`][s3_get_bucket_policy],
#' [`put_bucket_policy`][s3_put_bucket_policy], and
#' [`delete_bucket_policy`][s3_delete_bucket_policy] API actions, even if
#' their bucket policy explicitly denies the root principal's access.
#' Bucket owner root principals can only be blocked from performing these
#' API actions by VPC endpoint policies and Amazon Web Services
#' Organizations policies.
#' 
#' -   **General purpose bucket permissions** - The `s3:DeleteBucketPolicy`
#'     permission is required in a policy. For more information about
#'     general purpose buckets bucket policies, see [Using Bucket Policies
#'     and User
#'     Policies](https://docs.aws.amazon.com/AmazonS3/latest/userguide/security_iam_service-with-iam.html)
#'     in the *Amazon S3 User Guide*.
#' 
#' -   **Directory bucket permissions** - To grant access to this API
#'     operation, you must have the `s3express:DeleteBucketPolicy`
#'     permission in an IAM identity-based policy instead of a bucket
#'     policy. Cross-account access to this API operation isn't supported.
#'     This operation can only be performed by the Amazon Web Services
#'     account that owns the resource. For more information about directory
#'     bucket policies and permissions, see [Amazon Web Services Identity
#'     and Access Management (IAM) for S3 Express One
#'     Zone](https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-express-security-iam.html)
#'     in the *Amazon S3 User Guide*.
#' 
#' ### HTTP Host header syntax
#' 
#' **Directory buckets** - The HTTP Host header syntax is
#' `s3express-control.region-code.amazonaws.com`.
#' 
#' The following operations are related to
#' [`delete_bucket_policy`][s3_delete_bucket_policy]
#' 
#' -   [`create_bucket`][s3_create_bucket]
#' 
#' -   [`delete_object`][s3_delete_object]
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#'
#' @usage
#' s3_delete_bucket_policy(Bucket, ExpectedBucketOwner)
#'
#' @param Bucket &#91;required&#93; The bucket name.
#' 
#' **Directory buckets** - When you use this operation with a directory
#' bucket, you must use path-style requests in the format
#' `https://s3express-control.region-code.amazonaws.com/bucket-name `.
#' Virtual-hosted-style requests aren't supported. Directory bucket names
#' must be unique in the chosen Zone (Availability Zone or Local Zone).
#' Bucket names must also follow the format
#' ` bucket-base-name--zone-id--x-s3` (for example,
#' ` DOC-EXAMPLE-BUCKET--usw2-az1--x-s3`). For information about bucket
#' naming restrictions, see [Directory bucket naming
#' rules](https://docs.aws.amazon.com/AmazonS3/latest/userguide/directory-bucket-naming-rules.html)
#' in the *Amazon S3 User Guide*
#' @param ExpectedBucketOwner The account ID of the expected bucket owner. If the account ID that you
#' provide does not match the actual owner of the bucket, the request fails
#' with the HTTP status code `403 Forbidden` (access denied).
#' 
#' For directory buckets, this header is not supported in this API
#' operation. If you specify this header, the request fails with the HTTP
#' status code `501 Not Implemented`.
#'
#' @return
#' An empty list.
#'
#' @section Request syntax:
#' ```
#' svc$delete_bucket_policy(
#'   Bucket = "string",
#'   ExpectedBucketOwner = "string"
#' )
#' ```
#'
#' @examples
#' \dontrun{
#' # The following example deletes bucket policy on the specified bucket.
#' svc$delete_bucket_policy(
#'   Bucket = "examplebucket"
#' )
#' }
#'
#' @keywords internal
#'
#' @rdname s3_delete_bucket_policy
#'
#' @aliases s3_delete_bucket_policy
s3_delete_bucket_policy <- function(Bucket, ExpectedBucketOwner = NULL) {
  op <- new_operation(
    name = "DeleteBucketPolicy",
    http_method = "DELETE",
    http_path = "/{Bucket}?policy",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .s3$delete_bucket_policy_input(Bucket = Bucket, ExpectedBucketOwner = ExpectedBucketOwner)
  output <- .s3$delete_bucket_policy_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$delete_bucket_policy <- s3_delete_bucket_policy

#' This operation is not supported for directory buckets
#'
#' @description
#' This operation is not supported for directory buckets.
#' 
#' Deletes the replication configuration from the bucket.
#' 
#' To use this operation, you must have permissions to perform the
#' `s3:PutReplicationConfiguration` action. The bucket owner has these
#' permissions by default and can grant it to others. For more information
#' about permissions, see [Permissions Related to Bucket Subresource
#' Operations](https://docs.aws.amazon.com/AmazonS3/latest/userguide/security_iam_service-with-iam.html#security_iam_service-with-iam-id-based-policies-actions)
#' and [Managing Access Permissions to Your Amazon S3
#' Resources](https://docs.aws.amazon.com/AmazonS3/latest/userguide/security-iam.html).
#' 
#' It can take a while for the deletion of a replication configuration to
#' fully propagate.
#' 
#' For information about replication configuration, see
#' [Replication](https://docs.aws.amazon.com/AmazonS3/latest/userguide/replication.html)
#' in the *Amazon S3 User Guide*.
#' 
#' The following operations are related to
#' [`delete_bucket_replication`][s3_delete_bucket_replication]:
#' 
#' -   [`put_bucket_replication`][s3_put_bucket_replication]
#' 
#' -   [`get_bucket_replication`][s3_get_bucket_replication]
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#'
#' @usage
#' s3_delete_bucket_replication(Bucket, ExpectedBucketOwner)
#'
#' @param Bucket &#91;required&#93; The bucket name.
#' @param ExpectedBucketOwner The account ID of the expected bucket owner. If the account ID that you
#' provide does not match the actual owner of the bucket, the request fails
#' with the HTTP status code `403 Forbidden` (access denied).
#'
#' @return
#' An empty list.
#'
#' @section Request syntax:
#' ```
#' svc$delete_bucket_replication(
#'   Bucket = "string",
#'   ExpectedBucketOwner = "string"
#' )
#' ```
#'
#' @examples
#' \dontrun{
#' # The following example deletes replication configuration set on bucket.
#' svc$delete_bucket_replication(
#'   Bucket = "example"
#' )
#' }
#'
#' @keywords internal
#'
#' @rdname s3_delete_bucket_replication
#'
#' @aliases s3_delete_bucket_replication
s3_delete_bucket_replication <- function(Bucket, ExpectedBucketOwner = NULL) {
  op <- new_operation(
    name = "DeleteBucketReplication",
    http_method = "DELETE",
    http_path = "/{Bucket}?replication",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .s3$delete_bucket_replication_input(Bucket = Bucket, ExpectedBucketOwner = ExpectedBucketOwner)
  output <- .s3$delete_bucket_replication_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$delete_bucket_replication <- s3_delete_bucket_replication

#' This operation is not supported for directory buckets
#'
#' @description
#' This operation is not supported for directory buckets.
#' 
#' Deletes tags from the general purpose bucket if attribute based access
#' control (ABAC) is not enabled for the bucket. When you [enable ABAC for
#' a general purpose
#' bucket](https://docs.aws.amazon.com/AmazonS3/latest/userguide/buckets-tagging-enable-abac.html),
#' you can no longer use this operation for that bucket and must use
#' [UntagResource](https://docs.aws.amazon.com/AmazonS3/latest/API/API_control_UntagResource.html)
#' instead.
#' 
#' if ABAC is not enabled for the bucket. When you [enable ABAC for a
#' general purpose
#' bucket](https://docs.aws.amazon.com/AmazonS3/latest/userguide/buckets-tagging-enable-abac.html),
#' you can no longer use this operation for that bucket and must use
#' [UntagResource](https://docs.aws.amazon.com/AmazonS3/latest/API/API_control_UntagResource.html)
#' instead.
#' 
#' To use this operation, you must have permission to perform the
#' `s3:PutBucketTagging` action. By default, the bucket owner has this
#' permission and can grant this permission to others.
#' 
#' The following operations are related to
#' [`delete_bucket_tagging`][s3_delete_bucket_tagging]:
#' 
#' -   [`get_bucket_tagging`][s3_get_bucket_tagging]
#' 
#' -   [`put_bucket_tagging`][s3_put_bucket_tagging]
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#'
#' @usage
#' s3_delete_bucket_tagging(Bucket, ExpectedBucketOwner)
#'
#' @param Bucket &#91;required&#93; The bucket that has the tag set to be removed.
#' @param ExpectedBucketOwner The account ID of the expected bucket owner. If the account ID that you
#' provide does not match the actual owner of the bucket, the request fails
#' with the HTTP status code `403 Forbidden` (access denied).
#'
#' @return
#' An empty list.
#'
#' @section Request syntax:
#' ```
#' svc$delete_bucket_tagging(
#'   Bucket = "string",
#'   ExpectedBucketOwner = "string"
#' )
#' ```
#'
#' @examples
#' \dontrun{
#' # The following example deletes bucket tags.
#' svc$delete_bucket_tagging(
#'   Bucket = "examplebucket"
#' )
#' }
#'
#' @keywords internal
#'
#' @rdname s3_delete_bucket_tagging
#'
#' @aliases s3_delete_bucket_tagging
s3_delete_bucket_tagging <- function(Bucket, ExpectedBucketOwner = NULL) {
  op <- new_operation(
    name = "DeleteBucketTagging",
    http_method = "DELETE",
    http_path = "/{Bucket}?tagging",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .s3$delete_bucket_tagging_input(Bucket = Bucket, ExpectedBucketOwner = ExpectedBucketOwner)
  output <- .s3$delete_bucket_tagging_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$delete_bucket_tagging <- s3_delete_bucket_tagging

#' This operation is not supported for directory buckets
#'
#' @description
#' This operation is not supported for directory buckets.
#' 
#' This action removes the website configuration for a bucket. Amazon S3
#' returns a `200 OK` response upon successfully deleting a website
#' configuration on the specified bucket. You will get a `200 OK` response
#' if the website configuration you are trying to delete does not exist on
#' the bucket. Amazon S3 returns a `404` response if the bucket specified
#' in the request does not exist.
#' 
#' This DELETE action requires the `S3:DeleteBucketWebsite` permission. By
#' default, only the bucket owner can delete the website configuration
#' attached to a bucket. However, bucket owners can grant other users
#' permission to delete the website configuration by writing a bucket
#' policy granting them the `S3:DeleteBucketWebsite` permission.
#' 
#' For more information about hosting websites, see [Hosting Websites on
#' Amazon
#' S3](https://docs.aws.amazon.com/AmazonS3/latest/userguide/WebsiteHosting.html).
#' 
#' The following operations are related to
#' [`delete_bucket_website`][s3_delete_bucket_website]:
#' 
#' -   [`get_bucket_website`][s3_get_bucket_website]
#' 
#' -   [`put_bucket_website`][s3_put_bucket_website]
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#'
#' @usage
#' s3_delete_bucket_website(Bucket, ExpectedBucketOwner)
#'
#' @param Bucket &#91;required&#93; The bucket name for which you want to remove the website configuration.
#' @param ExpectedBucketOwner The account ID of the expected bucket owner. If the account ID that you
#' provide does not match the actual owner of the bucket, the request fails
#' with the HTTP status code `403 Forbidden` (access denied).
#'
#' @return
#' An empty list.
#'
#' @section Request syntax:
#' ```
#' svc$delete_bucket_website(
#'   Bucket = "string",
#'   ExpectedBucketOwner = "string"
#' )
#' ```
#'
#' @examples
#' \dontrun{
#' # The following example deletes bucket website configuration.
#' svc$delete_bucket_website(
#'   Bucket = "examplebucket"
#' )
#' }
#'
#' @keywords internal
#'
#' @rdname s3_delete_bucket_website
#'
#' @aliases s3_delete_bucket_website
s3_delete_bucket_website <- function(Bucket, ExpectedBucketOwner = NULL) {
  op <- new_operation(
    name = "DeleteBucketWebsite",
    http_method = "DELETE",
    http_path = "/{Bucket}?website",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .s3$delete_bucket_website_input(Bucket = Bucket, ExpectedBucketOwner = ExpectedBucketOwner)
  output <- .s3$delete_bucket_website_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$delete_bucket_website <- s3_delete_bucket_website

#' Removes an object from a bucket
#'
#' @description
#' Removes an object from a bucket. The behavior depends on the bucket's
#' versioning state:
#' 
#' -   If bucket versioning is not enabled, the operation permanently
#'     deletes the object.
#' 
#' -   If bucket versioning is enabled, the operation inserts a delete
#'     marker, which becomes the current version of the object. To
#'     permanently delete an object in a versioned bucket, you must include
#'     the object’s `versionId` in the request. For more information about
#'     versioning-enabled buckets, see [Deleting object versions from a
#'     versioning-enabled
#'     bucket](https://docs.aws.amazon.com/AmazonS3/latest/userguide/DeletingObjectVersions.html).
#' 
#' -   If bucket versioning is suspended, the operation removes the object
#'     that has a null `versionId`, if there is one, and inserts a delete
#'     marker that becomes the current version of the object. If there
#'     isn't an object with a null `versionId`, and all versions of the
#'     object have a `versionId`, Amazon S3 does not remove the object and
#'     only inserts a delete marker. To permanently delete an object that
#'     has a `versionId`, you must include the object’s `versionId` in the
#'     request. For more information about versioning-suspended buckets,
#'     see [Deleting objects from versioning-suspended
#'     buckets](https://docs.aws.amazon.com/AmazonS3/latest/userguide/DeletingObjectsfromVersioningSuspendedBuckets.html).
#' 
#' 
#' -   **Directory buckets** - S3 Versioning isn't enabled and supported
#'     for directory buckets. For this API operation, only the `null` value
#'     of the version ID is supported by directory buckets. You can only
#'     specify `null` to the `versionId` query parameter in the request.
#' 
#' -   **Directory buckets** - For directory buckets, you must make
#'     requests for this API operation to the Zonal endpoint. These
#'     endpoints support virtual-hosted-style requests in the format
#'     `https://amzn-s3-demo-bucket.s3express-zone-id.region-code.amazonaws.com/key-name `.
#'     Path-style requests are not supported. For more information about
#'     endpoints in Availability Zones, see [Regional and Zonal endpoints
#'     for directory buckets in Availability
#'     Zones](https://docs.aws.amazon.com/AmazonS3/latest/userguide/endpoint-directory-buckets-AZ.html)
#'     in the *Amazon S3 User Guide*. For more information about endpoints
#'     in Local Zones, see [Concepts for directory buckets in Local
#'     Zones](https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-lzs-for-directory-buckets.html)
#'     in the *Amazon S3 User Guide*.
#' 
#' To remove a specific version, you must use the `versionId` query
#' parameter. Using this query parameter permanently deletes the version.
#' If the object deleted is a delete marker, Amazon S3 sets the response
#' header `x-amz-delete-marker` to true.
#' 
#' If the object you want to delete is in a bucket where the bucket
#' versioning configuration is MFA Delete enabled, you must include the
#' `x-amz-mfa` request header in the DELETE `versionId` request. Requests
#' that include `x-amz-mfa` must use HTTPS. For more information about MFA
#' Delete, see [Using MFA
#' Delete](https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingMFADelete.html)
#' in the *Amazon S3 User Guide*. To see sample requests that use
#' versioning, see [Sample
#' Request](https://docs.aws.amazon.com/AmazonS3/latest/API/API_DeleteObject.html#ExampleVersionObjectDelete).
#' 
#' **Directory buckets** - MFA delete is not supported by directory
#' buckets.
#' 
#' You can delete objects by explicitly calling DELETE Object or calling
#' ([`put_bucket_lifecycle`][s3_put_bucket_lifecycle]) to enable Amazon S3
#' to remove them for you. If you want to block users or accounts from
#' removing or deleting objects from your bucket, you must deny them the
#' `s3:DeleteObject`, `s3:DeleteObjectVersion`, and
#' `s3:PutLifeCycleConfiguration` actions.
#' 
#' **Directory buckets** - S3 Lifecycle is not supported by directory
#' buckets.
#' 
#' ### Permissions
#' 
#' -   **General purpose bucket permissions** - The following permissions
#'     are required in your policies when your
#'     [`delete_objects`][s3_delete_objects] request includes specific
#'     headers.
#' 
#'     -   **`s3:DeleteObject`** - To delete an object from a bucket, you
#'         must always have the `s3:DeleteObject` permission.
#' 
#'     -   **`s3:DeleteObjectVersion`** - To delete a specific version of
#'         an object from a versioning-enabled bucket, you must have the
#'         `s3:DeleteObjectVersion` permission.
#' 
#'         If the `s3:DeleteObject` or `s3:DeleteObjectVersion` permissions
#'         are explicitly denied in your bucket policy, attempts to delete
#'         any unversioned objects result in a `403 Access Denied` error.
#' 
#' -   **Directory bucket permissions** - To grant access to this API
#'     operation on a directory bucket, we recommend that you use the
#'     [`create_session`](https://docs.aws.amazon.com/AmazonS3/latest/API/API_CreateSession.html)
#'     API operation for session-based authorization. Specifically, you
#'     grant the `s3express:CreateSession` permission to the directory
#'     bucket in a bucket policy or an IAM identity-based policy. Then, you
#'     make the [`create_session`][s3_create_session] API call on the
#'     bucket to obtain a session token. With the session token in your
#'     request header, you can make API requests to this operation. After
#'     the session token expires, you make another
#'     [`create_session`][s3_create_session] API call to generate a new
#'     session token for use. Amazon Web Services CLI or SDKs create
#'     session and refresh the session token automatically to avoid service
#'     interruptions when a session expires. For more information about
#'     authorization, see
#'     [`create_session`](https://docs.aws.amazon.com/AmazonS3/latest/API/API_CreateSession.html)
#'     .
#' 
#' ### HTTP Host header syntax
#' 
#' **Directory buckets** - The HTTP Host header syntax is
#' ` Bucket-name.s3express-zone-id.region-code.amazonaws.com`.
#' 
#' The following action is related to [`delete_object`][s3_delete_object]:
#' 
#' -   [`put_object`][s3_put_object]
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#' 
#' The `If-Match` header is supported for both general purpose and
#' directory buckets. `IfMatchLastModifiedTime` and `IfMatchSize` is only
#' supported for directory buckets.
#'
#' @usage
#' s3_delete_object(Bucket, Key, MFA, VersionId, RequestPayer,
#'   BypassGovernanceRetention, ExpectedBucketOwner, IfMatch,
#'   IfMatchLastModifiedTime, IfMatchSize)
#'
#' @param Bucket &#91;required&#93; The bucket name of the bucket containing the object.
#' 
#' **Directory buckets** - When you use this operation with a directory
#' bucket, you must use virtual-hosted-style requests in the format
#' ` Bucket-name.s3express-zone-id.region-code.amazonaws.com`. Path-style
#' requests are not supported. Directory bucket names must be unique in the
#' chosen Zone (Availability Zone or Local Zone). Bucket names must follow
#' the format ` bucket-base-name--zone-id--x-s3` (for example,
#' ` amzn-s3-demo-bucket--usw2-az1--x-s3`). For information about bucket
#' naming restrictions, see [Directory bucket naming
#' rules](https://docs.aws.amazon.com/AmazonS3/latest/userguide/directory-bucket-naming-rules.html)
#' in the *Amazon S3 User Guide*.
#' 
#' **Access points** - When you use this action with an access point for
#' general purpose buckets, you must provide the alias of the access point
#' in place of the bucket name or specify the access point ARN. When you
#' use this action with an access point for directory buckets, you must
#' provide the access point name in place of the bucket name. When using
#' the access point ARN, you must direct requests to the access point
#' hostname. The access point hostname takes the form
#' *AccessPointName*-*AccountId*.s3-accesspoint.*Region*.amazonaws.com.
#' When using this action with an access point through the Amazon Web
#' Services SDKs, you provide the access point ARN in place of the bucket
#' name. For more information about access point ARNs, see [Using access
#' points](https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-access-points.html)
#' in the *Amazon S3 User Guide*.
#' 
#' Object Lambda access points are not supported by directory buckets.
#' 
#' **S3 on Outposts** - When you use this action with S3 on Outposts, you
#' must direct requests to the S3 on Outposts hostname. The S3 on Outposts
#' hostname takes the form
#' ` AccessPointName-AccountId.outpostID.s3-outposts.Region.amazonaws.com`.
#' When you use this action with S3 on Outposts, the destination bucket
#' must be the Outposts access point ARN or the access point alias. For
#' more information about S3 on Outposts, see [What is S3 on
#' Outposts?](https://docs.aws.amazon.com/AmazonS3/latest/s3-outposts/S3onOutposts.html)
#' in the *Amazon S3 User Guide*.
#' @param Key &#91;required&#93; Key name of the object to delete.
#' @param MFA The concatenation of the authentication device's serial number, a space,
#' and the value that is displayed on your authentication device. Required
#' to permanently delete a versioned object if versioning is configured
#' with MFA delete enabled.
#' 
#' This functionality is not supported for directory buckets.
#' @param VersionId Version ID used to reference a specific version of the object.
#' 
#' For directory buckets in this API operation, only the `null` value of
#' the version ID is supported.
#' @param RequestPayer 
#' @param BypassGovernanceRetention Indicates whether S3 Object Lock should bypass Governance-mode
#' restrictions to process this operation. To use this header, you must
#' have the `s3:BypassGovernanceRetention` permission.
#' 
#' This functionality is not supported for directory buckets.
#' @param ExpectedBucketOwner The account ID of the expected bucket owner. If the account ID that you
#' provide does not match the actual owner of the bucket, the request fails
#' with the HTTP status code `403 Forbidden` (access denied).
#' @param IfMatch Deletes the object if the ETag (entity tag) value provided during the
#' delete operation matches the ETag of the object in S3. If the ETag
#' values do not match, the operation returns a `412 Precondition Failed`
#' error.
#' 
#' Expects the ETag value as a string. `If-Match` does accept a string
#' value of an '*' (asterisk) character to denote a match of any ETag.
#' 
#' For more information about conditional requests, see [RFC
#' 7232](https://datatracker.ietf.org/doc/html/rfc7232).
#' @param IfMatchLastModifiedTime If present, the object is deleted only if its modification times matches
#' the provided `Timestamp`. If the `Timestamp` values do not match, the
#' operation returns a `412 Precondition Failed` error. If the `Timestamp`
#' matches or if the object doesn’t exist, the operation returns a
#' `204 Success (No Content)` response.
#' 
#' This functionality is only supported for directory buckets.
#' @param IfMatchSize If present, the object is deleted only if its size matches the provided
#' size in bytes. If the `Size` value does not match, the operation returns
#' a `412 Precondition Failed` error. If the `Size` matches or if the
#' object doesn’t exist, the operation returns a `204 Success (No Content)`
#' response.
#' 
#' This functionality is only supported for directory buckets.
#' 
#' You can use the `If-Match`, `x-amz-if-match-last-modified-time` and
#' `x-amz-if-match-size` conditional headers in conjunction with each-other
#' or individually.
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   DeleteMarker = TRUE|FALSE,
#'   VersionId = "string",
#'   RequestCharged = "requester"
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$delete_object(
#'   Bucket = "string",
#'   Key = "string",
#'   MFA = "string",
#'   VersionId = "string",
#'   RequestPayer = "requester",
#'   BypassGovernanceRetention = TRUE|FALSE,
#'   ExpectedBucketOwner = "string",
#'   IfMatch = "string",
#'   IfMatchLastModifiedTime = as.POSIXct(
#'     "2015-01-01"
#'   ),
#'   IfMatchSize = 123
#' )
#' ```
#'
#' @examples
#' \dontrun{
#' # The following example deletes an object from an S3 bucket.
#' svc$delete_object(
#'   Bucket = "examplebucket",
#'   Key = "objectkey.jpg"
#' )
#' 
#' # The following example deletes an object from a non-versioned bucket.
#' svc$delete_object(
#'   Bucket = "ExampleBucket",
#'   Key = "HappyFace.jpg"
#' )
#' }
#'
#' @keywords internal
#'
#' @rdname s3_delete_object
#'
#' @aliases s3_delete_object
s3_delete_object <- function(Bucket, Key, MFA = NULL, VersionId = NULL, RequestPayer = NULL, BypassGovernanceRetention = NULL, ExpectedBucketOwner = NULL, IfMatch = NULL, IfMatchLastModifiedTime = NULL, IfMatchSize = NULL) {
  op <- new_operation(
    name = "DeleteObject",
    http_method = "DELETE",
    http_path = "/{Bucket}/{Key+}",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .s3$delete_object_input(Bucket = Bucket, Key = Key, MFA = MFA, VersionId = VersionId, RequestPayer = RequestPayer, BypassGovernanceRetention = BypassGovernanceRetention, ExpectedBucketOwner = ExpectedBucketOwner, IfMatch = IfMatch, IfMatchLastModifiedTime = IfMatchLastModifiedTime, IfMatchSize = IfMatchSize)
  output <- .s3$delete_object_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$delete_object <- s3_delete_object

#' This operation is not supported for directory buckets
#'
#' @description
#' This operation is not supported for directory buckets.
#' 
#' Removes the entire tag set from the specified object. For more
#' information about managing object tags, see [Object
#' Tagging](https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-tagging.html).
#' 
#' To use this operation, you must have permission to perform the
#' `s3:DeleteObjectTagging` action.
#' 
#' To delete tags of a specific object version, add the `versionId` query
#' parameter in the request. You will need permission for the
#' `s3:DeleteObjectVersionTagging` action.
#' 
#' The following operations are related to
#' [`delete_object_tagging`][s3_delete_object_tagging]:
#' 
#' -   [`put_object_tagging`][s3_put_object_tagging]
#' 
#' -   [`get_object_tagging`][s3_get_object_tagging]
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#'
#' @usage
#' s3_delete_object_tagging(Bucket, Key, VersionId, ExpectedBucketOwner)
#'
#' @param Bucket &#91;required&#93; The bucket name containing the objects from which to remove the tags.
#' 
#' **Access points** - When you use this action with an access point for
#' general purpose buckets, you must provide the alias of the access point
#' in place of the bucket name or specify the access point ARN. When you
#' use this action with an access point for directory buckets, you must
#' provide the access point name in place of the bucket name. When using
#' the access point ARN, you must direct requests to the access point
#' hostname. The access point hostname takes the form
#' *AccessPointName*-*AccountId*.s3-accesspoint.*Region*.amazonaws.com.
#' When using this action with an access point through the Amazon Web
#' Services SDKs, you provide the access point ARN in place of the bucket
#' name. For more information about access point ARNs, see [Using access
#' points](https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-access-points.html)
#' in the *Amazon S3 User Guide*.
#' 
#' **S3 on Outposts** - When you use this action with S3 on Outposts, you
#' must direct requests to the S3 on Outposts hostname. The S3 on Outposts
#' hostname takes the form
#' ` AccessPointName-AccountId.outpostID.s3-outposts.Region.amazonaws.com`.
#' When you use this action with S3 on Outposts, the destination bucket
#' must be the Outposts access point ARN or the access point alias. For
#' more information about S3 on Outposts, see [What is S3 on
#' Outposts?](https://docs.aws.amazon.com/AmazonS3/latest/s3-outposts/S3onOutposts.html)
#' in the *Amazon S3 User Guide*.
#' @param Key &#91;required&#93; The key that identifies the object in the bucket from which to remove
#' all tags.
#' @param VersionId The versionId of the object that the tag-set will be removed from.
#' @param ExpectedBucketOwner The account ID of the expected bucket owner. If the account ID that you
#' provide does not match the actual owner of the bucket, the request fails
#' with the HTTP status code `403 Forbidden` (access denied).
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   VersionId = "string"
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$delete_object_tagging(
#'   Bucket = "string",
#'   Key = "string",
#'   VersionId = "string",
#'   ExpectedBucketOwner = "string"
#' )
#' ```
#'
#' @examples
#' \dontrun{
#' # The following example removes tag set associated with the specified
#' # object version. The request specifies both the object key and object
#' # version.
#' svc$delete_object_tagging(
#'   Bucket = "examplebucket",
#'   Key = "HappyFace.jpg",
#'   VersionId = "ydlaNkwWm0SfKJR.T1b1fIdPRbldTYRI"
#' )
#' 
#' # The following example removes tag set associated with the specified
#' # object. If the bucket is versioning enabled, the operation removes tag
#' # set from the latest object version.
#' svc$delete_object_tagging(
#'   Bucket = "examplebucket",
#'   Key = "HappyFace.jpg"
#' )
#' }
#'
#' @keywords internal
#'
#' @rdname s3_delete_object_tagging
#'
#' @aliases s3_delete_object_tagging
s3_delete_object_tagging <- function(Bucket, Key, VersionId = NULL, ExpectedBucketOwner = NULL) {
  op <- new_operation(
    name = "DeleteObjectTagging",
    http_method = "DELETE",
    http_path = "/{Bucket}/{Key+}?tagging",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .s3$delete_object_tagging_input(Bucket = Bucket, Key = Key, VersionId = VersionId, ExpectedBucketOwner = ExpectedBucketOwner)
  output <- .s3$delete_object_tagging_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$delete_object_tagging <- s3_delete_object_tagging

#' This operation enables you to delete multiple objects from a bucket
#' using a single HTTP request
#'
#' @description
#' This operation enables you to delete multiple objects from a bucket
#' using a single HTTP request. If you know the object keys that you want
#' to delete, then this operation provides a suitable alternative to
#' sending individual delete requests, reducing per-request overhead.
#' 
#' The request can contain a list of up to 1,000 keys that you want to
#' delete. In the XML, you provide the object key names, and optionally,
#' version IDs if you want to delete a specific version of the object from
#' a versioning-enabled bucket. For each key, Amazon S3 performs a delete
#' operation and returns the result of that delete, success or failure, in
#' the response. If the object specified in the request isn't found, Amazon
#' S3 confirms the deletion by returning the result as deleted.
#' 
#' -   **Directory buckets** - S3 Versioning isn't enabled and supported
#'     for directory buckets.
#' 
#' -   **Directory buckets** - For directory buckets, you must make
#'     requests for this API operation to the Zonal endpoint. These
#'     endpoints support virtual-hosted-style requests in the format
#'     `https://amzn-s3-demo-bucket.s3express-zone-id.region-code.amazonaws.com/key-name `.
#'     Path-style requests are not supported. For more information about
#'     endpoints in Availability Zones, see [Regional and Zonal endpoints
#'     for directory buckets in Availability
#'     Zones](https://docs.aws.amazon.com/AmazonS3/latest/userguide/endpoint-directory-buckets-AZ.html)
#'     in the *Amazon S3 User Guide*. For more information about endpoints
#'     in Local Zones, see [Concepts for directory buckets in Local
#'     Zones](https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-lzs-for-directory-buckets.html)
#'     in the *Amazon S3 User Guide*.
#' 
#' The operation supports two modes for the response: verbose and quiet. By
#' default, the operation uses verbose mode in which the response includes
#' the result of deletion of each key in your request. In quiet mode the
#' response includes only keys where the delete operation encountered an
#' error. For a successful deletion in a quiet mode, the operation does not
#' return any information about the delete in the response body.
#' 
#' When performing this action on an MFA Delete enabled bucket, that
#' attempts to delete any versioned objects, you must include an MFA token.
#' If you do not provide one, the entire request will fail, even if there
#' are non-versioned objects you are trying to delete. If you provide an
#' invalid token, whether there are versioned keys in the request or not,
#' the entire Multi-Object Delete request will fail. For information about
#' MFA Delete, see [MFA
#' Delete](https://docs.aws.amazon.com/AmazonS3/latest/userguide/Versioning.html#MultiFactorAuthenticationDelete)
#' in the *Amazon S3 User Guide*.
#' 
#' **Directory buckets** - MFA delete is not supported by directory
#' buckets.
#' 
#' ### Permissions
#' 
#' -   **General purpose bucket permissions** - The following permissions
#'     are required in your policies when your
#'     [`delete_objects`][s3_delete_objects] request includes specific
#'     headers.
#' 
#'     -   **`s3:DeleteObject`** - To delete an object from a bucket, you
#'         must always specify the `s3:DeleteObject` permission.
#' 
#'     -   **`s3:DeleteObjectVersion`** - To delete a specific version of
#'         an object from a versioning-enabled bucket, you must specify the
#'         `s3:DeleteObjectVersion` permission.
#' 
#'         If the `s3:DeleteObject` or `s3:DeleteObjectVersion` permissions
#'         are explicitly denied in your bucket policy, attempts to delete
#'         any unversioned objects result in a `403 Access Denied` error.
#' 
#' -   **Directory bucket permissions** - To grant access to this API
#'     operation on a directory bucket, we recommend that you use the
#'     [`create_session`](https://docs.aws.amazon.com/AmazonS3/latest/API/API_CreateSession.html)
#'     API operation for session-based authorization. Specifically, you
#'     grant the `s3express:CreateSession` permission to the directory
#'     bucket in a bucket policy or an IAM identity-based policy. Then, you
#'     make the [`create_session`][s3_create_session] API call on the
#'     bucket to obtain a session token. With the session token in your
#'     request header, you can make API requests to this operation. After
#'     the session token expires, you make another
#'     [`create_session`][s3_create_session] API call to generate a new
#'     session token for use. Amazon Web Services CLI or SDKs create
#'     session and refresh the session token automatically to avoid service
#'     interruptions when a session expires. For more information about
#'     authorization, see
#'     [`create_session`](https://docs.aws.amazon.com/AmazonS3/latest/API/API_CreateSession.html)
#'     .
#' 
#' ### Content-MD5 request header
#' 
#' -   **General purpose bucket** - The Content-MD5 request header is
#'     required for all Multi-Object Delete requests. Amazon S3 uses the
#'     header value to ensure that your request body has not been altered
#'     in transit.
#' 
#' -   **Directory bucket** - The Content-MD5 request header or a
#'     additional checksum request header (including
#'     `x-amz-checksum-crc32`, `x-amz-checksum-crc32c`,
#'     `x-amz-checksum-sha1`, or `x-amz-checksum-sha256`) is required for
#'     all Multi-Object Delete requests.
#' 
#' ### HTTP Host header syntax
#' 
#' **Directory buckets** - The HTTP Host header syntax is
#' ` Bucket-name.s3express-zone-id.region-code.amazonaws.com`.
#' 
#' The following operations are related to
#' [`delete_objects`][s3_delete_objects]:
#' 
#' -   [`create_multipart_upload`][s3_create_multipart_upload]
#' 
#' -   [`upload_part`][s3_upload_part]
#' 
#' -   [`complete_multipart_upload`][s3_complete_multipart_upload]
#' 
#' -   [`list_parts`][s3_list_parts]
#' 
#' -   [`abort_multipart_upload`][s3_abort_multipart_upload]
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#'
#' @usage
#' s3_delete_objects(Bucket, Delete, MFA, RequestPayer,
#'   BypassGovernanceRetention, ExpectedBucketOwner, ChecksumAlgorithm)
#'
#' @param Bucket &#91;required&#93; The bucket name containing the objects to delete.
#' 
#' **Directory buckets** - When you use this operation with a directory
#' bucket, you must use virtual-hosted-style requests in the format
#' ` Bucket-name.s3express-zone-id.region-code.amazonaws.com`. Path-style
#' requests are not supported. Directory bucket names must be unique in the
#' chosen Zone (Availability Zone or Local Zone). Bucket names must follow
#' the format ` bucket-base-name--zone-id--x-s3` (for example,
#' ` amzn-s3-demo-bucket--usw2-az1--x-s3`). For information about bucket
#' naming restrictions, see [Directory bucket naming
#' rules](https://docs.aws.amazon.com/AmazonS3/latest/userguide/directory-bucket-naming-rules.html)
#' in the *Amazon S3 User Guide*.
#' 
#' **Access points** - When you use this action with an access point for
#' general purpose buckets, you must provide the alias of the access point
#' in place of the bucket name or specify the access point ARN. When you
#' use this action with an access point for directory buckets, you must
#' provide the access point name in place of the bucket name. When using
#' the access point ARN, you must direct requests to the access point
#' hostname. The access point hostname takes the form
#' *AccessPointName*-*AccountId*.s3-accesspoint.*Region*.amazonaws.com.
#' When using this action with an access point through the Amazon Web
#' Services SDKs, you provide the access point ARN in place of the bucket
#' name. For more information about access point ARNs, see [Using access
#' points](https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-access-points.html)
#' in the *Amazon S3 User Guide*.
#' 
#' Object Lambda access points are not supported by directory buckets.
#' 
#' **S3 on Outposts** - When you use this action with S3 on Outposts, you
#' must direct requests to the S3 on Outposts hostname. The S3 on Outposts
#' hostname takes the form
#' ` AccessPointName-AccountId.outpostID.s3-outposts.Region.amazonaws.com`.
#' When you use this action with S3 on Outposts, the destination bucket
#' must be the Outposts access point ARN or the access point alias. For
#' more information about S3 on Outposts, see [What is S3 on
#' Outposts?](https://docs.aws.amazon.com/AmazonS3/latest/s3-outposts/S3onOutposts.html)
#' in the *Amazon S3 User Guide*.
#' @param Delete &#91;required&#93; Container for the request.
#' @param MFA The concatenation of the authentication device's serial number, a space,
#' and the value that is displayed on your authentication device. Required
#' to permanently delete a versioned object if versioning is configured
#' with MFA delete enabled.
#' 
#' When performing the [`delete_objects`][s3_delete_objects] operation on
#' an MFA delete enabled bucket, which attempts to delete the specified
#' versioned objects, you must include an MFA token. If you don't provide
#' an MFA token, the entire request will fail, even if there are
#' non-versioned objects that you are trying to delete. If you provide an
#' invalid token, whether there are versioned object keys in the request or
#' not, the entire Multi-Object Delete request will fail. For information
#' about MFA Delete, see [MFA
#' Delete](https://docs.aws.amazon.com/AmazonS3/latest/userguide/Versioning.html#MultiFactorAuthenticationDelete)
#' in the *Amazon S3 User Guide*.
#' 
#' This functionality is not supported for directory buckets.
#' @param RequestPayer 
#' @param BypassGovernanceRetention Specifies whether you want to delete this object even if it has a
#' Governance-type Object Lock in place. To use this header, you must have
#' the `s3:BypassGovernanceRetention` permission.
#' 
#' This functionality is not supported for directory buckets.
#' @param ExpectedBucketOwner The account ID of the expected bucket owner. If the account ID that you
#' provide does not match the actual owner of the bucket, the request fails
#' with the HTTP status code `403 Forbidden` (access denied).
#' @param ChecksumAlgorithm Indicates the algorithm used to create the checksum for the object when
#' you use the SDK. This header will not provide any additional
#' functionality if you don't use the SDK. When you send this header, there
#' must be a corresponding `x-amz-checksum-algorithm ` or `x-amz-trailer`
#' header sent. Otherwise, Amazon S3 fails the request with the HTTP status
#' code `400 Bad Request`.
#' 
#' For the `x-amz-checksum-algorithm ` header, replace ` algorithm ` with
#' the supported algorithm from the following list:
#' 
#' -   `CRC32`
#' 
#' -   `CRC32C`
#' 
#' -   `CRC64NVME`
#' 
#' -   `SHA1`
#' 
#' -   `SHA256`
#' 
#' For more information, see [Checking object
#' integrity](https://docs.aws.amazon.com/AmazonS3/latest/userguide/checking-object-integrity.html)
#' in the *Amazon S3 User Guide*.
#' 
#' If the individual checksum value you provide through
#' `x-amz-checksum-algorithm ` doesn't match the checksum algorithm you set
#' through `x-amz-sdk-checksum-algorithm`, Amazon S3 fails the request with
#' a `BadDigest` error.
#' 
#' If you provide an individual checksum, Amazon S3 ignores any provided
#' `ChecksumAlgorithm` parameter.
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   Deleted = list(
#'     list(
#'       Key = "string",
#'       VersionId = "string",
#'       DeleteMarker = TRUE|FALSE,
#'       DeleteMarkerVersionId = "string"
#'     )
#'   ),
#'   RequestCharged = "requester",
#'   Errors = list(
#'     list(
#'       Key = "string",
#'       VersionId = "string",
#'       Code = "string",
#'       Message = "string"
#'     )
#'   )
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$delete_objects(
#'   Bucket = "string",
#'   Delete = list(
#'     Objects = list(
#'       list(
#'         Key = "string",
#'         VersionId = "string",
#'         ETag = "string",
#'         LastModifiedTime = as.POSIXct(
#'           "2015-01-01"
#'         ),
#'         Size = 123
#'       )
#'     ),
#'     Quiet = TRUE|FALSE
#'   ),
#'   MFA = "string",
#'   RequestPayer = "requester",
#'   BypassGovernanceRetention = TRUE|FALSE,
#'   ExpectedBucketOwner = "string",
#'   ChecksumAlgorithm = "CRC32"|"CRC32C"|"SHA1"|"SHA256"|"CRC64NVME"
#' )
#' ```
#'
#' @examples
#' \dontrun{
#' # The following example deletes objects from a bucket. The request
#' # specifies object versions. S3 deletes specific object versions and
#' # returns the key and versions of deleted objects in the response.
#' svc$delete_objects(
#'   Bucket = "examplebucket",
#'   Delete = list(
#'     Objects = list(
#'       list(
#'         Key = "HappyFace.jpg",
#'         VersionId = "2LWg7lQLnY41.maGB5Z6SWW.dcq0vx7b"
#'       ),
#'       list(
#'         Key = "HappyFace.jpg",
#'         VersionId = "yoz3HB.ZhCS_tKVEmIOr7qYyyAaZSKVd"
#'       )
#'     ),
#'     Quiet = FALSE
#'   )
#' )
#' 
#' # The following example deletes objects from a bucket. The bucket is
#' # versioned, and the request does not specify the object version to
#' # delete. In this case, all versions remain in the bucket and S3 adds a
#' # delete marker.
#' svc$delete_objects(
#'   Bucket = "examplebucket",
#'   Delete = list(
#'     Objects = list(
#'       list(
#'         Key = "objectkey1"
#'       ),
#'       list(
#'         Key = "objectkey2"
#'       )
#'     ),
#'     Quiet = FALSE
#'   )
#' )
#' }
#'
#' @keywords internal
#'
#' @rdname s3_delete_objects
#'
#' @aliases s3_delete_objects
s3_delete_objects <- function(Bucket, Delete, MFA = NULL, RequestPayer = NULL, BypassGovernanceRetention = NULL, ExpectedBucketOwner = NULL, ChecksumAlgorithm = NULL) {
  op <- new_operation(
    name = "DeleteObjects",
    http_method = "POST",
    http_path = "/{Bucket}?delete",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .s3$delete_objects_input(Bucket = Bucket, Delete = Delete, MFA = MFA, RequestPayer = RequestPayer, BypassGovernanceRetention = BypassGovernanceRetention, ExpectedBucketOwner = ExpectedBucketOwner, ChecksumAlgorithm = ChecksumAlgorithm)
  output <- .s3$delete_objects_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$delete_objects <- s3_delete_objects

#' This operation is not supported for directory buckets
#'
#' @description
#' This operation is not supported for directory buckets.
#' 
#' Removes the `PublicAccessBlock` configuration for an Amazon S3 bucket.
#' This operation removes the bucket-level configuration only. The
#' effective public access behavior will still be governed by account-level
#' settings (which may inherit from organization-level policies). To use
#' this operation, you must have the `s3:PutBucketPublicAccessBlock`
#' permission. For more information about permissions, see [Permissions
#' Related to Bucket Subresource
#' Operations](https://docs.aws.amazon.com/AmazonS3/latest/userguide/security_iam_service-with-iam.html#security_iam_service-with-iam-id-based-policies-actions)
#' and [Managing Access Permissions to Your Amazon S3
#' Resources](https://docs.aws.amazon.com/AmazonS3/latest/userguide/security-iam.html).
#' 
#' The following operations are related to
#' [`delete_public_access_block`][s3_delete_public_access_block]:
#' 
#' -   [Using Amazon S3 Block Public
#'     Access](https://docs.aws.amazon.com/AmazonS3/latest/userguide/access-control-block-public-access.html)
#' 
#' -   [`get_public_access_block`][s3_get_public_access_block]
#' 
#' -   [`put_public_access_block`][s3_put_public_access_block]
#' 
#' -   [`get_bucket_policy_status`][s3_get_bucket_policy_status]
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#'
#' @usage
#' s3_delete_public_access_block(Bucket, ExpectedBucketOwner)
#'
#' @param Bucket &#91;required&#93; The Amazon S3 bucket whose `PublicAccessBlock` configuration you want to
#' delete.
#' @param ExpectedBucketOwner The account ID of the expected bucket owner. If the account ID that you
#' provide does not match the actual owner of the bucket, the request fails
#' with the HTTP status code `403 Forbidden` (access denied).
#'
#' @return
#' An empty list.
#'
#' @section Request syntax:
#' ```
#' svc$delete_public_access_block(
#'   Bucket = "string",
#'   ExpectedBucketOwner = "string"
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname s3_delete_public_access_block
#'
#' @aliases s3_delete_public_access_block
s3_delete_public_access_block <- function(Bucket, ExpectedBucketOwner = NULL) {
  op <- new_operation(
    name = "DeletePublicAccessBlock",
    http_method = "DELETE",
    http_path = "/{Bucket}?publicAccessBlock",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .s3$delete_public_access_block_input(Bucket = Bucket, ExpectedBucketOwner = ExpectedBucketOwner)
  output <- .s3$delete_public_access_block_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$delete_public_access_block <- s3_delete_public_access_block

#' Returns the attribute-based access control (ABAC) property of the
#' general purpose bucket
#'
#' @description
#' Returns the attribute-based access control (ABAC) property of the
#' general purpose bucket. If ABAC is enabled on your bucket, you can use
#' tags on the bucket for access control. For more information, see
#' [Enabling ABAC in general purpose
#' buckets](https://docs.aws.amazon.com/AmazonS3/latest/userguide/buckets-tagging-enable-abac.html).
#'
#' @usage
#' s3_get_bucket_abac(Bucket, ExpectedBucketOwner)
#'
#' @param Bucket &#91;required&#93; The name of the general purpose bucket.
#' @param ExpectedBucketOwner The Amazon Web Services account ID of the general purpose bucket's
#' owner.
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   AbacStatus = list(
#'     Status = "Enabled"|"Disabled"
#'   )
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$get_bucket_abac(
#'   Bucket = "string",
#'   ExpectedBucketOwner = "string"
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname s3_get_bucket_abac
#'
#' @aliases s3_get_bucket_abac
s3_get_bucket_abac <- function(Bucket, ExpectedBucketOwner = NULL) {
  op <- new_operation(
    name = "GetBucketAbac",
    http_method = "GET",
    http_path = "/{Bucket}?abac",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .s3$get_bucket_abac_input(Bucket = Bucket, ExpectedBucketOwner = ExpectedBucketOwner)
  output <- .s3$get_bucket_abac_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$get_bucket_abac <- s3_get_bucket_abac

#' This operation is not supported for directory buckets
#'
#' @description
#' This operation is not supported for directory buckets.
#' 
#' This implementation of the GET action uses the `accelerate` subresource
#' to return the Transfer Acceleration state of a bucket, which is either
#' `Enabled` or `Suspended`. Amazon S3 Transfer Acceleration is a
#' bucket-level feature that enables you to perform faster data transfers
#' to and from Amazon S3.
#' 
#' To use this operation, you must have permission to perform the
#' `s3:GetAccelerateConfiguration` action. The bucket owner has this
#' permission by default. The bucket owner can grant this permission to
#' others. For more information about permissions, see [Permissions Related
#' to Bucket Subresource
#' Operations](https://docs.aws.amazon.com/AmazonS3/latest/userguide/security_iam_service-with-iam.html#security_iam_service-with-iam-id-based-policies-actions)
#' and [Managing Access Permissions to your Amazon S3
#' Resources](https://docs.aws.amazon.com/AmazonS3/latest/userguide/security-iam.html)
#' in the *Amazon S3 User Guide*.
#' 
#' You set the Transfer Acceleration state of an existing bucket to
#' `Enabled` or `Suspended` by using the
#' [`put_bucket_accelerate_configuration`][s3_put_bucket_accelerate_configuration]
#' operation.
#' 
#' A GET `accelerate` request does not return a state value for a bucket
#' that has no transfer acceleration state. A bucket has no Transfer
#' Acceleration state if a state has never been set on the bucket.
#' 
#' For more information about transfer acceleration, see [Transfer
#' Acceleration](https://docs.aws.amazon.com/AmazonS3/latest/userguide/transfer-acceleration.html)
#' in the Amazon S3 User Guide.
#' 
#' The following operations are related to
#' [`get_bucket_accelerate_configuration`][s3_get_bucket_accelerate_configuration]:
#' 
#' -   [`put_bucket_accelerate_configuration`][s3_put_bucket_accelerate_configuration]
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#'
#' @usage
#' s3_get_bucket_accelerate_configuration(Bucket, ExpectedBucketOwner,
#'   RequestPayer)
#'
#' @param Bucket &#91;required&#93; The name of the bucket for which the accelerate configuration is
#' retrieved.
#' @param ExpectedBucketOwner The account ID of the expected bucket owner. If the account ID that you
#' provide does not match the actual owner of the bucket, the request fails
#' with the HTTP status code `403 Forbidden` (access denied).
#' @param RequestPayer 
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   Status = "Enabled"|"Suspended",
#'   RequestCharged = "requester"
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$get_bucket_accelerate_configuration(
#'   Bucket = "string",
#'   ExpectedBucketOwner = "string",
#'   RequestPayer = "requester"
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname s3_get_bucket_accelerate_configuration
#'
#' @aliases s3_get_bucket_accelerate_configuration
s3_get_bucket_accelerate_configuration <- function(Bucket, ExpectedBucketOwner = NULL, RequestPayer = NULL) {
  op <- new_operation(
    name = "GetBucketAccelerateConfiguration",
    http_method = "GET",
    http_path = "/{Bucket}?accelerate",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .s3$get_bucket_accelerate_configuration_input(Bucket = Bucket, ExpectedBucketOwner = ExpectedBucketOwner, RequestPayer = RequestPayer)
  output <- .s3$get_bucket_accelerate_configuration_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$get_bucket_accelerate_configuration <- s3_get_bucket_accelerate_configuration

#' This operation is not supported for directory buckets
#'
#' @description
#' This operation is not supported for directory buckets.
#' 
#' This implementation of the `GET` action uses the `acl` subresource to
#' return the access control list (ACL) of a bucket. To use `GET` to return
#' the ACL of the bucket, you must have the `READ_ACP` access to the
#' bucket. If `READ_ACP` permission is granted to the anonymous user, you
#' can return the ACL of the bucket without using an authorization header.
#' 
#' When you use this API operation with an access point, provide the alias
#' of the access point in place of the bucket name.
#' 
#' When you use this API operation with an Object Lambda access point,
#' provide the alias of the Object Lambda access point in place of the
#' bucket name. If the Object Lambda access point alias in a request is not
#' valid, the error code `InvalidAccessPointAliasError` is returned. For
#' more information about `InvalidAccessPointAliasError`, see [List of
#' Error
#' Codes](https://docs.aws.amazon.com/AmazonS3/latest/API/ErrorResponses.html#ErrorCodeList).
#' 
#' If your bucket uses the bucket owner enforced setting for S3 Object
#' Ownership, requests to read ACLs are still supported and return the
#' `bucket-owner-full-control` ACL with the owner being the account that
#' created the bucket. For more information, see [Controlling object
#' ownership and disabling
#' ACLs](https://docs.aws.amazon.com/AmazonS3/latest/userguide/about-object-ownership.html)
#' in the *Amazon S3 User Guide*.
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#' 
#' The following operations are related to
#' [`get_bucket_acl`][s3_get_bucket_acl]:
#' 
#' -   [`list_objects`][s3_list_objects]
#'
#' @usage
#' s3_get_bucket_acl(Bucket, ExpectedBucketOwner)
#'
#' @param Bucket &#91;required&#93; Specifies the S3 bucket whose ACL is being requested.
#' 
#' When you use this API operation with an access point, provide the alias
#' of the access point in place of the bucket name.
#' 
#' When you use this API operation with an Object Lambda access point,
#' provide the alias of the Object Lambda access point in place of the
#' bucket name. If the Object Lambda access point alias in a request is not
#' valid, the error code `InvalidAccessPointAliasError` is returned. For
#' more information about `InvalidAccessPointAliasError`, see [List of
#' Error
#' Codes](https://docs.aws.amazon.com/AmazonS3/latest/API/ErrorResponses.html#ErrorCodeList).
#' @param ExpectedBucketOwner The account ID of the expected bucket owner. If the account ID that you
#' provide does not match the actual owner of the bucket, the request fails
#' with the HTTP status code `403 Forbidden` (access denied).
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   Owner = list(
#'     DisplayName = "string",
#'     ID = "string"
#'   ),
#'   Grants = list(
#'     list(
#'       Grantee = list(
#'         DisplayName = "string",
#'         EmailAddress = "string",
#'         ID = "string",
#'         Type = "CanonicalUser"|"AmazonCustomerByEmail"|"Group",
#'         URI = "string"
#'       ),
#'       Permission = "FULL_CONTROL"|"WRITE"|"WRITE_ACP"|"READ"|"READ_ACP"
#'     )
#'   )
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$get_bucket_acl(
#'   Bucket = "string",
#'   ExpectedBucketOwner = "string"
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname s3_get_bucket_acl
#'
#' @aliases s3_get_bucket_acl
s3_get_bucket_acl <- function(Bucket, ExpectedBucketOwner = NULL) {
  op <- new_operation(
    name = "GetBucketAcl",
    http_method = "GET",
    http_path = "/{Bucket}?acl",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .s3$get_bucket_acl_input(Bucket = Bucket, ExpectedBucketOwner = ExpectedBucketOwner)
  output <- .s3$get_bucket_acl_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$get_bucket_acl <- s3_get_bucket_acl

#' This operation is not supported for directory buckets
#'
#' @description
#' This operation is not supported for directory buckets.
#' 
#' This implementation of the GET action returns an analytics configuration
#' (identified by the analytics configuration ID) from the bucket.
#' 
#' To use this operation, you must have permissions to perform the
#' `s3:GetAnalyticsConfiguration` action. The bucket owner has this
#' permission by default. The bucket owner can grant this permission to
#' others. For more information about permissions, see [Permissions Related
#' to Bucket Subresource
#' Operations](https://docs.aws.amazon.com/AmazonS3/latest/userguide/security_iam_service-with-iam.html#security_iam_service-with-iam-id-based-policies-actions)
#' and [Managing Access Permissions to Your Amazon S3
#' Resources](https://docs.aws.amazon.com/AmazonS3/latest/userguide/security-iam.html)
#' in the *Amazon S3 User Guide*.
#' 
#' For information about Amazon S3 analytics feature, see [Amazon S3
#' Analytics – Storage Class
#' Analysis](https://docs.aws.amazon.com/AmazonS3/latest/userguide/analytics-storage-class.html)
#' in the *Amazon S3 User Guide*.
#' 
#' The following operations are related to
#' [`get_bucket_analytics_configuration`][s3_get_bucket_analytics_configuration]:
#' 
#' -   [`delete_bucket_analytics_configuration`][s3_delete_bucket_analytics_configuration]
#' 
#' -   [`list_bucket_analytics_configurations`][s3_list_bucket_analytics_configurations]
#' 
#' -   [`put_bucket_analytics_configuration`][s3_put_bucket_analytics_configuration]
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#'
#' @usage
#' s3_get_bucket_analytics_configuration(Bucket, Id, ExpectedBucketOwner)
#'
#' @param Bucket &#91;required&#93; The name of the bucket from which an analytics configuration is
#' retrieved.
#' @param Id &#91;required&#93; The ID that identifies the analytics configuration.
#' @param ExpectedBucketOwner The account ID of the expected bucket owner. If the account ID that you
#' provide does not match the actual owner of the bucket, the request fails
#' with the HTTP status code `403 Forbidden` (access denied).
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   AnalyticsConfiguration = list(
#'     Id = "string",
#'     Filter = list(
#'       Prefix = "string",
#'       Tag = list(
#'         Key = "string",
#'         Value = "string"
#'       ),
#'       And = list(
#'         Prefix = "string",
#'         Tags = list(
#'           list(
#'             Key = "string",
#'             Value = "string"
#'           )
#'         )
#'       )
#'     ),
#'     StorageClassAnalysis = list(
#'       DataExport = list(
#'         OutputSchemaVersion = "V_1",
#'         Destination = list(
#'           S3BucketDestination = list(
#'             Format = "CSV",
#'             BucketAccountId = "string",
#'             Bucket = "string",
#'             Prefix = "string"
#'           )
#'         )
#'       )
#'     )
#'   )
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$get_bucket_analytics_configuration(
#'   Bucket = "string",
#'   Id = "string",
#'   ExpectedBucketOwner = "string"
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname s3_get_bucket_analytics_configuration
#'
#' @aliases s3_get_bucket_analytics_configuration
s3_get_bucket_analytics_configuration <- function(Bucket, Id, ExpectedBucketOwner = NULL) {
  op <- new_operation(
    name = "GetBucketAnalyticsConfiguration",
    http_method = "GET",
    http_path = "/{Bucket}?analytics",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .s3$get_bucket_analytics_configuration_input(Bucket = Bucket, Id = Id, ExpectedBucketOwner = ExpectedBucketOwner)
  output <- .s3$get_bucket_analytics_configuration_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$get_bucket_analytics_configuration <- s3_get_bucket_analytics_configuration

#' This operation is not supported for directory buckets
#'
#' @description
#' This operation is not supported for directory buckets.
#' 
#' Returns the Cross-Origin Resource Sharing (CORS) configuration
#' information set for the bucket.
#' 
#' To use this operation, you must have permission to perform the
#' `s3:GetBucketCORS` action. By default, the bucket owner has this
#' permission and can grant it to others.
#' 
#' When you use this API operation with an access point, provide the alias
#' of the access point in place of the bucket name.
#' 
#' When you use this API operation with an Object Lambda access point,
#' provide the alias of the Object Lambda access point in place of the
#' bucket name. If the Object Lambda access point alias in a request is not
#' valid, the error code `InvalidAccessPointAliasError` is returned. For
#' more information about `InvalidAccessPointAliasError`, see [List of
#' Error
#' Codes](https://docs.aws.amazon.com/AmazonS3/latest/API/ErrorResponses.html#ErrorCodeList).
#' 
#' For more information about CORS, see [Enabling Cross-Origin Resource
#' Sharing](https://docs.aws.amazon.com/AmazonS3/latest/userguide/cors.html).
#' 
#' The following operations are related to
#' [`get_bucket_cors`][s3_get_bucket_cors]:
#' 
#' -   [`put_bucket_cors`][s3_put_bucket_cors]
#' 
#' -   [`delete_bucket_cors`][s3_delete_bucket_cors]
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#'
#' @usage
#' s3_get_bucket_cors(Bucket, ExpectedBucketOwner)
#'
#' @param Bucket &#91;required&#93; The bucket name for which to get the cors configuration.
#' 
#' When you use this API operation with an access point, provide the alias
#' of the access point in place of the bucket name.
#' 
#' When you use this API operation with an Object Lambda access point,
#' provide the alias of the Object Lambda access point in place of the
#' bucket name. If the Object Lambda access point alias in a request is not
#' valid, the error code `InvalidAccessPointAliasError` is returned. For
#' more information about `InvalidAccessPointAliasError`, see [List of
#' Error
#' Codes](https://docs.aws.amazon.com/AmazonS3/latest/API/ErrorResponses.html#ErrorCodeList).
#' @param ExpectedBucketOwner The account ID of the expected bucket owner. If the account ID that you
#' provide does not match the actual owner of the bucket, the request fails
#' with the HTTP status code `403 Forbidden` (access denied).
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   CORSRules = list(
#'     list(
#'       ID = "string",
#'       AllowedHeaders = list(
#'         "string"
#'       ),
#'       AllowedMethods = list(
#'         "string"
#'       ),
#'       AllowedOrigins = list(
#'         "string"
#'       ),
#'       ExposeHeaders = list(
#'         "string"
#'       ),
#'       MaxAgeSeconds = 123
#'     )
#'   )
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$get_bucket_cors(
#'   Bucket = "string",
#'   ExpectedBucketOwner = "string"
#' )
#' ```
#'
#' @examples
#' \dontrun{
#' # The following example returns cross-origin resource sharing (CORS)
#' # configuration set on a bucket.
#' svc$get_bucket_cors(
#'   Bucket = "examplebucket"
#' )
#' }
#'
#' @keywords internal
#'
#' @rdname s3_get_bucket_cors
#'
#' @aliases s3_get_bucket_cors
s3_get_bucket_cors <- function(Bucket, ExpectedBucketOwner = NULL) {
  op <- new_operation(
    name = "GetBucketCors",
    http_method = "GET",
    http_path = "/{Bucket}?cors",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .s3$get_bucket_cors_input(Bucket = Bucket, ExpectedBucketOwner = ExpectedBucketOwner)
  output <- .s3$get_bucket_cors_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$get_bucket_cors <- s3_get_bucket_cors

#' Returns the default encryption configuration for an Amazon S3 bucket
#'
#' @description
#' Returns the default encryption configuration for an Amazon S3 bucket. By
#' default, all buckets have a default encryption configuration that uses
#' server-side encryption with Amazon S3 managed keys (SSE-S3). This
#' operation also returns the
#' [BucketKeyEnabled](https://docs.aws.amazon.com/AmazonS3/latest/API/API_ServerSideEncryptionRule.html#AmazonS3-Type-ServerSideEncryptionRule-BucketKeyEnabled)
#' and
#' [BlockedEncryptionTypes](https://docs.aws.amazon.com/AmazonS3/latest/API/API_ServerSideEncryptionRule.html#AmazonS3-Type-ServerSideEncryptionRule-BlockedEncryptionTypes)
#' statuses.
#' 
#' -   **General purpose buckets** - For information about the bucket
#'     default encryption feature, see [Amazon S3 Bucket Default
#'     Encryption](https://docs.aws.amazon.com/AmazonS3/latest/userguide/bucket-encryption.html)
#'     in the *Amazon S3 User Guide*.
#' 
#' -   **Directory buckets** - For directory buckets, there are only two
#'     supported options for server-side encryption: SSE-S3 and SSE-KMS.
#'     For information about the default encryption configuration in
#'     directory buckets, see [Setting default server-side encryption
#'     behavior for directory
#'     buckets](https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-express-bucket-encryption.html).
#' 
#' ### Permissions
#' 
#' -   **General purpose bucket permissions** - The
#'     `s3:GetEncryptionConfiguration` permission is required in a policy.
#'     The bucket owner has this permission by default. The bucket owner
#'     can grant this permission to others. For more information about
#'     permissions, see [Permissions Related to Bucket
#'     Operations](https://docs.aws.amazon.com/AmazonS3/latest/userguide/security_iam_service-with-iam.html#security_iam_service-with-iam-id-based-policies-actions)
#'     and [Managing Access Permissions to Your Amazon S3
#'     Resources](https://docs.aws.amazon.com/AmazonS3/latest/userguide/security-iam.html).
#' 
#' -   **Directory bucket permissions** - To grant access to this API
#'     operation, you must have the `s3express:GetEncryptionConfiguration`
#'     permission in an IAM identity-based policy instead of a bucket
#'     policy. Cross-account access to this API operation isn't supported.
#'     This operation can only be performed by the Amazon Web Services
#'     account that owns the resource. For more information about directory
#'     bucket policies and permissions, see [Amazon Web Services Identity
#'     and Access Management (IAM) for S3 Express One
#'     Zone](https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-express-security-iam.html)
#'     in the *Amazon S3 User Guide*.
#' 
#' ### HTTP Host header syntax
#' 
#' **Directory buckets** - The HTTP Host header syntax is
#' `s3express-control.region-code.amazonaws.com`.
#' 
#' The following operations are related to
#' [`get_bucket_encryption`][s3_get_bucket_encryption]:
#' 
#' -   [`put_bucket_encryption`][s3_put_bucket_encryption]
#' 
#' -   [`delete_bucket_encryption`][s3_delete_bucket_encryption]
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#'
#' @usage
#' s3_get_bucket_encryption(Bucket, ExpectedBucketOwner)
#'
#' @param Bucket &#91;required&#93; The name of the bucket from which the server-side encryption
#' configuration is retrieved.
#' 
#' **Directory buckets** - When you use this operation with a directory
#' bucket, you must use path-style requests in the format
#' `https://s3express-control.region-code.amazonaws.com/bucket-name `.
#' Virtual-hosted-style requests aren't supported. Directory bucket names
#' must be unique in the chosen Zone (Availability Zone or Local Zone).
#' Bucket names must also follow the format
#' ` bucket-base-name--zone-id--x-s3` (for example,
#' ` DOC-EXAMPLE-BUCKET--usw2-az1--x-s3`). For information about bucket
#' naming restrictions, see [Directory bucket naming
#' rules](https://docs.aws.amazon.com/AmazonS3/latest/userguide/directory-bucket-naming-rules.html)
#' in the *Amazon S3 User Guide*
#' @param ExpectedBucketOwner The account ID of the expected bucket owner. If the account ID that you
#' provide does not match the actual owner of the bucket, the request fails
#' with the HTTP status code `403 Forbidden` (access denied).
#' 
#' For directory buckets, this header is not supported in this API
#' operation. If you specify this header, the request fails with the HTTP
#' status code `501 Not Implemented`.
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   ServerSideEncryptionConfiguration = list(
#'     Rules = list(
#'       list(
#'         ApplyServerSideEncryptionByDefault = list(
#'           SSEAlgorithm = "AES256"|"aws:fsx"|"aws:kms"|"aws:kms:dsse",
#'           KMSMasterKeyID = "string"
#'         ),
#'         BucketKeyEnabled = TRUE|FALSE,
#'         BlockedEncryptionTypes = list(
#'           EncryptionType = list(
#'             "NONE"|"SSE-C"
#'           )
#'         )
#'       )
#'     )
#'   )
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$get_bucket_encryption(
#'   Bucket = "string",
#'   ExpectedBucketOwner = "string"
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname s3_get_bucket_encryption
#'
#' @aliases s3_get_bucket_encryption
s3_get_bucket_encryption <- function(Bucket, ExpectedBucketOwner = NULL) {
  op <- new_operation(
    name = "GetBucketEncryption",
    http_method = "GET",
    http_path = "/{Bucket}?encryption",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .s3$get_bucket_encryption_input(Bucket = Bucket, ExpectedBucketOwner = ExpectedBucketOwner)
  output <- .s3$get_bucket_encryption_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$get_bucket_encryption <- s3_get_bucket_encryption

#' This operation is not supported for directory buckets
#'
#' @description
#' This operation is not supported for directory buckets.
#' 
#' Gets the S3 Intelligent-Tiering configuration from the specified bucket.
#' 
#' The S3 Intelligent-Tiering storage class is designed to optimize storage
#' costs by automatically moving data to the most cost-effective storage
#' access tier, without performance impact or operational overhead. S3
#' Intelligent-Tiering delivers automatic cost savings in three low latency
#' and high throughput access tiers. To get the lowest storage cost on data
#' that can be accessed in minutes to hours, you can choose to activate
#' additional archiving capabilities.
#' 
#' The S3 Intelligent-Tiering storage class is the ideal storage class for
#' data with unknown, changing, or unpredictable access patterns,
#' independent of object size or retention period. If the size of an object
#' is less than 128 KB, it is not monitored and not eligible for
#' auto-tiering. Smaller objects can be stored, but they are always charged
#' at the Frequent Access tier rates in the S3 Intelligent-Tiering storage
#' class.
#' 
#' For more information, see [Storage class for automatically optimizing
#' frequently and infrequently accessed
#' objects](https://docs.aws.amazon.com/AmazonS3/latest/userguide/storage-class-intro.html#sc-dynamic-data-access).
#' 
#' Operations related to
#' [`get_bucket_intelligent_tiering_configuration`][s3_get_bucket_intelligent_tiering_configuration]
#' include:
#' 
#' -   [`delete_bucket_intelligent_tiering_configuration`][s3_delete_bucket_intelligent_tiering_configuration]
#' 
#' -   [`put_bucket_intelligent_tiering_configuration`][s3_put_bucket_intelligent_tiering_configuration]
#' 
#' -   [`list_bucket_intelligent_tiering_configurations`][s3_list_bucket_intelligent_tiering_configurations]
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#'
#' @usage
#' s3_get_bucket_intelligent_tiering_configuration(Bucket, Id,
#'   ExpectedBucketOwner)
#'
#' @param Bucket &#91;required&#93; The name of the Amazon S3 bucket whose configuration you want to modify
#' or retrieve.
#' @param Id &#91;required&#93; The ID used to identify the S3 Intelligent-Tiering configuration.
#' @param ExpectedBucketOwner The account ID of the expected bucket owner. If the account ID that you
#' provide does not match the actual owner of the bucket, the request fails
#' with the HTTP status code `403 Forbidden` (access denied).
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   IntelligentTieringConfiguration = list(
#'     Id = "string",
#'     Filter = list(
#'       Prefix = "string",
#'       Tag = list(
#'         Key = "string",
#'         Value = "string"
#'       ),
#'       And = list(
#'         Prefix = "string",
#'         Tags = list(
#'           list(
#'             Key = "string",
#'             Value = "string"
#'           )
#'         )
#'       )
#'     ),
#'     Status = "Enabled"|"Disabled",
#'     Tierings = list(
#'       list(
#'         Days = 123,
#'         AccessTier = "ARCHIVE_ACCESS"|"DEEP_ARCHIVE_ACCESS"
#'       )
#'     )
#'   )
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$get_bucket_intelligent_tiering_configuration(
#'   Bucket = "string",
#'   Id = "string",
#'   ExpectedBucketOwner = "string"
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname s3_get_bucket_intelligent_tiering_configuration
#'
#' @aliases s3_get_bucket_intelligent_tiering_configuration
s3_get_bucket_intelligent_tiering_configuration <- function(Bucket, Id, ExpectedBucketOwner = NULL) {
  op <- new_operation(
    name = "GetBucketIntelligentTieringConfiguration",
    http_method = "GET",
    http_path = "/{Bucket}?intelligent-tiering",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .s3$get_bucket_intelligent_tiering_configuration_input(Bucket = Bucket, Id = Id, ExpectedBucketOwner = ExpectedBucketOwner)
  output <- .s3$get_bucket_intelligent_tiering_configuration_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$get_bucket_intelligent_tiering_configuration <- s3_get_bucket_intelligent_tiering_configuration

#' This operation is not supported for directory buckets
#'
#' @description
#' This operation is not supported for directory buckets.
#' 
#' Returns an S3 Inventory configuration (identified by the inventory
#' configuration ID) from the bucket.
#' 
#' To use this operation, you must have permissions to perform the
#' `s3:GetInventoryConfiguration` action. The bucket owner has this
#' permission by default and can grant this permission to others. For more
#' information about permissions, see [Permissions Related to Bucket
#' Subresource
#' Operations](https://docs.aws.amazon.com/AmazonS3/latest/userguide/security_iam_service-with-iam.html#security_iam_service-with-iam-id-based-policies-actions)
#' and [Managing Access Permissions to Your Amazon S3
#' Resources](https://docs.aws.amazon.com/AmazonS3/latest/userguide/security-iam.html).
#' 
#' For information about the Amazon S3 inventory feature, see [Amazon S3
#' Inventory](https://docs.aws.amazon.com/AmazonS3/latest/userguide/storage-inventory.html).
#' 
#' The following operations are related to
#' [`get_bucket_inventory_configuration`][s3_get_bucket_inventory_configuration]:
#' 
#' -   [`delete_bucket_inventory_configuration`][s3_delete_bucket_inventory_configuration]
#' 
#' -   [`list_bucket_inventory_configurations`][s3_list_bucket_inventory_configurations]
#' 
#' -   [`put_bucket_inventory_configuration`][s3_put_bucket_inventory_configuration]
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#'
#' @usage
#' s3_get_bucket_inventory_configuration(Bucket, Id, ExpectedBucketOwner)
#'
#' @param Bucket &#91;required&#93; The name of the bucket containing the inventory configuration to
#' retrieve.
#' @param Id &#91;required&#93; The ID used to identify the inventory configuration.
#' @param ExpectedBucketOwner The account ID of the expected bucket owner. If the account ID that you
#' provide does not match the actual owner of the bucket, the request fails
#' with the HTTP status code `403 Forbidden` (access denied).
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   InventoryConfiguration = list(
#'     Destination = list(
#'       S3BucketDestination = list(
#'         AccountId = "string",
#'         Bucket = "string",
#'         Format = "CSV"|"ORC"|"Parquet",
#'         Prefix = "string",
#'         Encryption = list(
#'           SSES3 = list(),
#'           SSEKMS = list(
#'             KeyId = "string"
#'           )
#'         )
#'       )
#'     ),
#'     IsEnabled = TRUE|FALSE,
#'     Filter = list(
#'       Prefix = "string"
#'     ),
#'     Id = "string",
#'     IncludedObjectVersions = "All"|"Current",
#'     OptionalFields = list(
#'       "Size"|"LastModifiedDate"|"StorageClass"|"ETag"|"IsMultipartUploaded"|"ReplicationStatus"|"EncryptionStatus"|"ObjectLockRetainUntilDate"|"ObjectLockMode"|"ObjectLockLegalHoldStatus"|"IntelligentTieringAccessTier"|"BucketKeyStatus"|"ChecksumAlgorithm"|"ObjectAccessControlList"|"ObjectOwner"|"LifecycleExpirationDate"
#'     ),
#'     Schedule = list(
#'       Frequency = "Daily"|"Weekly"
#'     )
#'   )
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$get_bucket_inventory_configuration(
#'   Bucket = "string",
#'   Id = "string",
#'   ExpectedBucketOwner = "string"
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname s3_get_bucket_inventory_configuration
#'
#' @aliases s3_get_bucket_inventory_configuration
s3_get_bucket_inventory_configuration <- function(Bucket, Id, ExpectedBucketOwner = NULL) {
  op <- new_operation(
    name = "GetBucketInventoryConfiguration",
    http_method = "GET",
    http_path = "/{Bucket}?inventory",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .s3$get_bucket_inventory_configuration_input(Bucket = Bucket, Id = Id, ExpectedBucketOwner = ExpectedBucketOwner)
  output <- .s3$get_bucket_inventory_configuration_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$get_bucket_inventory_configuration <- s3_get_bucket_inventory_configuration

#' For an updated version of this API, see GetBucketLifecycleConfiguration
#'
#' @description
#' For an updated version of this API, see
#' [`get_bucket_lifecycle_configuration`][s3_get_bucket_lifecycle_configuration].
#' If you configured a bucket lifecycle using the `filter` element, you
#' should see the updated version of this topic. This topic is provided for
#' backward compatibility.
#' 
#' This operation is not supported for directory buckets.
#' 
#' Returns the lifecycle configuration information set on the bucket. For
#' information about lifecycle configuration, see [Object Lifecycle
#' Management](https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lifecycle-mgmt.html).
#' 
#' To use this operation, you must have permission to perform the
#' `s3:GetLifecycleConfiguration` action. The bucket owner has this
#' permission by default. The bucket owner can grant this permission to
#' others. For more information about permissions, see [Permissions Related
#' to Bucket Subresource
#' Operations](https://docs.aws.amazon.com/AmazonS3/latest/userguide/security_iam_service-with-iam.html#security_iam_service-with-iam-id-based-policies-actions)
#' and [Managing Access Permissions to Your Amazon S3
#' Resources](https://docs.aws.amazon.com/AmazonS3/latest/userguide/security-iam.html).
#' 
#' [`get_bucket_lifecycle`][s3_get_bucket_lifecycle] has the following
#' special error:
#' 
#' -   Error code: `NoSuchLifecycleConfiguration`
#' 
#'     -   Description: The lifecycle configuration does not exist.
#' 
#'     -   HTTP Status Code: 404 Not Found
#' 
#'     -   SOAP Fault Code Prefix: Client
#' 
#' The following operations are related to
#' [`get_bucket_lifecycle`][s3_get_bucket_lifecycle]:
#' 
#' -   [`get_bucket_lifecycle_configuration`][s3_get_bucket_lifecycle_configuration]
#' 
#' -   [`put_bucket_lifecycle`][s3_put_bucket_lifecycle]
#' 
#' -   [`delete_bucket_lifecycle`][s3_delete_bucket_lifecycle]
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#'
#' @usage
#' s3_get_bucket_lifecycle(Bucket, ExpectedBucketOwner)
#'
#' @param Bucket &#91;required&#93; The name of the bucket for which to get the lifecycle information.
#' @param ExpectedBucketOwner The account ID of the expected bucket owner. If the account ID that you
#' provide does not match the actual owner of the bucket, the request fails
#' with the HTTP status code `403 Forbidden` (access denied).
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   Rules = list(
#'     list(
#'       Expiration = list(
#'         Date = as.POSIXct(
#'           "2015-01-01"
#'         ),
#'         Days = 123,
#'         ExpiredObjectDeleteMarker = TRUE|FALSE
#'       ),
#'       ID = "string",
#'       Prefix = "string",
#'       Status = "Enabled"|"Disabled",
#'       Transition = list(
#'         Date = as.POSIXct(
#'           "2015-01-01"
#'         ),
#'         Days = 123,
#'         StorageClass = "GLACIER"|"STANDARD_IA"|"ONEZONE_IA"|"INTELLIGENT_TIERING"|"DEEP_ARCHIVE"|"GLACIER_IR"
#'       ),
#'       NoncurrentVersionTransition = list(
#'         NoncurrentDays = 123,
#'         StorageClass = "GLACIER"|"STANDARD_IA"|"ONEZONE_IA"|"INTELLIGENT_TIERING"|"DEEP_ARCHIVE"|"GLACIER_IR",
#'         NewerNoncurrentVersions = 123
#'       ),
#'       NoncurrentVersionExpiration = list(
#'         NoncurrentDays = 123,
#'         NewerNoncurrentVersions = 123
#'       ),
#'       AbortIncompleteMultipartUpload = list(
#'         DaysAfterInitiation = 123
#'       )
#'     )
#'   )
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$get_bucket_lifecycle(
#'   Bucket = "string",
#'   ExpectedBucketOwner = "string"
#' )
#' ```
#'
#' @examples
#' \dontrun{
#' # The following example gets ACL on the specified bucket.
#' svc$get_bucket_lifecycle(
#'   Bucket = "acl1"
#' )
#' }
#'
#' @keywords internal
#'
#' @rdname s3_get_bucket_lifecycle
#'
#' @aliases s3_get_bucket_lifecycle
s3_get_bucket_lifecycle <- function(Bucket, ExpectedBucketOwner = NULL) {
  op <- new_operation(
    name = "GetBucketLifecycle",
    http_method = "GET",
    http_path = "/{Bucket}?lifecycle",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .s3$get_bucket_lifecycle_input(Bucket = Bucket, ExpectedBucketOwner = ExpectedBucketOwner)
  output <- .s3$get_bucket_lifecycle_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$get_bucket_lifecycle <- s3_get_bucket_lifecycle

#' Returns the lifecycle configuration information set on the bucket
#'
#' @description
#' Returns the lifecycle configuration information set on the bucket. For
#' information about lifecycle configuration, see [Object Lifecycle
#' Management](https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lifecycle-mgmt.html).
#' 
#' Bucket lifecycle configuration now supports specifying a lifecycle rule
#' using an object key name prefix, one or more object tags, object size,
#' or any combination of these. Accordingly, this section describes the
#' latest API, which is compatible with the new functionality. The previous
#' version of the API supported filtering based only on an object key name
#' prefix, which is supported for general purpose buckets for backward
#' compatibility. For the related API description, see
#' [`get_bucket_lifecycle`][s3_get_bucket_lifecycle].
#' 
#' Lifecyle configurations for directory buckets only support expiring
#' objects and cancelling multipart uploads. Expiring of versioned objects,
#' transitions and tag filters are not supported.
#' 
#' ### Permissions
#' 
#' -   **General purpose bucket permissions** - By default, all Amazon S3
#'     resources are private, including buckets, objects, and related
#'     subresources (for example, lifecycle configuration and website
#'     configuration). Only the resource owner (that is, the Amazon Web
#'     Services account that created it) can access the resource. The
#'     resource owner can optionally grant access permissions to others by
#'     writing an access policy. For this operation, a user must have the
#'     `s3:GetLifecycleConfiguration` permission.
#' 
#'     For more information about permissions, see [Managing Access
#'     Permissions to Your Amazon S3
#'     Resources](https://docs.aws.amazon.com/AmazonS3/latest/userguide/security-iam.html).
#' 
#' 
#' -   **Directory bucket permissions** - You must have the
#'     `s3express:GetLifecycleConfiguration` permission in an IAM
#'     identity-based policy to use this operation. Cross-account access to
#'     this API operation isn't supported. The resource owner can
#'     optionally grant access permissions to others by creating a role or
#'     user for them as long as they are within the same account as the
#'     owner and resource.
#' 
#'     For more information about directory bucket policies and
#'     permissions, see [Authorizing Regional endpoint APIs with
#'     IAM](https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-express-security-iam.html)
#'     in the *Amazon S3 User Guide*.
#' 
#'     **Directory buckets** - For directory buckets, you must make
#'     requests for this API operation to the Regional endpoint. These
#'     endpoints support path-style requests in the format
#'     `https://s3express-control.region-code.amazonaws.com/bucket-name `.
#'     Virtual-hosted-style requests aren't supported. For more information
#'     about endpoints in Availability Zones, see [Regional and Zonal
#'     endpoints for directory buckets in Availability
#'     Zones](https://docs.aws.amazon.com/AmazonS3/latest/userguide/endpoint-directory-buckets-AZ.html)
#'     in the *Amazon S3 User Guide*. For more information about endpoints
#'     in Local Zones, see [Concepts for directory buckets in Local
#'     Zones](https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-lzs-for-directory-buckets.html)
#'     in the *Amazon S3 User Guide*.
#' 
#' ### HTTP Host header syntax
#' 
#' **Directory buckets** - The HTTP Host header syntax is
#' `s3express-control.region.amazonaws.com`.
#' 
#' [`get_bucket_lifecycle_configuration`][s3_get_bucket_lifecycle_configuration]
#' has the following special error:
#' 
#' -   Error code: `NoSuchLifecycleConfiguration`
#' 
#'     -   Description: The lifecycle configuration does not exist.
#' 
#'     -   HTTP Status Code: 404 Not Found
#' 
#'     -   SOAP Fault Code Prefix: Client
#' 
#' The following operations are related to
#' [`get_bucket_lifecycle_configuration`][s3_get_bucket_lifecycle_configuration]:
#' 
#' -   [`get_bucket_lifecycle`][s3_get_bucket_lifecycle]
#' 
#' -   [`put_bucket_lifecycle`][s3_put_bucket_lifecycle]
#' 
#' -   [`delete_bucket_lifecycle`][s3_delete_bucket_lifecycle]
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#'
#' @usage
#' s3_get_bucket_lifecycle_configuration(Bucket, ExpectedBucketOwner)
#'
#' @param Bucket &#91;required&#93; The name of the bucket for which to get the lifecycle information.
#' @param ExpectedBucketOwner The account ID of the expected bucket owner. If the account ID that you
#' provide does not match the actual owner of the bucket, the request fails
#' with the HTTP status code `403 Forbidden` (access denied).
#' 
#' This parameter applies to general purpose buckets only. It is not
#' supported for directory bucket lifecycle configurations.
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   Rules = list(
#'     list(
#'       Expiration = list(
#'         Date = as.POSIXct(
#'           "2015-01-01"
#'         ),
#'         Days = 123,
#'         ExpiredObjectDeleteMarker = TRUE|FALSE
#'       ),
#'       ID = "string",
#'       Prefix = "string",
#'       Filter = list(
#'         Prefix = "string",
#'         Tag = list(
#'           Key = "string",
#'           Value = "string"
#'         ),
#'         ObjectSizeGreaterThan = 123,
#'         ObjectSizeLessThan = 123,
#'         And = list(
#'           Prefix = "string",
#'           Tags = list(
#'             list(
#'               Key = "string",
#'               Value = "string"
#'             )
#'           ),
#'           ObjectSizeGreaterThan = 123,
#'           ObjectSizeLessThan = 123
#'         )
#'       ),
#'       Status = "Enabled"|"Disabled",
#'       Transitions = list(
#'         list(
#'           Date = as.POSIXct(
#'             "2015-01-01"
#'           ),
#'           Days = 123,
#'           StorageClass = "GLACIER"|"STANDARD_IA"|"ONEZONE_IA"|"INTELLIGENT_TIERING"|"DEEP_ARCHIVE"|"GLACIER_IR"
#'         )
#'       ),
#'       NoncurrentVersionTransitions = list(
#'         list(
#'           NoncurrentDays = 123,
#'           StorageClass = "GLACIER"|"STANDARD_IA"|"ONEZONE_IA"|"INTELLIGENT_TIERING"|"DEEP_ARCHIVE"|"GLACIER_IR",
#'           NewerNoncurrentVersions = 123
#'         )
#'       ),
#'       NoncurrentVersionExpiration = list(
#'         NoncurrentDays = 123,
#'         NewerNoncurrentVersions = 123
#'       ),
#'       AbortIncompleteMultipartUpload = list(
#'         DaysAfterInitiation = 123
#'       )
#'     )
#'   ),
#'   TransitionDefaultMinimumObjectSize = "varies_by_storage_class"|"all_storage_classes_128K"
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$get_bucket_lifecycle_configuration(
#'   Bucket = "string",
#'   ExpectedBucketOwner = "string"
#' )
#' ```
#'
#' @examples
#' \dontrun{
#' # The following example retrieves lifecycle configuration on set on a
#' # bucket.
#' svc$get_bucket_lifecycle_configuration(
#'   Bucket = "examplebucket"
#' )
#' }
#'
#' @keywords internal
#'
#' @rdname s3_get_bucket_lifecycle_configuration
#'
#' @aliases s3_get_bucket_lifecycle_configuration
s3_get_bucket_lifecycle_configuration <- function(Bucket, ExpectedBucketOwner = NULL) {
  op <- new_operation(
    name = "GetBucketLifecycleConfiguration",
    http_method = "GET",
    http_path = "/{Bucket}?lifecycle",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .s3$get_bucket_lifecycle_configuration_input(Bucket = Bucket, ExpectedBucketOwner = ExpectedBucketOwner)
  output <- .s3$get_bucket_lifecycle_configuration_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$get_bucket_lifecycle_configuration <- s3_get_bucket_lifecycle_configuration

#' Using the GetBucketLocation operation is no longer a best practice
#'
#' @description
#' Using the [`get_bucket_location`][s3_get_bucket_location] operation is
#' no longer a best practice. To return the Region that a bucket resides
#' in, we recommend that you use the [`head_bucket`][s3_head_bucket]
#' operation instead. For backward compatibility, Amazon S3 continues to
#' support the [`get_bucket_location`][s3_get_bucket_location] operation.
#' 
#' Returns the Region the bucket resides in. You set the bucket's Region
#' using the `LocationConstraint` request parameter in a
#' [`create_bucket`][s3_create_bucket] request. For more information, see
#' [`create_bucket`][s3_create_bucket].
#' 
#' In a bucket's home Region, calls to the
#' [`get_bucket_location`][s3_get_bucket_location] operation are governed
#' by the bucket's policy. In other Regions, the bucket policy doesn't
#' apply, which means that cross-account access won't be authorized.
#' However, calls to the [`head_bucket`][s3_head_bucket] operation always
#' return the bucket’s location through an HTTP response header, whether
#' access to the bucket is authorized or not. Therefore, we recommend using
#' the [`head_bucket`][s3_head_bucket] operation for bucket Region
#' discovery and to avoid using the
#' [`get_bucket_location`][s3_get_bucket_location] operation.
#' 
#' When you use this API operation with an access point, provide the alias
#' of the access point in place of the bucket name.
#' 
#' When you use this API operation with an Object Lambda access point,
#' provide the alias of the Object Lambda access point in place of the
#' bucket name. If the Object Lambda access point alias in a request is not
#' valid, the error code `InvalidAccessPointAliasError` is returned. For
#' more information about `InvalidAccessPointAliasError`, see [List of
#' Error
#' Codes](https://docs.aws.amazon.com/AmazonS3/latest/API/ErrorResponses.html#ErrorCodeList).
#' 
#' This operation is not supported for directory buckets.
#' 
#' The following operations are related to
#' [`get_bucket_location`][s3_get_bucket_location]:
#' 
#' -   [`get_object`][s3_get_object]
#' 
#' -   [`create_bucket`][s3_create_bucket]
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#'
#' @usage
#' s3_get_bucket_location(Bucket, ExpectedBucketOwner)
#'
#' @param Bucket &#91;required&#93; The name of the bucket for which to get the location.
#' 
#' When you use this API operation with an access point, provide the alias
#' of the access point in place of the bucket name.
#' 
#' When you use this API operation with an Object Lambda access point,
#' provide the alias of the Object Lambda access point in place of the
#' bucket name. If the Object Lambda access point alias in a request is not
#' valid, the error code `InvalidAccessPointAliasError` is returned. For
#' more information about `InvalidAccessPointAliasError`, see [List of
#' Error
#' Codes](https://docs.aws.amazon.com/AmazonS3/latest/API/ErrorResponses.html#ErrorCodeList).
#' @param ExpectedBucketOwner The account ID of the expected bucket owner. If the account ID that you
#' provide does not match the actual owner of the bucket, the request fails
#' with the HTTP status code `403 Forbidden` (access denied).
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   LocationConstraint = "af-south-1"|"ap-east-1"|"ap-northeast-1"|"ap-northeast-2"|"ap-northeast-3"|"ap-south-1"|"ap-south-2"|"ap-southeast-1"|"ap-southeast-2"|"ap-southeast-3"|"ap-southeast-4"|"ap-southeast-5"|"ca-central-1"|"cn-north-1"|"cn-northwest-1"|"EU"|"eu-central-1"|"eu-central-2"|"eu-north-1"|"eu-south-1"|"eu-south-2"|"eu-west-1"|"eu-west-2"|"eu-west-3"|"il-central-1"|"me-central-1"|"me-south-1"|"sa-east-1"|"us-east-2"|"us-gov-east-1"|"us-gov-west-1"|"us-west-1"|"us-west-2"
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$get_bucket_location(
#'   Bucket = "string",
#'   ExpectedBucketOwner = "string"
#' )
#' ```
#'
#' @examples
#' \dontrun{
#' # The following example returns bucket location.
#' svc$get_bucket_location(
#'   Bucket = "examplebucket"
#' )
#' }
#'
#' @keywords internal
#'
#' @rdname s3_get_bucket_location
#'
#' @aliases s3_get_bucket_location
s3_get_bucket_location <- function(Bucket, ExpectedBucketOwner = NULL) {
  op <- new_operation(
    name = "GetBucketLocation",
    http_method = "GET",
    http_path = "/{Bucket}?location",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .s3$get_bucket_location_input(Bucket = Bucket, ExpectedBucketOwner = ExpectedBucketOwner)
  output <- .s3$get_bucket_location_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$get_bucket_location <- s3_get_bucket_location

#' This operation is not supported for directory buckets
#'
#' @description
#' This operation is not supported for directory buckets.
#' 
#' Returns the logging status of a bucket and the permissions users have to
#' view and modify that status.
#' 
#' The following operations are related to
#' [`get_bucket_logging`][s3_get_bucket_logging]:
#' 
#' -   [`create_bucket`][s3_create_bucket]
#' 
#' -   [`put_bucket_logging`][s3_put_bucket_logging]
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#'
#' @usage
#' s3_get_bucket_logging(Bucket, ExpectedBucketOwner)
#'
#' @param Bucket &#91;required&#93; The bucket name for which to get the logging information.
#' @param ExpectedBucketOwner The account ID of the expected bucket owner. If the account ID that you
#' provide does not match the actual owner of the bucket, the request fails
#' with the HTTP status code `403 Forbidden` (access denied).
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   LoggingEnabled = list(
#'     TargetBucket = "string",
#'     TargetGrants = list(
#'       list(
#'         Grantee = list(
#'           DisplayName = "string",
#'           EmailAddress = "string",
#'           ID = "string",
#'           Type = "CanonicalUser"|"AmazonCustomerByEmail"|"Group",
#'           URI = "string"
#'         ),
#'         Permission = "FULL_CONTROL"|"READ"|"WRITE"
#'       )
#'     ),
#'     TargetPrefix = "string",
#'     TargetObjectKeyFormat = list(
#'       SimplePrefix = list(),
#'       PartitionedPrefix = list(
#'         PartitionDateSource = "EventTime"|"DeliveryTime"
#'       )
#'     )
#'   )
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$get_bucket_logging(
#'   Bucket = "string",
#'   ExpectedBucketOwner = "string"
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname s3_get_bucket_logging
#'
#' @aliases s3_get_bucket_logging
s3_get_bucket_logging <- function(Bucket, ExpectedBucketOwner = NULL) {
  op <- new_operation(
    name = "GetBucketLogging",
    http_method = "GET",
    http_path = "/{Bucket}?logging",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .s3$get_bucket_logging_input(Bucket = Bucket, ExpectedBucketOwner = ExpectedBucketOwner)
  output <- .s3$get_bucket_logging_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$get_bucket_logging <- s3_get_bucket_logging

#' Retrieves the S3 Metadata configuration for a general purpose bucket
#'
#' @description
#' Retrieves the S3 Metadata configuration for a general purpose bucket.
#' For more information, see [Accelerating data discovery with S3
#' Metadata](https://docs.aws.amazon.com/AmazonS3/latest/userguide/metadata-tables-overview.html)
#' in the *Amazon S3 User Guide*.
#' 
#' You can use the V2
#' [`get_bucket_metadata_configuration`][s3_get_bucket_metadata_configuration]
#' API operation with V1 or V2 metadata configurations. However, if you try
#' to use the V1
#' [`get_bucket_metadata_table_configuration`][s3_get_bucket_metadata_table_configuration]
#' API operation with V2 configurations, you will receive an HTTP
#' `405 Method Not Allowed` error.
#' 
#' ### Permissions
#' 
#' To use this operation, you must have the
#' `s3:GetBucketMetadataTableConfiguration` permission. For more
#' information, see [Setting up permissions for configuring metadata
#' tables](https://docs.aws.amazon.com/AmazonS3/latest/userguide/metadata-tables-permissions.html)
#' in the *Amazon S3 User Guide*.
#' 
#' The IAM policy action name is the same for the V1 and V2 API operations.
#' 
#' The following operations are related to
#' [`get_bucket_metadata_configuration`][s3_get_bucket_metadata_configuration]:
#' 
#' -   [`create_bucket_metadata_configuration`][s3_create_bucket_metadata_configuration]
#' 
#' -   [`delete_bucket_metadata_configuration`][s3_delete_bucket_metadata_configuration]
#' 
#' -   [`update_bucket_metadata_inventory_table_configuration`][s3_update_bucket_metadata_inventory_table_configuration]
#' 
#' -   [`update_bucket_metadata_journal_table_configuration`][s3_update_bucket_metadata_journal_table_configuration]
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#'
#' @usage
#' s3_get_bucket_metadata_configuration(Bucket, ExpectedBucketOwner)
#'
#' @param Bucket &#91;required&#93; The general purpose bucket that corresponds to the metadata
#' configuration that you want to retrieve.
#' @param ExpectedBucketOwner The expected owner of the general purpose bucket that you want to
#' retrieve the metadata table configuration for.
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   GetBucketMetadataConfigurationResult = list(
#'     MetadataConfigurationResult = list(
#'       DestinationResult = list(
#'         TableBucketType = "aws"|"customer",
#'         TableBucketArn = "string",
#'         TableNamespace = "string"
#'       ),
#'       JournalTableConfigurationResult = list(
#'         TableStatus = "string",
#'         Error = list(
#'           ErrorCode = "string",
#'           ErrorMessage = "string"
#'         ),
#'         TableName = "string",
#'         TableArn = "string",
#'         RecordExpiration = list(
#'           Expiration = "ENABLED"|"DISABLED",
#'           Days = 123
#'         )
#'       ),
#'       InventoryTableConfigurationResult = list(
#'         ConfigurationState = "ENABLED"|"DISABLED",
#'         TableStatus = "string",
#'         Error = list(
#'           ErrorCode = "string",
#'           ErrorMessage = "string"
#'         ),
#'         TableName = "string",
#'         TableArn = "string"
#'       )
#'     )
#'   )
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$get_bucket_metadata_configuration(
#'   Bucket = "string",
#'   ExpectedBucketOwner = "string"
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname s3_get_bucket_metadata_configuration
#'
#' @aliases s3_get_bucket_metadata_configuration
s3_get_bucket_metadata_configuration <- function(Bucket, ExpectedBucketOwner = NULL) {
  op <- new_operation(
    name = "GetBucketMetadataConfiguration",
    http_method = "GET",
    http_path = "/{Bucket}?metadataConfiguration",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .s3$get_bucket_metadata_configuration_input(Bucket = Bucket, ExpectedBucketOwner = ExpectedBucketOwner)
  output <- .s3$get_bucket_metadata_configuration_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$get_bucket_metadata_configuration <- s3_get_bucket_metadata_configuration

#' We recommend that you retrieve your S3 Metadata configurations by using
#' the V2 GetBucketMetadataTableConfiguration API operation
#'
#' @description
#' We recommend that you retrieve your S3 Metadata configurations by using
#' the V2
#' [`get_bucket_metadata_table_configuration`][s3_get_bucket_metadata_table_configuration]
#' API operation. We no longer recommend using the V1
#' [`get_bucket_metadata_table_configuration`][s3_get_bucket_metadata_table_configuration]
#' API operation.
#' 
#' If you created your S3 Metadata configuration before July 15, 2025, we
#' recommend that you delete and re-create your configuration by using
#' [`create_bucket_metadata_configuration`][s3_create_bucket_metadata_configuration]
#' so that you can expire journal table records and create a live inventory
#' table.
#' 
#' Retrieves the V1 S3 Metadata configuration for a general purpose bucket.
#' For more information, see [Accelerating data discovery with S3
#' Metadata](https://docs.aws.amazon.com/AmazonS3/latest/userguide/metadata-tables-overview.html)
#' in the *Amazon S3 User Guide*.
#' 
#' You can use the V2
#' [`get_bucket_metadata_configuration`][s3_get_bucket_metadata_configuration]
#' API operation with V1 or V2 metadata table configurations. However, if
#' you try to use the V1
#' [`get_bucket_metadata_table_configuration`][s3_get_bucket_metadata_table_configuration]
#' API operation with V2 configurations, you will receive an HTTP
#' `405 Method Not Allowed` error.
#' 
#' Make sure that you update your processes to use the new V2 API
#' operations
#' ([`create_bucket_metadata_configuration`][s3_create_bucket_metadata_configuration],
#' [`get_bucket_metadata_configuration`][s3_get_bucket_metadata_configuration],
#' and
#' [`delete_bucket_metadata_configuration`][s3_delete_bucket_metadata_configuration])
#' instead of the V1 API operations.
#' 
#' ### Permissions
#' 
#' To use this operation, you must have the
#' `s3:GetBucketMetadataTableConfiguration` permission. For more
#' information, see [Setting up permissions for configuring metadata
#' tables](https://docs.aws.amazon.com/AmazonS3/latest/userguide/metadata-tables-permissions.html)
#' in the *Amazon S3 User Guide*.
#' 
#' The following operations are related to
#' [`get_bucket_metadata_table_configuration`][s3_get_bucket_metadata_table_configuration]:
#' 
#' -   [`create_bucket_metadata_table_configuration`][s3_create_bucket_metadata_table_configuration]
#' 
#' -   [`delete_bucket_metadata_table_configuration`][s3_delete_bucket_metadata_table_configuration]
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#'
#' @usage
#' s3_get_bucket_metadata_table_configuration(Bucket, ExpectedBucketOwner)
#'
#' @param Bucket &#91;required&#93; The general purpose bucket that corresponds to the metadata table
#' configuration that you want to retrieve.
#' @param ExpectedBucketOwner The expected owner of the general purpose bucket that you want to
#' retrieve the metadata table configuration for.
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   GetBucketMetadataTableConfigurationResult = list(
#'     MetadataTableConfigurationResult = list(
#'       S3TablesDestinationResult = list(
#'         TableBucketArn = "string",
#'         TableName = "string",
#'         TableArn = "string",
#'         TableNamespace = "string"
#'       )
#'     ),
#'     Status = "string",
#'     Error = list(
#'       ErrorCode = "string",
#'       ErrorMessage = "string"
#'     )
#'   )
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$get_bucket_metadata_table_configuration(
#'   Bucket = "string",
#'   ExpectedBucketOwner = "string"
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname s3_get_bucket_metadata_table_configuration
#'
#' @aliases s3_get_bucket_metadata_table_configuration
s3_get_bucket_metadata_table_configuration <- function(Bucket, ExpectedBucketOwner = NULL) {
  op <- new_operation(
    name = "GetBucketMetadataTableConfiguration",
    http_method = "GET",
    http_path = "/{Bucket}?metadataTable",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .s3$get_bucket_metadata_table_configuration_input(Bucket = Bucket, ExpectedBucketOwner = ExpectedBucketOwner)
  output <- .s3$get_bucket_metadata_table_configuration_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$get_bucket_metadata_table_configuration <- s3_get_bucket_metadata_table_configuration

#' This operation is not supported for directory buckets
#'
#' @description
#' This operation is not supported for directory buckets.
#' 
#' Gets a metrics configuration (specified by the metrics configuration ID)
#' from the bucket. Note that this doesn't include the daily storage
#' metrics.
#' 
#' To use this operation, you must have permissions to perform the
#' `s3:GetMetricsConfiguration` action. The bucket owner has this
#' permission by default. The bucket owner can grant this permission to
#' others. For more information about permissions, see [Permissions Related
#' to Bucket Subresource
#' Operations](https://docs.aws.amazon.com/AmazonS3/latest/userguide/security_iam_service-with-iam.html#security_iam_service-with-iam-id-based-policies-actions)
#' and [Managing Access Permissions to Your Amazon S3
#' Resources](https://docs.aws.amazon.com/AmazonS3/latest/userguide/security-iam.html).
#' 
#' For information about CloudWatch request metrics for Amazon S3, see
#' [Monitoring Metrics with Amazon
#' CloudWatch](https://docs.aws.amazon.com/AmazonS3/latest/userguide/cloudwatch-monitoring.html).
#' 
#' The following operations are related to
#' [`get_bucket_metrics_configuration`][s3_get_bucket_metrics_configuration]:
#' 
#' -   [`put_bucket_metrics_configuration`][s3_put_bucket_metrics_configuration]
#' 
#' -   [`delete_bucket_metrics_configuration`][s3_delete_bucket_metrics_configuration]
#' 
#' -   [`list_bucket_metrics_configurations`][s3_list_bucket_metrics_configurations]
#' 
#' -   [Monitoring Metrics with Amazon
#'     CloudWatch](https://docs.aws.amazon.com/AmazonS3/latest/userguide/cloudwatch-monitoring.html)
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#'
#' @usage
#' s3_get_bucket_metrics_configuration(Bucket, Id, ExpectedBucketOwner)
#'
#' @param Bucket &#91;required&#93; The name of the bucket containing the metrics configuration to retrieve.
#' @param Id &#91;required&#93; The ID used to identify the metrics configuration. The ID has a 64
#' character limit and can only contain letters, numbers, periods, dashes,
#' and underscores.
#' @param ExpectedBucketOwner The account ID of the expected bucket owner. If the account ID that you
#' provide does not match the actual owner of the bucket, the request fails
#' with the HTTP status code `403 Forbidden` (access denied).
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   MetricsConfiguration = list(
#'     Id = "string",
#'     Filter = list(
#'       Prefix = "string",
#'       Tag = list(
#'         Key = "string",
#'         Value = "string"
#'       ),
#'       AccessPointArn = "string",
#'       And = list(
#'         Prefix = "string",
#'         Tags = list(
#'           list(
#'             Key = "string",
#'             Value = "string"
#'           )
#'         ),
#'         AccessPointArn = "string"
#'       )
#'     )
#'   )
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$get_bucket_metrics_configuration(
#'   Bucket = "string",
#'   Id = "string",
#'   ExpectedBucketOwner = "string"
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname s3_get_bucket_metrics_configuration
#'
#' @aliases s3_get_bucket_metrics_configuration
s3_get_bucket_metrics_configuration <- function(Bucket, Id, ExpectedBucketOwner = NULL) {
  op <- new_operation(
    name = "GetBucketMetricsConfiguration",
    http_method = "GET",
    http_path = "/{Bucket}?metrics",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .s3$get_bucket_metrics_configuration_input(Bucket = Bucket, Id = Id, ExpectedBucketOwner = ExpectedBucketOwner)
  output <- .s3$get_bucket_metrics_configuration_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$get_bucket_metrics_configuration <- s3_get_bucket_metrics_configuration

#' This operation is not supported for directory buckets
#'
#' @description
#' This operation is not supported for directory buckets.
#' 
#' No longer used, see
#' [`get_bucket_notification_configuration`][s3_get_bucket_notification_configuration].
#'
#' @usage
#' s3_get_bucket_notification(Bucket, ExpectedBucketOwner)
#'
#' @param Bucket &#91;required&#93; The name of the bucket for which to get the notification configuration.
#' 
#' When you use this API operation with an access point, provide the alias
#' of the access point in place of the bucket name.
#' 
#' When you use this API operation with an Object Lambda access point,
#' provide the alias of the Object Lambda access point in place of the
#' bucket name. If the Object Lambda access point alias in a request is not
#' valid, the error code `InvalidAccessPointAliasError` is returned. For
#' more information about `InvalidAccessPointAliasError`, see [List of
#' Error
#' Codes](https://docs.aws.amazon.com/AmazonS3/latest/API/ErrorResponses.html#ErrorCodeList).
#' @param ExpectedBucketOwner The account ID of the expected bucket owner. If the account ID that you
#' provide does not match the actual owner of the bucket, the request fails
#' with the HTTP status code `403 Forbidden` (access denied).
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   TopicConfiguration = list(
#'     Id = "string",
#'     Events = list(
#'       "s3:ReducedRedundancyLostObject"|"s3:ObjectCreated:*"|"s3:ObjectCreated:Put"|"s3:ObjectCreated:Post"|"s3:ObjectCreated:Copy"|"s3:ObjectCreated:CompleteMultipartUpload"|"s3:ObjectRemoved:*"|"s3:ObjectRemoved:Delete"|"s3:ObjectRemoved:DeleteMarkerCreated"|"s3:ObjectRestore:*"|"s3:ObjectRestore:Post"|"s3:ObjectRestore:Completed"|"s3:Replication:*"|"s3:Replication:OperationFailedReplication"|"s3:Replication:OperationNotTracked"|"s3:Replication:OperationMissedThreshold"|"s3:Replication:OperationReplicatedAfterThreshold"|"s3:ObjectRestore:Delete"|"s3:LifecycleTransition"|"s3:IntelligentTiering"|"s3:ObjectAcl:Put"|"s3:LifecycleExpiration:*"|"s3:LifecycleExpiration:Delete"|"s3:LifecycleExpiration:DeleteMarkerCreated"|"s3:ObjectTagging:*"|"s3:ObjectTagging:Put"|"s3:ObjectTagging:Delete"
#'     ),
#'     Event = "s3:ReducedRedundancyLostObject"|"s3:ObjectCreated:*"|"s3:ObjectCreated:Put"|"s3:ObjectCreated:Post"|"s3:ObjectCreated:Copy"|"s3:ObjectCreated:CompleteMultipartUpload"|"s3:ObjectRemoved:*"|"s3:ObjectRemoved:Delete"|"s3:ObjectRemoved:DeleteMarkerCreated"|"s3:ObjectRestore:*"|"s3:ObjectRestore:Post"|"s3:ObjectRestore:Completed"|"s3:Replication:*"|"s3:Replication:OperationFailedReplication"|"s3:Replication:OperationNotTracked"|"s3:Replication:OperationMissedThreshold"|"s3:Replication:OperationReplicatedAfterThreshold"|"s3:ObjectRestore:Delete"|"s3:LifecycleTransition"|"s3:IntelligentTiering"|"s3:ObjectAcl:Put"|"s3:LifecycleExpiration:*"|"s3:LifecycleExpiration:Delete"|"s3:LifecycleExpiration:DeleteMarkerCreated"|"s3:ObjectTagging:*"|"s3:ObjectTagging:Put"|"s3:ObjectTagging:Delete",
#'     Topic = "string"
#'   ),
#'   QueueConfiguration = list(
#'     Id = "string",
#'     Event = "s3:ReducedRedundancyLostObject"|"s3:ObjectCreated:*"|"s3:ObjectCreated:Put"|"s3:ObjectCreated:Post"|"s3:ObjectCreated:Copy"|"s3:ObjectCreated:CompleteMultipartUpload"|"s3:ObjectRemoved:*"|"s3:ObjectRemoved:Delete"|"s3:ObjectRemoved:DeleteMarkerCreated"|"s3:ObjectRestore:*"|"s3:ObjectRestore:Post"|"s3:ObjectRestore:Completed"|"s3:Replication:*"|"s3:Replication:OperationFailedReplication"|"s3:Replication:OperationNotTracked"|"s3:Replication:OperationMissedThreshold"|"s3:Replication:OperationReplicatedAfterThreshold"|"s3:ObjectRestore:Delete"|"s3:LifecycleTransition"|"s3:IntelligentTiering"|"s3:ObjectAcl:Put"|"s3:LifecycleExpiration:*"|"s3:LifecycleExpiration:Delete"|"s3:LifecycleExpiration:DeleteMarkerCreated"|"s3:ObjectTagging:*"|"s3:ObjectTagging:Put"|"s3:ObjectTagging:Delete",
#'     Events = list(
#'       "s3:ReducedRedundancyLostObject"|"s3:ObjectCreated:*"|"s3:ObjectCreated:Put"|"s3:ObjectCreated:Post"|"s3:ObjectCreated:Copy"|"s3:ObjectCreated:CompleteMultipartUpload"|"s3:ObjectRemoved:*"|"s3:ObjectRemoved:Delete"|"s3:ObjectRemoved:DeleteMarkerCreated"|"s3:ObjectRestore:*"|"s3:ObjectRestore:Post"|"s3:ObjectRestore:Completed"|"s3:Replication:*"|"s3:Replication:OperationFailedReplication"|"s3:Replication:OperationNotTracked"|"s3:Replication:OperationMissedThreshold"|"s3:Replication:OperationReplicatedAfterThreshold"|"s3:ObjectRestore:Delete"|"s3:LifecycleTransition"|"s3:IntelligentTiering"|"s3:ObjectAcl:Put"|"s3:LifecycleExpiration:*"|"s3:LifecycleExpiration:Delete"|"s3:LifecycleExpiration:DeleteMarkerCreated"|"s3:ObjectTagging:*"|"s3:ObjectTagging:Put"|"s3:ObjectTagging:Delete"
#'     ),
#'     Queue = "string"
#'   ),
#'   CloudFunctionConfiguration = list(
#'     Id = "string",
#'     Event = "s3:ReducedRedundancyLostObject"|"s3:ObjectCreated:*"|"s3:ObjectCreated:Put"|"s3:ObjectCreated:Post"|"s3:ObjectCreated:Copy"|"s3:ObjectCreated:CompleteMultipartUpload"|"s3:ObjectRemoved:*"|"s3:ObjectRemoved:Delete"|"s3:ObjectRemoved:DeleteMarkerCreated"|"s3:ObjectRestore:*"|"s3:ObjectRestore:Post"|"s3:ObjectRestore:Completed"|"s3:Replication:*"|"s3:Replication:OperationFailedReplication"|"s3:Replication:OperationNotTracked"|"s3:Replication:OperationMissedThreshold"|"s3:Replication:OperationReplicatedAfterThreshold"|"s3:ObjectRestore:Delete"|"s3:LifecycleTransition"|"s3:IntelligentTiering"|"s3:ObjectAcl:Put"|"s3:LifecycleExpiration:*"|"s3:LifecycleExpiration:Delete"|"s3:LifecycleExpiration:DeleteMarkerCreated"|"s3:ObjectTagging:*"|"s3:ObjectTagging:Put"|"s3:ObjectTagging:Delete",
#'     Events = list(
#'       "s3:ReducedRedundancyLostObject"|"s3:ObjectCreated:*"|"s3:ObjectCreated:Put"|"s3:ObjectCreated:Post"|"s3:ObjectCreated:Copy"|"s3:ObjectCreated:CompleteMultipartUpload"|"s3:ObjectRemoved:*"|"s3:ObjectRemoved:Delete"|"s3:ObjectRemoved:DeleteMarkerCreated"|"s3:ObjectRestore:*"|"s3:ObjectRestore:Post"|"s3:ObjectRestore:Completed"|"s3:Replication:*"|"s3:Replication:OperationFailedReplication"|"s3:Replication:OperationNotTracked"|"s3:Replication:OperationMissedThreshold"|"s3:Replication:OperationReplicatedAfterThreshold"|"s3:ObjectRestore:Delete"|"s3:LifecycleTransition"|"s3:IntelligentTiering"|"s3:ObjectAcl:Put"|"s3:LifecycleExpiration:*"|"s3:LifecycleExpiration:Delete"|"s3:LifecycleExpiration:DeleteMarkerCreated"|"s3:ObjectTagging:*"|"s3:ObjectTagging:Put"|"s3:ObjectTagging:Delete"
#'     ),
#'     CloudFunction = "string",
#'     InvocationRole = "string"
#'   )
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$get_bucket_notification(
#'   Bucket = "string",
#'   ExpectedBucketOwner = "string"
#' )
#' ```
#'
#' @examples
#' \dontrun{
#' # The following example returns notification configuration set on a
#' # bucket.
#' svc$get_bucket_notification(
#'   Bucket = "examplebucket"
#' )
#' 
#' # The following example returns notification configuration set on a
#' # bucket.
#' svc$get_bucket_notification(
#'   Bucket = "examplebucket"
#' )
#' }
#'
#' @keywords internal
#'
#' @rdname s3_get_bucket_notification
#'
#' @aliases s3_get_bucket_notification
s3_get_bucket_notification <- function(Bucket, ExpectedBucketOwner = NULL) {
  op <- new_operation(
    name = "GetBucketNotification",
    http_method = "GET",
    http_path = "/{Bucket}?notification",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .s3$get_bucket_notification_input(Bucket = Bucket, ExpectedBucketOwner = ExpectedBucketOwner)
  output <- .s3$get_bucket_notification_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$get_bucket_notification <- s3_get_bucket_notification

#' This operation is not supported for directory buckets
#'
#' @description
#' This operation is not supported for directory buckets.
#' 
#' Returns the notification configuration of a bucket.
#' 
#' If notifications are not enabled on the bucket, the action returns an
#' empty `NotificationConfiguration` element.
#' 
#' By default, you must be the bucket owner to read the notification
#' configuration of a bucket. However, the bucket owner can use a bucket
#' policy to grant permission to other users to read this configuration
#' with the `s3:GetBucketNotification` permission.
#' 
#' When you use this API operation with an access point, provide the alias
#' of the access point in place of the bucket name.
#' 
#' When you use this API operation with an Object Lambda access point,
#' provide the alias of the Object Lambda access point in place of the
#' bucket name. If the Object Lambda access point alias in a request is not
#' valid, the error code `InvalidAccessPointAliasError` is returned. For
#' more information about `InvalidAccessPointAliasError`, see [List of
#' Error
#' Codes](https://docs.aws.amazon.com/AmazonS3/latest/API/ErrorResponses.html#ErrorCodeList).
#' 
#' For more information about setting and reading the notification
#' configuration on a bucket, see [Setting Up Notification of Bucket
#' Events](https://docs.aws.amazon.com/AmazonS3/latest/userguide/EventNotifications.html).
#' For more information about bucket policies, see [Using Bucket
#' Policies](https://docs.aws.amazon.com/AmazonS3/latest/userguide/security_iam_service-with-iam.html).
#' 
#' The following action is related to
#' [`get_bucket_notification`][s3_get_bucket_notification]:
#' 
#' -   [`put_bucket_notification`][s3_put_bucket_notification]
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#'
#' @usage
#' s3_get_bucket_notification_configuration(Bucket, ExpectedBucketOwner)
#'
#' @param Bucket &#91;required&#93; The name of the bucket for which to get the notification configuration.
#' 
#' When you use this API operation with an access point, provide the alias
#' of the access point in place of the bucket name.
#' 
#' When you use this API operation with an Object Lambda access point,
#' provide the alias of the Object Lambda access point in place of the
#' bucket name. If the Object Lambda access point alias in a request is not
#' valid, the error code `InvalidAccessPointAliasError` is returned. For
#' more information about `InvalidAccessPointAliasError`, see [List of
#' Error
#' Codes](https://docs.aws.amazon.com/AmazonS3/latest/API/ErrorResponses.html#ErrorCodeList).
#' @param ExpectedBucketOwner The account ID of the expected bucket owner. If the account ID that you
#' provide does not match the actual owner of the bucket, the request fails
#' with the HTTP status code `403 Forbidden` (access denied).
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   TopicConfigurations = list(
#'     list(
#'       Id = "string",
#'       TopicArn = "string",
#'       Events = list(
#'         "s3:ReducedRedundancyLostObject"|"s3:ObjectCreated:*"|"s3:ObjectCreated:Put"|"s3:ObjectCreated:Post"|"s3:ObjectCreated:Copy"|"s3:ObjectCreated:CompleteMultipartUpload"|"s3:ObjectRemoved:*"|"s3:ObjectRemoved:Delete"|"s3:ObjectRemoved:DeleteMarkerCreated"|"s3:ObjectRestore:*"|"s3:ObjectRestore:Post"|"s3:ObjectRestore:Completed"|"s3:Replication:*"|"s3:Replication:OperationFailedReplication"|"s3:Replication:OperationNotTracked"|"s3:Replication:OperationMissedThreshold"|"s3:Replication:OperationReplicatedAfterThreshold"|"s3:ObjectRestore:Delete"|"s3:LifecycleTransition"|"s3:IntelligentTiering"|"s3:ObjectAcl:Put"|"s3:LifecycleExpiration:*"|"s3:LifecycleExpiration:Delete"|"s3:LifecycleExpiration:DeleteMarkerCreated"|"s3:ObjectTagging:*"|"s3:ObjectTagging:Put"|"s3:ObjectTagging:Delete"
#'       ),
#'       Filter = list(
#'         Key = list(
#'           FilterRules = list(
#'             list(
#'               Name = "prefix"|"suffix",
#'               Value = "string"
#'             )
#'           )
#'         )
#'       )
#'     )
#'   ),
#'   QueueConfigurations = list(
#'     list(
#'       Id = "string",
#'       QueueArn = "string",
#'       Events = list(
#'         "s3:ReducedRedundancyLostObject"|"s3:ObjectCreated:*"|"s3:ObjectCreated:Put"|"s3:ObjectCreated:Post"|"s3:ObjectCreated:Copy"|"s3:ObjectCreated:CompleteMultipartUpload"|"s3:ObjectRemoved:*"|"s3:ObjectRemoved:Delete"|"s3:ObjectRemoved:DeleteMarkerCreated"|"s3:ObjectRestore:*"|"s3:ObjectRestore:Post"|"s3:ObjectRestore:Completed"|"s3:Replication:*"|"s3:Replication:OperationFailedReplication"|"s3:Replication:OperationNotTracked"|"s3:Replication:OperationMissedThreshold"|"s3:Replication:OperationReplicatedAfterThreshold"|"s3:ObjectRestore:Delete"|"s3:LifecycleTransition"|"s3:IntelligentTiering"|"s3:ObjectAcl:Put"|"s3:LifecycleExpiration:*"|"s3:LifecycleExpiration:Delete"|"s3:LifecycleExpiration:DeleteMarkerCreated"|"s3:ObjectTagging:*"|"s3:ObjectTagging:Put"|"s3:ObjectTagging:Delete"
#'       ),
#'       Filter = list(
#'         Key = list(
#'           FilterRules = list(
#'             list(
#'               Name = "prefix"|"suffix",
#'               Value = "string"
#'             )
#'           )
#'         )
#'       )
#'     )
#'   ),
#'   LambdaFunctionConfigurations = list(
#'     list(
#'       Id = "string",
#'       LambdaFunctionArn = "string",
#'       Events = list(
#'         "s3:ReducedRedundancyLostObject"|"s3:ObjectCreated:*"|"s3:ObjectCreated:Put"|"s3:ObjectCreated:Post"|"s3:ObjectCreated:Copy"|"s3:ObjectCreated:CompleteMultipartUpload"|"s3:ObjectRemoved:*"|"s3:ObjectRemoved:Delete"|"s3:ObjectRemoved:DeleteMarkerCreated"|"s3:ObjectRestore:*"|"s3:ObjectRestore:Post"|"s3:ObjectRestore:Completed"|"s3:Replication:*"|"s3:Replication:OperationFailedReplication"|"s3:Replication:OperationNotTracked"|"s3:Replication:OperationMissedThreshold"|"s3:Replication:OperationReplicatedAfterThreshold"|"s3:ObjectRestore:Delete"|"s3:LifecycleTransition"|"s3:IntelligentTiering"|"s3:ObjectAcl:Put"|"s3:LifecycleExpiration:*"|"s3:LifecycleExpiration:Delete"|"s3:LifecycleExpiration:DeleteMarkerCreated"|"s3:ObjectTagging:*"|"s3:ObjectTagging:Put"|"s3:ObjectTagging:Delete"
#'       ),
#'       Filter = list(
#'         Key = list(
#'           FilterRules = list(
#'             list(
#'               Name = "prefix"|"suffix",
#'               Value = "string"
#'             )
#'           )
#'         )
#'       )
#'     )
#'   ),
#'   EventBridgeConfiguration = list()
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$get_bucket_notification_configuration(
#'   Bucket = "string",
#'   ExpectedBucketOwner = "string"
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname s3_get_bucket_notification_configuration
#'
#' @aliases s3_get_bucket_notification_configuration
s3_get_bucket_notification_configuration <- function(Bucket, ExpectedBucketOwner = NULL) {
  op <- new_operation(
    name = "GetBucketNotificationConfiguration",
    http_method = "GET",
    http_path = "/{Bucket}?notification",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .s3$get_bucket_notification_configuration_input(Bucket = Bucket, ExpectedBucketOwner = ExpectedBucketOwner)
  output <- .s3$get_bucket_notification_configuration_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$get_bucket_notification_configuration <- s3_get_bucket_notification_configuration

#' This operation is not supported for directory buckets
#'
#' @description
#' This operation is not supported for directory buckets.
#' 
#' Retrieves `OwnershipControls` for an Amazon S3 bucket. To use this
#' operation, you must have the `s3:GetBucketOwnershipControls` permission.
#' For more information about Amazon S3 permissions, see [Specifying
#' permissions in a
#' policy](https://docs.aws.amazon.com/AmazonS3/latest/userguide/security_iam_service-with-iam.html#security_iam_service-with-iam-id-based-policies-actions).
#' 
#' A bucket doesn't have `OwnershipControls` settings in the following
#' cases:
#' 
#' -   The bucket was created before the `BucketOwnerEnforced` ownership
#'     setting was introduced and you've never explicitly applied this
#'     value
#' 
#' -   You've manually deleted the bucket ownership control value using the
#'     [`delete_bucket_ownership_controls`][s3_delete_bucket_ownership_controls]
#'     API operation.
#' 
#' By default, Amazon S3 sets `OwnershipControls` for all newly created
#' buckets.
#' 
#' For information about Amazon S3 Object Ownership, see [Using Object
#' Ownership](https://docs.aws.amazon.com/AmazonS3/latest/userguide/about-object-ownership.html).
#' 
#' The following operations are related to
#' [`get_bucket_ownership_controls`][s3_get_bucket_ownership_controls]:
#' 
#' -   [`put_bucket_ownership_controls`][s3_put_bucket_ownership_controls]
#' 
#' -   [`delete_bucket_ownership_controls`][s3_delete_bucket_ownership_controls]
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#'
#' @usage
#' s3_get_bucket_ownership_controls(Bucket, ExpectedBucketOwner)
#'
#' @param Bucket &#91;required&#93; The name of the Amazon S3 bucket whose `OwnershipControls` you want to
#' retrieve.
#' @param ExpectedBucketOwner The account ID of the expected bucket owner. If the account ID that you
#' provide does not match the actual owner of the bucket, the request fails
#' with the HTTP status code `403 Forbidden` (access denied).
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   OwnershipControls = list(
#'     Rules = list(
#'       list(
#'         ObjectOwnership = "BucketOwnerPreferred"|"ObjectWriter"|"BucketOwnerEnforced"
#'       )
#'     )
#'   )
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$get_bucket_ownership_controls(
#'   Bucket = "string",
#'   ExpectedBucketOwner = "string"
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname s3_get_bucket_ownership_controls
#'
#' @aliases s3_get_bucket_ownership_controls
s3_get_bucket_ownership_controls <- function(Bucket, ExpectedBucketOwner = NULL) {
  op <- new_operation(
    name = "GetBucketOwnershipControls",
    http_method = "GET",
    http_path = "/{Bucket}?ownershipControls",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .s3$get_bucket_ownership_controls_input(Bucket = Bucket, ExpectedBucketOwner = ExpectedBucketOwner)
  output <- .s3$get_bucket_ownership_controls_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$get_bucket_ownership_controls <- s3_get_bucket_ownership_controls

#' Returns the policy of a specified bucket
#'
#' @description
#' Returns the policy of a specified bucket.
#' 
#' **Directory buckets** - For directory buckets, you must make requests
#' for this API operation to the Regional endpoint. These endpoints support
#' path-style requests in the format
#' `https://s3express-control.region-code.amazonaws.com/bucket-name `.
#' Virtual-hosted-style requests aren't supported. For more information
#' about endpoints in Availability Zones, see [Regional and Zonal endpoints
#' for directory buckets in Availability
#' Zones](https://docs.aws.amazon.com/AmazonS3/latest/userguide/endpoint-directory-buckets-AZ.html)
#' in the *Amazon S3 User Guide*. For more information about endpoints in
#' Local Zones, see [Concepts for directory buckets in Local
#' Zones](https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-lzs-for-directory-buckets.html)
#' in the *Amazon S3 User Guide*.
#' 
#' ### Permissions
#' 
#' If you are using an identity other than the root user of the Amazon Web
#' Services account that owns the bucket, the calling identity must both
#' have the [`get_bucket_policy`][s3_get_bucket_policy] permissions on the
#' specified bucket and belong to the bucket owner's account in order to
#' use this operation.
#' 
#' If you don't have [`get_bucket_policy`][s3_get_bucket_policy]
#' permissions, Amazon S3 returns a `403 Access Denied` error. If you have
#' the correct permissions, but you're not using an identity that belongs
#' to the bucket owner's account, Amazon S3 returns a
#' `405 Method Not Allowed` error.
#' 
#' To ensure that bucket owners don't inadvertently lock themselves out of
#' their own buckets, the root principal in a bucket owner's Amazon Web
#' Services account can perform the
#' [`get_bucket_policy`][s3_get_bucket_policy],
#' [`put_bucket_policy`][s3_put_bucket_policy], and
#' [`delete_bucket_policy`][s3_delete_bucket_policy] API actions, even if
#' their bucket policy explicitly denies the root principal's access.
#' Bucket owner root principals can only be blocked from performing these
#' API actions by VPC endpoint policies and Amazon Web Services
#' Organizations policies.
#' 
#' -   **General purpose bucket permissions** - The `s3:GetBucketPolicy`
#'     permission is required in a policy. For more information about
#'     general purpose buckets bucket policies, see [Using Bucket Policies
#'     and User
#'     Policies](https://docs.aws.amazon.com/AmazonS3/latest/userguide/security_iam_service-with-iam.html)
#'     in the *Amazon S3 User Guide*.
#' 
#' -   **Directory bucket permissions** - To grant access to this API
#'     operation, you must have the `s3express:GetBucketPolicy` permission
#'     in an IAM identity-based policy instead of a bucket policy.
#'     Cross-account access to this API operation isn't supported. This
#'     operation can only be performed by the Amazon Web Services account
#'     that owns the resource. For more information about directory bucket
#'     policies and permissions, see [Amazon Web Services Identity and
#'     Access Management (IAM) for S3 Express One
#'     Zone](https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-express-security-iam.html)
#'     in the *Amazon S3 User Guide*.
#' 
#' ### Example bucket policies
#' 
#' **General purpose buckets example bucket policies** - See [Bucket policy
#' examples](https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-bucket-policies.html)
#' in the *Amazon S3 User Guide*.
#' 
#' **Directory bucket example bucket policies** - See [Example bucket
#' policies for S3 Express One
#' Zone](https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-express-security-iam-example-bucket-policies.html)
#' in the *Amazon S3 User Guide*.
#' 
#' ### HTTP Host header syntax
#' 
#' **Directory buckets** - The HTTP Host header syntax is
#' `s3express-control.region-code.amazonaws.com`.
#' 
#' The following action is related to
#' [`get_bucket_policy`][s3_get_bucket_policy]:
#' 
#' -   [`get_object`][s3_get_object]
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#'
#' @usage
#' s3_get_bucket_policy(Bucket, ExpectedBucketOwner)
#'
#' @param Bucket &#91;required&#93; The bucket name to get the bucket policy for.
#' 
#' **Directory buckets** - When you use this operation with a directory
#' bucket, you must use path-style requests in the format
#' `https://s3express-control.region-code.amazonaws.com/bucket-name `.
#' Virtual-hosted-style requests aren't supported. Directory bucket names
#' must be unique in the chosen Zone (Availability Zone or Local Zone).
#' Bucket names must also follow the format
#' ` bucket-base-name--zone-id--x-s3` (for example,
#' ` DOC-EXAMPLE-BUCKET--usw2-az1--x-s3`). For information about bucket
#' naming restrictions, see [Directory bucket naming
#' rules](https://docs.aws.amazon.com/AmazonS3/latest/userguide/directory-bucket-naming-rules.html)
#' in the *Amazon S3 User Guide*
#' 
#' **Access points** - When you use this API operation with an access
#' point, provide the alias of the access point in place of the bucket
#' name.
#' 
#' **Object Lambda access points** - When you use this API operation with
#' an Object Lambda access point, provide the alias of the Object Lambda
#' access point in place of the bucket name. If the Object Lambda access
#' point alias in a request is not valid, the error code
#' `InvalidAccessPointAliasError` is returned. For more information about
#' `InvalidAccessPointAliasError`, see [List of Error
#' Codes](https://docs.aws.amazon.com/AmazonS3/latest/API/ErrorResponses.html#ErrorCodeList).
#' 
#' Object Lambda access points are not supported by directory buckets.
#' @param ExpectedBucketOwner The account ID of the expected bucket owner. If the account ID that you
#' provide does not match the actual owner of the bucket, the request fails
#' with the HTTP status code `403 Forbidden` (access denied).
#' 
#' For directory buckets, this header is not supported in this API
#' operation. If you specify this header, the request fails with the HTTP
#' status code `501 Not Implemented`.
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   Policy = "string"
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$get_bucket_policy(
#'   Bucket = "string",
#'   ExpectedBucketOwner = "string"
#' )
#' ```
#'
#' @examples
#' \dontrun{
#' # The following example returns bucket policy associated with a bucket.
#' svc$get_bucket_policy(
#'   Bucket = "examplebucket"
#' )
#' }
#'
#' @keywords internal
#'
#' @rdname s3_get_bucket_policy
#'
#' @aliases s3_get_bucket_policy
s3_get_bucket_policy <- function(Bucket, ExpectedBucketOwner = NULL) {
  op <- new_operation(
    name = "GetBucketPolicy",
    http_method = "GET",
    http_path = "/{Bucket}?policy",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .s3$get_bucket_policy_input(Bucket = Bucket, ExpectedBucketOwner = ExpectedBucketOwner)
  output <- .s3$get_bucket_policy_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$get_bucket_policy <- s3_get_bucket_policy

#' This operation is not supported for directory buckets
#'
#' @description
#' This operation is not supported for directory buckets.
#' 
#' Retrieves the policy status for an Amazon S3 bucket, indicating whether
#' the bucket is public. In order to use this operation, you must have the
#' `s3:GetBucketPolicyStatus` permission. For more information about Amazon
#' S3 permissions, see [Specifying Permissions in a
#' Policy](https://docs.aws.amazon.com/AmazonS3/latest/userguide/security_iam_service-with-iam.html#security_iam_service-with-iam-id-based-policies-actions).
#' 
#' For more information about when Amazon S3 considers a bucket public, see
#' [The Meaning of
#' "Public"](https://docs.aws.amazon.com/AmazonS3/latest/userguide/access-control-block-public-access.html#access-control-block-public-access-policy-status).
#' 
#' The following operations are related to
#' [`get_bucket_policy_status`][s3_get_bucket_policy_status]:
#' 
#' -   [Using Amazon S3 Block Public
#'     Access](https://docs.aws.amazon.com/AmazonS3/latest/userguide/access-control-block-public-access.html)
#' 
#' -   [`get_public_access_block`][s3_get_public_access_block]
#' 
#' -   [`put_public_access_block`][s3_put_public_access_block]
#' 
#' -   [`delete_public_access_block`][s3_delete_public_access_block]
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#'
#' @usage
#' s3_get_bucket_policy_status(Bucket, ExpectedBucketOwner)
#'
#' @param Bucket &#91;required&#93; The name of the Amazon S3 bucket whose policy status you want to
#' retrieve.
#' @param ExpectedBucketOwner The account ID of the expected bucket owner. If the account ID that you
#' provide does not match the actual owner of the bucket, the request fails
#' with the HTTP status code `403 Forbidden` (access denied).
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   PolicyStatus = list(
#'     IsPublic = TRUE|FALSE
#'   )
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$get_bucket_policy_status(
#'   Bucket = "string",
#'   ExpectedBucketOwner = "string"
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname s3_get_bucket_policy_status
#'
#' @aliases s3_get_bucket_policy_status
s3_get_bucket_policy_status <- function(Bucket, ExpectedBucketOwner = NULL) {
  op <- new_operation(
    name = "GetBucketPolicyStatus",
    http_method = "GET",
    http_path = "/{Bucket}?policyStatus",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .s3$get_bucket_policy_status_input(Bucket = Bucket, ExpectedBucketOwner = ExpectedBucketOwner)
  output <- .s3$get_bucket_policy_status_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$get_bucket_policy_status <- s3_get_bucket_policy_status

#' This operation is not supported for directory buckets
#'
#' @description
#' This operation is not supported for directory buckets.
#' 
#' Returns the replication configuration of a bucket.
#' 
#' It can take a while to propagate the put or delete a replication
#' configuration to all Amazon S3 systems. Therefore, a get request soon
#' after put or delete can return a wrong result.
#' 
#' For information about replication configuration, see
#' [Replication](https://docs.aws.amazon.com/AmazonS3/latest/userguide/replication.html)
#' in the *Amazon S3 User Guide*.
#' 
#' This action requires permissions for the
#' `s3:GetReplicationConfiguration` action. For more information about
#' permissions, see [Using Bucket Policies and User
#' Policies](https://docs.aws.amazon.com/AmazonS3/latest/userguide/security_iam_service-with-iam.html).
#' 
#' If you include the `Filter` element in a replication configuration, you
#' must also include the `DeleteMarkerReplication` and `Priority` elements.
#' The response also returns those elements.
#' 
#' For information about
#' [`get_bucket_replication`][s3_get_bucket_replication] errors, see [List
#' of replication-related error
#' codes](https://docs.aws.amazon.com/AmazonS3/latest/API/ErrorResponses.html#ReplicationErrorCodeList)
#' 
#' The following operations are related to
#' [`get_bucket_replication`][s3_get_bucket_replication]:
#' 
#' -   [`put_bucket_replication`][s3_put_bucket_replication]
#' 
#' -   [`delete_bucket_replication`][s3_delete_bucket_replication]
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#'
#' @usage
#' s3_get_bucket_replication(Bucket, ExpectedBucketOwner)
#'
#' @param Bucket &#91;required&#93; The bucket name for which to get the replication information.
#' @param ExpectedBucketOwner The account ID of the expected bucket owner. If the account ID that you
#' provide does not match the actual owner of the bucket, the request fails
#' with the HTTP status code `403 Forbidden` (access denied).
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   ReplicationConfiguration = list(
#'     Role = "string",
#'     Rules = list(
#'       list(
#'         ID = "string",
#'         Priority = 123,
#'         Prefix = "string",
#'         Filter = list(
#'           Prefix = "string",
#'           Tag = list(
#'             Key = "string",
#'             Value = "string"
#'           ),
#'           And = list(
#'             Prefix = "string",
#'             Tags = list(
#'               list(
#'                 Key = "string",
#'                 Value = "string"
#'               )
#'             )
#'           )
#'         ),
#'         Status = "Enabled"|"Disabled",
#'         SourceSelectionCriteria = list(
#'           SseKmsEncryptedObjects = list(
#'             Status = "Enabled"|"Disabled"
#'           ),
#'           ReplicaModifications = list(
#'             Status = "Enabled"|"Disabled"
#'           )
#'         ),
#'         ExistingObjectReplication = list(
#'           Status = "Enabled"|"Disabled"
#'         ),
#'         Destination = list(
#'           Bucket = "string",
#'           Account = "string",
#'           StorageClass = "STANDARD"|"REDUCED_REDUNDANCY"|"STANDARD_IA"|"ONEZONE_IA"|"INTELLIGENT_TIERING"|"GLACIER"|"DEEP_ARCHIVE"|"OUTPOSTS"|"GLACIER_IR"|"SNOW"|"EXPRESS_ONEZONE"|"FSX_OPENZFS"|"FSX_ONTAP",
#'           AccessControlTranslation = list(
#'             Owner = "Destination"
#'           ),
#'           EncryptionConfiguration = list(
#'             ReplicaKmsKeyID = "string"
#'           ),
#'           ReplicationTime = list(
#'             Status = "Enabled"|"Disabled",
#'             Time = list(
#'               Minutes = 123
#'             )
#'           ),
#'           Metrics = list(
#'             Status = "Enabled"|"Disabled",
#'             EventThreshold = list(
#'               Minutes = 123
#'             )
#'           )
#'         ),
#'         DeleteMarkerReplication = list(
#'           Status = "Enabled"|"Disabled"
#'         )
#'       )
#'     )
#'   )
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$get_bucket_replication(
#'   Bucket = "string",
#'   ExpectedBucketOwner = "string"
#' )
#' ```
#'
#' @examples
#' \dontrun{
#' # The following example returns replication configuration set on a bucket.
#' svc$get_bucket_replication(
#'   Bucket = "examplebucket"
#' )
#' }
#'
#' @keywords internal
#'
#' @rdname s3_get_bucket_replication
#'
#' @aliases s3_get_bucket_replication
s3_get_bucket_replication <- function(Bucket, ExpectedBucketOwner = NULL) {
  op <- new_operation(
    name = "GetBucketReplication",
    http_method = "GET",
    http_path = "/{Bucket}?replication",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .s3$get_bucket_replication_input(Bucket = Bucket, ExpectedBucketOwner = ExpectedBucketOwner)
  output <- .s3$get_bucket_replication_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$get_bucket_replication <- s3_get_bucket_replication

#' This operation is not supported for directory buckets
#'
#' @description
#' This operation is not supported for directory buckets.
#' 
#' Returns the request payment configuration of a bucket. To use this
#' version of the operation, you must be the bucket owner. For more
#' information, see [Requester Pays
#' Buckets](https://docs.aws.amazon.com/AmazonS3/latest/userguide/RequesterPaysBuckets.html).
#' 
#' The following operations are related to
#' [`get_bucket_request_payment`][s3_get_bucket_request_payment]:
#' 
#' -   [`list_objects`][s3_list_objects]
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#'
#' @usage
#' s3_get_bucket_request_payment(Bucket, ExpectedBucketOwner)
#'
#' @param Bucket &#91;required&#93; The name of the bucket for which to get the payment request
#' configuration
#' @param ExpectedBucketOwner The account ID of the expected bucket owner. If the account ID that you
#' provide does not match the actual owner of the bucket, the request fails
#' with the HTTP status code `403 Forbidden` (access denied).
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   Payer = "Requester"|"BucketOwner"
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$get_bucket_request_payment(
#'   Bucket = "string",
#'   ExpectedBucketOwner = "string"
#' )
#' ```
#'
#' @examples
#' \dontrun{
#' # The following example retrieves bucket versioning configuration.
#' svc$get_bucket_request_payment(
#'   Bucket = "examplebucket"
#' )
#' }
#'
#' @keywords internal
#'
#' @rdname s3_get_bucket_request_payment
#'
#' @aliases s3_get_bucket_request_payment
s3_get_bucket_request_payment <- function(Bucket, ExpectedBucketOwner = NULL) {
  op <- new_operation(
    name = "GetBucketRequestPayment",
    http_method = "GET",
    http_path = "/{Bucket}?requestPayment",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .s3$get_bucket_request_payment_input(Bucket = Bucket, ExpectedBucketOwner = ExpectedBucketOwner)
  output <- .s3$get_bucket_request_payment_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$get_bucket_request_payment <- s3_get_bucket_request_payment

#' This operation is not supported for directory buckets
#'
#' @description
#' This operation is not supported for directory buckets.
#' 
#' Returns the tag set associated with the general purpose bucket.
#' 
#' if ABAC is not enabled for the bucket. When you [enable ABAC for a
#' general purpose
#' bucket](https://docs.aws.amazon.com/AmazonS3/latest/userguide/buckets-tagging-enable-abac.html),
#' you can no longer use this operation for that bucket and must use
#' [ListTagsForResource](https://docs.aws.amazon.com/AmazonS3/latest/API/API_control_ListTagsForResource.html)
#' instead.
#' 
#' To use this operation, you must have permission to perform the
#' `s3:GetBucketTagging` action. By default, the bucket owner has this
#' permission and can grant this permission to others.
#' 
#' [`get_bucket_tagging`][s3_get_bucket_tagging] has the following special
#' error:
#' 
#' -   Error code: `NoSuchTagSet`
#' 
#'     -   Description: There is no tag set associated with the bucket.
#' 
#' The following operations are related to
#' [`get_bucket_tagging`][s3_get_bucket_tagging]:
#' 
#' -   [`put_bucket_tagging`][s3_put_bucket_tagging]
#' 
#' -   [`delete_bucket_tagging`][s3_delete_bucket_tagging]
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#'
#' @usage
#' s3_get_bucket_tagging(Bucket, ExpectedBucketOwner)
#'
#' @param Bucket &#91;required&#93; The name of the bucket for which to get the tagging information.
#' @param ExpectedBucketOwner The account ID of the expected bucket owner. If the account ID that you
#' provide does not match the actual owner of the bucket, the request fails
#' with the HTTP status code `403 Forbidden` (access denied).
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   TagSet = list(
#'     list(
#'       Key = "string",
#'       Value = "string"
#'     )
#'   )
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$get_bucket_tagging(
#'   Bucket = "string",
#'   ExpectedBucketOwner = "string"
#' )
#' ```
#'
#' @examples
#' \dontrun{
#' # The following example returns tag set associated with a bucket
#' svc$get_bucket_tagging(
#'   Bucket = "examplebucket"
#' )
#' }
#'
#' @keywords internal
#'
#' @rdname s3_get_bucket_tagging
#'
#' @aliases s3_get_bucket_tagging
s3_get_bucket_tagging <- function(Bucket, ExpectedBucketOwner = NULL) {
  op <- new_operation(
    name = "GetBucketTagging",
    http_method = "GET",
    http_path = "/{Bucket}?tagging",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .s3$get_bucket_tagging_input(Bucket = Bucket, ExpectedBucketOwner = ExpectedBucketOwner)
  output <- .s3$get_bucket_tagging_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$get_bucket_tagging <- s3_get_bucket_tagging

#' This operation is not supported for directory buckets
#'
#' @description
#' This operation is not supported for directory buckets.
#' 
#' Returns the versioning state of a bucket.
#' 
#' To retrieve the versioning state of a bucket, you must be the bucket
#' owner.
#' 
#' This implementation also returns the MFA Delete status of the versioning
#' state. If the MFA Delete status is `enabled`, the bucket owner must use
#' an authentication device to change the versioning state of the bucket.
#' 
#' The following operations are related to
#' [`get_bucket_versioning`][s3_get_bucket_versioning]:
#' 
#' -   [`get_object`][s3_get_object]
#' 
#' -   [`put_object`][s3_put_object]
#' 
#' -   [`delete_object`][s3_delete_object]
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#'
#' @usage
#' s3_get_bucket_versioning(Bucket, ExpectedBucketOwner)
#'
#' @param Bucket &#91;required&#93; The name of the bucket for which to get the versioning information.
#' @param ExpectedBucketOwner The account ID of the expected bucket owner. If the account ID that you
#' provide does not match the actual owner of the bucket, the request fails
#' with the HTTP status code `403 Forbidden` (access denied).
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   Status = "Enabled"|"Suspended",
#'   MFADelete = "Enabled"|"Disabled"
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$get_bucket_versioning(
#'   Bucket = "string",
#'   ExpectedBucketOwner = "string"
#' )
#' ```
#'
#' @examples
#' \dontrun{
#' # The following example retrieves bucket versioning configuration.
#' svc$get_bucket_versioning(
#'   Bucket = "examplebucket"
#' )
#' }
#'
#' @keywords internal
#'
#' @rdname s3_get_bucket_versioning
#'
#' @aliases s3_get_bucket_versioning
s3_get_bucket_versioning <- function(Bucket, ExpectedBucketOwner = NULL) {
  op <- new_operation(
    name = "GetBucketVersioning",
    http_method = "GET",
    http_path = "/{Bucket}?versioning",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .s3$get_bucket_versioning_input(Bucket = Bucket, ExpectedBucketOwner = ExpectedBucketOwner)
  output <- .s3$get_bucket_versioning_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$get_bucket_versioning <- s3_get_bucket_versioning

#' This operation is not supported for directory buckets
#'
#' @description
#' This operation is not supported for directory buckets.
#' 
#' Returns the website configuration for a bucket. To host website on
#' Amazon S3, you can configure a bucket as website by adding a website
#' configuration. For more information about hosting websites, see [Hosting
#' Websites on Amazon
#' S3](https://docs.aws.amazon.com/AmazonS3/latest/userguide/WebsiteHosting.html).
#' 
#' This GET action requires the `S3:GetBucketWebsite` permission. By
#' default, only the bucket owner can read the bucket website
#' configuration. However, bucket owners can allow other users to read the
#' website configuration by writing a bucket policy granting them the
#' `S3:GetBucketWebsite` permission.
#' 
#' The following operations are related to
#' [`get_bucket_website`][s3_get_bucket_website]:
#' 
#' -   [`delete_bucket_website`][s3_delete_bucket_website]
#' 
#' -   [`put_bucket_website`][s3_put_bucket_website]
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#'
#' @usage
#' s3_get_bucket_website(Bucket, ExpectedBucketOwner)
#'
#' @param Bucket &#91;required&#93; The bucket name for which to get the website configuration.
#' @param ExpectedBucketOwner The account ID of the expected bucket owner. If the account ID that you
#' provide does not match the actual owner of the bucket, the request fails
#' with the HTTP status code `403 Forbidden` (access denied).
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   RedirectAllRequestsTo = list(
#'     HostName = "string",
#'     Protocol = "http"|"https"
#'   ),
#'   IndexDocument = list(
#'     Suffix = "string"
#'   ),
#'   ErrorDocument = list(
#'     Key = "string"
#'   ),
#'   RoutingRules = list(
#'     list(
#'       Condition = list(
#'         HttpErrorCodeReturnedEquals = "string",
#'         KeyPrefixEquals = "string"
#'       ),
#'       Redirect = list(
#'         HostName = "string",
#'         HttpRedirectCode = "string",
#'         Protocol = "http"|"https",
#'         ReplaceKeyPrefixWith = "string",
#'         ReplaceKeyWith = "string"
#'       )
#'     )
#'   )
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$get_bucket_website(
#'   Bucket = "string",
#'   ExpectedBucketOwner = "string"
#' )
#' ```
#'
#' @examples
#' \dontrun{
#' # The following example retrieves website configuration of a bucket.
#' svc$get_bucket_website(
#'   Bucket = "examplebucket"
#' )
#' }
#'
#' @keywords internal
#'
#' @rdname s3_get_bucket_website
#'
#' @aliases s3_get_bucket_website
s3_get_bucket_website <- function(Bucket, ExpectedBucketOwner = NULL) {
  op <- new_operation(
    name = "GetBucketWebsite",
    http_method = "GET",
    http_path = "/{Bucket}?website",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .s3$get_bucket_website_input(Bucket = Bucket, ExpectedBucketOwner = ExpectedBucketOwner)
  output <- .s3$get_bucket_website_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$get_bucket_website <- s3_get_bucket_website

#' Retrieves an object from Amazon S3
#'
#' @description
#' Retrieves an object from Amazon S3.
#' 
#' In the [`get_object`][s3_get_object] request, specify the full key name
#' for the object.
#' 
#' **General purpose buckets** - Both the virtual-hosted-style requests and
#' the path-style requests are supported. For a virtual hosted-style
#' request example, if you have the object
#' `photos/2006/February/sample.jpg`, specify the object key name as
#' `/photos/2006/February/sample.jpg`. For a path-style request example, if
#' you have the object `photos/2006/February/sample.jpg` in the bucket
#' named `examplebucket`, specify the object key name as
#' `/examplebucket/photos/2006/February/sample.jpg`. For more information
#' about request types, see [HTTP Host Header Bucket
#' Specification](https://docs.aws.amazon.com/AmazonS3/latest/userguide/VirtualHosting.html#VirtualHostingSpecifyBucket)
#' in the *Amazon S3 User Guide*.
#' 
#' **Directory buckets** - Only virtual-hosted-style requests are
#' supported. For a virtual hosted-style request example, if you have the
#' object `photos/2006/February/sample.jpg` in the bucket named
#' `amzn-s3-demo-bucket--usw2-az1--x-s3`, specify the object key name as
#' `/photos/2006/February/sample.jpg`. Also, when you make requests to this
#' API operation, your requests are sent to the Zonal endpoint. These
#' endpoints support virtual-hosted-style requests in the format
#' `https://bucket-name.s3express-zone-id.region-code.amazonaws.com/key-name `.
#' Path-style requests are not supported. For more information about
#' endpoints in Availability Zones, see [Regional and Zonal endpoints for
#' directory buckets in Availability
#' Zones](https://docs.aws.amazon.com/AmazonS3/latest/userguide/endpoint-directory-buckets-AZ.html)
#' in the *Amazon S3 User Guide*. For more information about endpoints in
#' Local Zones, see [Concepts for directory buckets in Local
#' Zones](https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-lzs-for-directory-buckets.html)
#' in the *Amazon S3 User Guide*.
#' 
#' ### Permissions
#' 
#' -   **General purpose bucket permissions** - You must have the required
#'     permissions in a policy. To use [`get_object`][s3_get_object], you
#'     must have the `READ` access to the object (or version). If you grant
#'     `READ` access to the anonymous user, the
#'     [`get_object`][s3_get_object] operation returns the object without
#'     using an authorization header. For more information, see [Specifying
#'     permissions in a
#'     policy](https://docs.aws.amazon.com/AmazonS3/latest/userguide/security_iam_service-with-iam.html#security_iam_service-with-iam-id-based-policies-actions)
#'     in the *Amazon S3 User Guide*.
#' 
#'     If you include a `versionId` in your request header, you must have
#'     the `s3:GetObjectVersion` permission to access a specific version of
#'     an object. The `s3:GetObject` permission is not required in this
#'     scenario.
#' 
#'     If you request the current version of an object without a specific
#'     `versionId` in the request header, only the `s3:GetObject`
#'     permission is required. The `s3:GetObjectVersion` permission is not
#'     required in this scenario.
#' 
#'     If the object that you request doesn’t exist, the error that Amazon
#'     S3 returns depends on whether you also have the `s3:ListBucket`
#'     permission.
#' 
#'     -   If you have the `s3:ListBucket` permission on the bucket, Amazon
#'         S3 returns an HTTP status code `404 Not Found` error.
#' 
#'     -   If you don’t have the `s3:ListBucket` permission, Amazon S3
#'         returns an HTTP status code `403 Access Denied` error.
#' 
#' -   **Directory bucket permissions** - To grant access to this API
#'     operation on a directory bucket, we recommend that you use the
#'     [`create_session`](https://docs.aws.amazon.com/AmazonS3/latest/API/API_CreateSession.html)
#'     API operation for session-based authorization. Specifically, you
#'     grant the `s3express:CreateSession` permission to the directory
#'     bucket in a bucket policy or an IAM identity-based policy. Then, you
#'     make the [`create_session`][s3_create_session] API call on the
#'     bucket to obtain a session token. With the session token in your
#'     request header, you can make API requests to this operation. After
#'     the session token expires, you make another
#'     [`create_session`][s3_create_session] API call to generate a new
#'     session token for use. Amazon Web Services CLI or SDKs create
#'     session and refresh the session token automatically to avoid service
#'     interruptions when a session expires. For more information about
#'     authorization, see
#'     [`create_session`](https://docs.aws.amazon.com/AmazonS3/latest/API/API_CreateSession.html)
#'     .
#' 
#'     If the object is encrypted using SSE-KMS, you must also have the
#'     `kms:GenerateDataKey` and `kms:Decrypt` permissions in IAM
#'     identity-based policies and KMS key policies for the KMS key.
#' 
#' ### Storage classes
#' 
#' If the object you are retrieving is stored in the S3 Glacier Flexible
#' Retrieval storage class, the S3 Glacier Deep Archive storage class, the
#' S3 Intelligent-Tiering Archive Access tier, or the S3
#' Intelligent-Tiering Deep Archive Access tier, before you can retrieve
#' the object you must first restore a copy using
#' [`restore_object`][s3_restore_object]. Otherwise, this operation returns
#' an `InvalidObjectState` error. For information about restoring archived
#' objects, see [Restoring Archived
#' Objects](https://docs.aws.amazon.com/AmazonS3/latest/userguide/restoring-objects.html)
#' in the *Amazon S3 User Guide*.
#' 
#' **Directory buckets** - Directory buckets only support `EXPRESS_ONEZONE`
#' (the S3 Express One Zone storage class) in Availability Zones and
#' `ONEZONE_IA` (the S3 One Zone-Infrequent Access storage class) in
#' Dedicated Local Zones. Unsupported storage class values won't write a
#' destination object and will respond with the HTTP status code
#' `400 Bad Request`.
#' 
#' ### Encryption
#' 
#' Encryption request headers, like `x-amz-server-side-encryption`, should
#' not be sent for the [`get_object`][s3_get_object] requests, if your
#' object uses server-side encryption with Amazon S3 managed encryption
#' keys (SSE-S3), server-side encryption with Key Management Service (KMS)
#' keys (SSE-KMS), or dual-layer server-side encryption with Amazon Web
#' Services KMS keys (DSSE-KMS). If you include the header in your
#' [`get_object`][s3_get_object] requests for the object that uses these
#' types of keys, you’ll get an HTTP `400 Bad Request` error.
#' 
#' **Directory buckets** - For directory buckets, there are only two
#' supported options for server-side encryption: SSE-S3 and SSE-KMS. SSE-C
#' isn't supported. For more information, see [Protecting data with
#' server-side
#' encryption](https://docs.aws.amazon.com/AmazonS3/latest/userguide/) in
#' the *Amazon S3 User Guide*.
#' 
#' ### Overriding response header values through the request
#' 
#' There are times when you want to override certain response header values
#' of a [`get_object`][s3_get_object] response. For example, you might
#' override the `Content-Disposition` response header value through your
#' [`get_object`][s3_get_object] request.
#' 
#' You can override values for a set of response headers. These modified
#' response header values are included only in a successful response, that
#' is, when the HTTP status code `200 OK` is returned. The headers you can
#' override using the following query parameters in the request are a
#' subset of the headers that Amazon S3 accepts when you create an object.
#' 
#' The response headers that you can override for the
#' [`get_object`][s3_get_object] response are `Cache-Control`,
#' `Content-Disposition`, `Content-Encoding`, `Content-Language`,
#' `Content-Type`, and `Expires`.
#' 
#' To override values for a set of response headers in the
#' [`get_object`][s3_get_object] response, you can use the following query
#' parameters in the request.
#' 
#' -   `response-cache-control`
#' 
#' -   `response-content-disposition`
#' 
#' -   `response-content-encoding`
#' 
#' -   `response-content-language`
#' 
#' -   `response-content-type`
#' 
#' -   `response-expires`
#' 
#' When you use these parameters, you must sign the request by using either
#' an Authorization header or a presigned URL. These parameters cannot be
#' used with an unsigned (anonymous) request.
#' 
#' ### HTTP Host header syntax
#' 
#' **Directory buckets** - The HTTP Host header syntax is
#' ` Bucket-name.s3express-zone-id.region-code.amazonaws.com`.
#' 
#' The following operations are related to [`get_object`][s3_get_object]:
#' 
#' -   [`list_buckets`][s3_list_buckets]
#' 
#' -   [`get_object_acl`][s3_get_object_acl]
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#'
#' @usage
#' s3_get_object(Bucket, IfMatch, IfModifiedSince, IfNoneMatch,
#'   IfUnmodifiedSince, Key, Range, ResponseCacheControl,
#'   ResponseContentDisposition, ResponseContentEncoding,
#'   ResponseContentLanguage, ResponseContentType, ResponseExpires,
#'   VersionId, SSECustomerAlgorithm, SSECustomerKey, SSECustomerKeyMD5,
#'   RequestPayer, PartNumber, ExpectedBucketOwner, ChecksumMode)
#'
#' @param Bucket &#91;required&#93; The bucket name containing the object.
#' 
#' **Directory buckets** - When you use this operation with a directory
#' bucket, you must use virtual-hosted-style requests in the format
#' ` Bucket-name.s3express-zone-id.region-code.amazonaws.com`. Path-style
#' requests are not supported. Directory bucket names must be unique in the
#' chosen Zone (Availability Zone or Local Zone). Bucket names must follow
#' the format ` bucket-base-name--zone-id--x-s3` (for example,
#' ` amzn-s3-demo-bucket--usw2-az1--x-s3`). For information about bucket
#' naming restrictions, see [Directory bucket naming
#' rules](https://docs.aws.amazon.com/AmazonS3/latest/userguide/directory-bucket-naming-rules.html)
#' in the *Amazon S3 User Guide*.
#' 
#' **Access points** - When you use this action with an access point for
#' general purpose buckets, you must provide the alias of the access point
#' in place of the bucket name or specify the access point ARN. When you
#' use this action with an access point for directory buckets, you must
#' provide the access point name in place of the bucket name. When using
#' the access point ARN, you must direct requests to the access point
#' hostname. The access point hostname takes the form
#' *AccessPointName*-*AccountId*.s3-accesspoint.*Region*.amazonaws.com.
#' When using this action with an access point through the Amazon Web
#' Services SDKs, you provide the access point ARN in place of the bucket
#' name. For more information about access point ARNs, see [Using access
#' points](https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-access-points.html)
#' in the *Amazon S3 User Guide*.
#' 
#' **Object Lambda access points** - When you use this action with an
#' Object Lambda access point, you must direct requests to the Object
#' Lambda access point hostname. The Object Lambda access point hostname
#' takes the form
#' *AccessPointName*-*AccountId*.s3-object-lambda.*Region*.amazonaws.com.
#' 
#' Object Lambda access points are not supported by directory buckets.
#' 
#' **S3 on Outposts** - When you use this action with S3 on Outposts, you
#' must direct requests to the S3 on Outposts hostname. The S3 on Outposts
#' hostname takes the form
#' ` AccessPointName-AccountId.outpostID.s3-outposts.Region.amazonaws.com`.
#' When you use this action with S3 on Outposts, the destination bucket
#' must be the Outposts access point ARN or the access point alias. For
#' more information about S3 on Outposts, see [What is S3 on
#' Outposts?](https://docs.aws.amazon.com/AmazonS3/latest/s3-outposts/S3onOutposts.html)
#' in the *Amazon S3 User Guide*.
#' @param IfMatch Return the object only if its entity tag (ETag) is the same as the one
#' specified in this header; otherwise, return a `412 Precondition Failed`
#' error.
#' 
#' If both of the `If-Match` and `If-Unmodified-Since` headers are present
#' in the request as follows: `If-Match` condition evaluates to `true`,
#' and; `If-Unmodified-Since` condition evaluates to `false`; then, S3
#' returns `200 OK` and the data requested.
#' 
#' For more information about conditional requests, see [RFC
#' 7232](https://datatracker.ietf.org/doc/html/rfc7232).
#' @param IfModifiedSince Return the object only if it has been modified since the specified time;
#' otherwise, return a `304 Not Modified` error.
#' 
#' If both of the `If-None-Match` and `If-Modified-Since` headers are
#' present in the request as follows:` If-None-Match` condition evaluates
#' to `false`, and; `If-Modified-Since` condition evaluates to `true`;
#' then, S3 returns `304 Not Modified` status code.
#' 
#' For more information about conditional requests, see [RFC
#' 7232](https://datatracker.ietf.org/doc/html/rfc7232).
#' @param IfNoneMatch Return the object only if its entity tag (ETag) is different from the
#' one specified in this header; otherwise, return a `304 Not Modified`
#' error.
#' 
#' If both of the `If-None-Match` and `If-Modified-Since` headers are
#' present in the request as follows:` If-None-Match` condition evaluates
#' to `false`, and; `If-Modified-Since` condition evaluates to `true`;
#' then, S3 returns `304 Not Modified` HTTP status code.
#' 
#' For more information about conditional requests, see [RFC
#' 7232](https://datatracker.ietf.org/doc/html/rfc7232).
#' @param IfUnmodifiedSince Return the object only if it has not been modified since the specified
#' time; otherwise, return a `412 Precondition Failed` error.
#' 
#' If both of the `If-Match` and `If-Unmodified-Since` headers are present
#' in the request as follows: `If-Match` condition evaluates to `true`,
#' and; `If-Unmodified-Since` condition evaluates to `false`; then, S3
#' returns `200 OK` and the data requested.
#' 
#' For more information about conditional requests, see [RFC
#' 7232](https://datatracker.ietf.org/doc/html/rfc7232).
#' @param Key &#91;required&#93; Key of the object to get.
#' @param Range Downloads the specified byte range of an object. For more information
#' about the HTTP Range header, see
#' <https://www.rfc-editor.org/rfc/rfc9110.html#name-range>.
#' 
#' Amazon S3 doesn't support retrieving multiple ranges of data per `GET`
#' request.
#' @param ResponseCacheControl Sets the `Cache-Control` header of the response.
#' @param ResponseContentDisposition Sets the `Content-Disposition` header of the response.
#' @param ResponseContentEncoding Sets the `Content-Encoding` header of the response.
#' @param ResponseContentLanguage Sets the `Content-Language` header of the response.
#' @param ResponseContentType Sets the `Content-Type` header of the response.
#' @param ResponseExpires Sets the `Expires` header of the response.
#' @param VersionId Version ID used to reference a specific version of the object.
#' 
#' By default, the [`get_object`][s3_get_object] operation returns the
#' current version of an object. To return a different version, use the
#' `versionId` subresource.
#' 
#' -   If you include a `versionId` in your request header, you must have
#'     the `s3:GetObjectVersion` permission to access a specific version of
#'     an object. The `s3:GetObject` permission is not required in this
#'     scenario.
#' 
#' -   If you request the current version of an object without a specific
#'     `versionId` in the request header, only the `s3:GetObject`
#'     permission is required. The `s3:GetObjectVersion` permission is not
#'     required in this scenario.
#' 
#' -   **Directory buckets** - S3 Versioning isn't enabled and supported
#'     for directory buckets. For this API operation, only the `null` value
#'     of the version ID is supported by directory buckets. You can only
#'     specify `null` to the `versionId` query parameter in the request.
#' 
#' For more information about versioning, see
#' [`put_bucket_versioning`][s3_put_bucket_versioning].
#' @param SSECustomerAlgorithm Specifies the algorithm to use when decrypting the object (for example,
#' `AES256`).
#' 
#' If you encrypt an object by using server-side encryption with
#' customer-provided encryption keys (SSE-C) when you store the object in
#' Amazon S3, then when you GET the object, you must use the following
#' headers:
#' 
#' -   `x-amz-server-side-encryption-customer-algorithm`
#' 
#' -   `x-amz-server-side-encryption-customer-key`
#' 
#' -   `x-amz-server-side-encryption-customer-key-MD5`
#' 
#' For more information about SSE-C, see [Server-Side Encryption (Using
#' Customer-Provided Encryption
#' Keys)](https://docs.aws.amazon.com/AmazonS3/latest/userguide/ServerSideEncryptionCustomerKeys.html)
#' in the *Amazon S3 User Guide*.
#' 
#' This functionality is not supported for directory buckets.
#' @param SSECustomerKey Specifies the customer-provided encryption key that you originally
#' provided for Amazon S3 to encrypt the data before storing it. This value
#' is used to decrypt the object when recovering it and must match the one
#' used when storing the data. The key must be appropriate for use with the
#' algorithm specified in the
#' `x-amz-server-side-encryption-customer-algorithm` header.
#' 
#' If you encrypt an object by using server-side encryption with
#' customer-provided encryption keys (SSE-C) when you store the object in
#' Amazon S3, then when you GET the object, you must use the following
#' headers:
#' 
#' -   `x-amz-server-side-encryption-customer-algorithm`
#' 
#' -   `x-amz-server-side-encryption-customer-key`
#' 
#' -   `x-amz-server-side-encryption-customer-key-MD5`
#' 
#' For more information about SSE-C, see [Server-Side Encryption (Using
#' Customer-Provided Encryption
#' Keys)](https://docs.aws.amazon.com/AmazonS3/latest/userguide/ServerSideEncryptionCustomerKeys.html)
#' in the *Amazon S3 User Guide*.
#' 
#' This functionality is not supported for directory buckets.
#' @param SSECustomerKeyMD5 Specifies the 128-bit MD5 digest of the customer-provided encryption key
#' according to RFC 1321. Amazon S3 uses this header for a message
#' integrity check to ensure that the encryption key was transmitted
#' without error.
#' 
#' If you encrypt an object by using server-side encryption with
#' customer-provided encryption keys (SSE-C) when you store the object in
#' Amazon S3, then when you GET the object, you must use the following
#' headers:
#' 
#' -   `x-amz-server-side-encryption-customer-algorithm`
#' 
#' -   `x-amz-server-side-encryption-customer-key`
#' 
#' -   `x-amz-server-side-encryption-customer-key-MD5`
#' 
#' For more information about SSE-C, see [Server-Side Encryption (Using
#' Customer-Provided Encryption
#' Keys)](https://docs.aws.amazon.com/AmazonS3/latest/userguide/ServerSideEncryptionCustomerKeys.html)
#' in the *Amazon S3 User Guide*.
#' 
#' This functionality is not supported for directory buckets.
#' @param RequestPayer 
#' @param PartNumber Part number of the object being read. This is a positive integer between
#' 1 and 10,000. Effectively performs a 'ranged' GET request for the part
#' specified. Useful for downloading just a part of an object.
#' @param ExpectedBucketOwner The account ID of the expected bucket owner. If the account ID that you
#' provide does not match the actual owner of the bucket, the request fails
#' with the HTTP status code `403 Forbidden` (access denied).
#' @param ChecksumMode To retrieve the checksum, this mode must be enabled.
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   Body = raw,
#'   DeleteMarker = TRUE|FALSE,
#'   AcceptRanges = "string",
#'   Expiration = "string",
#'   Restore = "string",
#'   LastModified = as.POSIXct(
#'     "2015-01-01"
#'   ),
#'   ContentLength = 123,
#'   ETag = "string",
#'   ChecksumCRC32 = "string",
#'   ChecksumCRC32C = "string",
#'   ChecksumCRC64NVME = "string",
#'   ChecksumSHA1 = "string",
#'   ChecksumSHA256 = "string",
#'   ChecksumType = "COMPOSITE"|"FULL_OBJECT",
#'   MissingMeta = 123,
#'   VersionId = "string",
#'   CacheControl = "string",
#'   ContentDisposition = "string",
#'   ContentEncoding = "string",
#'   ContentLanguage = "string",
#'   ContentRange = "string",
#'   ContentType = "string",
#'   Expires = "string",
#'   WebsiteRedirectLocation = "string",
#'   ServerSideEncryption = "AES256"|"aws:fsx"|"aws:kms"|"aws:kms:dsse",
#'   Metadata = list(
#'     "string"
#'   ),
#'   SSECustomerAlgorithm = "string",
#'   SSECustomerKeyMD5 = "string",
#'   SSEKMSKeyId = "string",
#'   BucketKeyEnabled = TRUE|FALSE,
#'   StorageClass = "STANDARD"|"REDUCED_REDUNDANCY"|"STANDARD_IA"|"ONEZONE_IA"|"INTELLIGENT_TIERING"|"GLACIER"|"DEEP_ARCHIVE"|"OUTPOSTS"|"GLACIER_IR"|"SNOW"|"EXPRESS_ONEZONE"|"FSX_OPENZFS"|"FSX_ONTAP",
#'   RequestCharged = "requester",
#'   ReplicationStatus = "COMPLETE"|"PENDING"|"FAILED"|"REPLICA"|"COMPLETED",
#'   PartsCount = 123,
#'   TagCount = 123,
#'   ObjectLockMode = "GOVERNANCE"|"COMPLIANCE",
#'   ObjectLockRetainUntilDate = as.POSIXct(
#'     "2015-01-01"
#'   ),
#'   ObjectLockLegalHoldStatus = "ON"|"OFF"
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$get_object(
#'   Bucket = "string",
#'   IfMatch = "string",
#'   IfModifiedSince = as.POSIXct(
#'     "2015-01-01"
#'   ),
#'   IfNoneMatch = "string",
#'   IfUnmodifiedSince = as.POSIXct(
#'     "2015-01-01"
#'   ),
#'   Key = "string",
#'   Range = "string",
#'   ResponseCacheControl = "string",
#'   ResponseContentDisposition = "string",
#'   ResponseContentEncoding = "string",
#'   ResponseContentLanguage = "string",
#'   ResponseContentType = "string",
#'   ResponseExpires = as.POSIXct(
#'     "2015-01-01"
#'   ),
#'   VersionId = "string",
#'   SSECustomerAlgorithm = "string",
#'   SSECustomerKey = "string",
#'   SSECustomerKeyMD5 = "string",
#'   RequestPayer = "requester",
#'   PartNumber = 123,
#'   ExpectedBucketOwner = "string",
#'   ChecksumMode = "ENABLED"
#' )
#' ```
#'
#' @examples
#' \dontrun{
#' # The following example retrieves an object for an S3 bucket.
#' svc$get_object(
#'   Bucket = "examplebucket",
#'   Key = "HappyFace.jpg"
#' )
#' 
#' # The following example retrieves an object for an S3 bucket. The request
#' # specifies the range header to retrieve a specific byte range.
#' svc$get_object(
#'   Bucket = "examplebucket",
#'   Key = "SampleFile.txt",
#'   Range = "bytes=0-9"
#' )
#' }
#'
#' @keywords internal
#'
#' @rdname s3_get_object
#'
#' @aliases s3_get_object
s3_get_object <- function(Bucket, IfMatch = NULL, IfModifiedSince = NULL, IfNoneMatch = NULL, IfUnmodifiedSince = NULL, Key, Range = NULL, ResponseCacheControl = NULL, ResponseContentDisposition = NULL, ResponseContentEncoding = NULL, ResponseContentLanguage = NULL, ResponseContentType = NULL, ResponseExpires = NULL, VersionId = NULL, SSECustomerAlgorithm = NULL, SSECustomerKey = NULL, SSECustomerKeyMD5 = NULL, RequestPayer = NULL, PartNumber = NULL, ExpectedBucketOwner = NULL, ChecksumMode = NULL) {
  op <- new_operation(
    name = "GetObject",
    http_method = "GET",
    http_path = "/{Bucket}/{Key+}",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .s3$get_object_input(Bucket = Bucket, IfMatch = IfMatch, IfModifiedSince = IfModifiedSince, IfNoneMatch = IfNoneMatch, IfUnmodifiedSince = IfUnmodifiedSince, Key = Key, Range = Range, ResponseCacheControl = ResponseCacheControl, ResponseContentDisposition = ResponseContentDisposition, ResponseContentEncoding = ResponseContentEncoding, ResponseContentLanguage = ResponseContentLanguage, ResponseContentType = ResponseContentType, ResponseExpires = ResponseExpires, VersionId = VersionId, SSECustomerAlgorithm = SSECustomerAlgorithm, SSECustomerKey = SSECustomerKey, SSECustomerKeyMD5 = SSECustomerKeyMD5, RequestPayer = RequestPayer, PartNumber = PartNumber, ExpectedBucketOwner = ExpectedBucketOwner, ChecksumMode = ChecksumMode)
  output <- .s3$get_object_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$get_object <- s3_get_object

#' This operation is not supported for directory buckets
#'
#' @description
#' This operation is not supported for directory buckets.
#' 
#' Returns the access control list (ACL) of an object. To use this
#' operation, you must have `s3:GetObjectAcl` permissions or `READ_ACP`
#' access to the object. For more information, see [Mapping of ACL
#' permissions and access policy
#' permissions](https://docs.aws.amazon.com/AmazonS3/latest/userguide/acl-overview.html#acl-access-policy-permission-mapping)
#' in the *Amazon S3 User Guide*
#' 
#' This functionality is not supported for Amazon S3 on Outposts.
#' 
#' By default, GET returns ACL information about the current version of an
#' object. To return ACL information about a different version, use the
#' versionId subresource.
#' 
#' If your bucket uses the bucket owner enforced setting for S3 Object
#' Ownership, requests to read ACLs are still supported and return the
#' `bucket-owner-full-control` ACL with the owner being the account that
#' created the bucket. For more information, see [Controlling object
#' ownership and disabling
#' ACLs](https://docs.aws.amazon.com/AmazonS3/latest/userguide/about-object-ownership.html)
#' in the *Amazon S3 User Guide*.
#' 
#' The following operations are related to
#' [`get_object_acl`][s3_get_object_acl]:
#' 
#' -   [`get_object`][s3_get_object]
#' 
#' -   [`get_object_attributes`][s3_get_object_attributes]
#' 
#' -   [`delete_object`][s3_delete_object]
#' 
#' -   [`put_object`][s3_put_object]
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#'
#' @usage
#' s3_get_object_acl(Bucket, Key, VersionId, RequestPayer,
#'   ExpectedBucketOwner)
#'
#' @param Bucket &#91;required&#93; The bucket name that contains the object for which to get the ACL
#' information.
#' 
#' **Access points** - When you use this action with an access point for
#' general purpose buckets, you must provide the alias of the access point
#' in place of the bucket name or specify the access point ARN. When you
#' use this action with an access point for directory buckets, you must
#' provide the access point name in place of the bucket name. When using
#' the access point ARN, you must direct requests to the access point
#' hostname. The access point hostname takes the form
#' *AccessPointName*-*AccountId*.s3-accesspoint.*Region*.amazonaws.com.
#' When using this action with an access point through the Amazon Web
#' Services SDKs, you provide the access point ARN in place of the bucket
#' name. For more information about access point ARNs, see [Using access
#' points](https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-access-points.html)
#' in the *Amazon S3 User Guide*.
#' @param Key &#91;required&#93; The key of the object for which to get the ACL information.
#' @param VersionId Version ID used to reference a specific version of the object.
#' 
#' This functionality is not supported for directory buckets.
#' @param RequestPayer 
#' @param ExpectedBucketOwner The account ID of the expected bucket owner. If the account ID that you
#' provide does not match the actual owner of the bucket, the request fails
#' with the HTTP status code `403 Forbidden` (access denied).
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   Owner = list(
#'     DisplayName = "string",
#'     ID = "string"
#'   ),
#'   Grants = list(
#'     list(
#'       Grantee = list(
#'         DisplayName = "string",
#'         EmailAddress = "string",
#'         ID = "string",
#'         Type = "CanonicalUser"|"AmazonCustomerByEmail"|"Group",
#'         URI = "string"
#'       ),
#'       Permission = "FULL_CONTROL"|"WRITE"|"WRITE_ACP"|"READ"|"READ_ACP"
#'     )
#'   ),
#'   RequestCharged = "requester"
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$get_object_acl(
#'   Bucket = "string",
#'   Key = "string",
#'   VersionId = "string",
#'   RequestPayer = "requester",
#'   ExpectedBucketOwner = "string"
#' )
#' ```
#'
#' @examples
#' \dontrun{
#' # The following example retrieves access control list (ACL) of an object.
#' svc$get_object_acl(
#'   Bucket = "examplebucket",
#'   Key = "HappyFace.jpg"
#' )
#' }
#'
#' @keywords internal
#'
#' @rdname s3_get_object_acl
#'
#' @aliases s3_get_object_acl
s3_get_object_acl <- function(Bucket, Key, VersionId = NULL, RequestPayer = NULL, ExpectedBucketOwner = NULL) {
  op <- new_operation(
    name = "GetObjectAcl",
    http_method = "GET",
    http_path = "/{Bucket}/{Key+}?acl",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .s3$get_object_acl_input(Bucket = Bucket, Key = Key, VersionId = VersionId, RequestPayer = RequestPayer, ExpectedBucketOwner = ExpectedBucketOwner)
  output <- .s3$get_object_acl_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$get_object_acl <- s3_get_object_acl

#' Retrieves all of the metadata from an object without returning the
#' object itself
#'
#' @description
#' Retrieves all of the metadata from an object without returning the
#' object itself. This operation is useful if you're interested only in an
#' object's metadata.
#' 
#' [`get_object_attributes`][s3_get_object_attributes] combines the
#' functionality of [`head_object`][s3_head_object] and
#' [`list_parts`][s3_list_parts]. All of the data returned with both of
#' those individual calls can be returned with a single call to
#' [`get_object_attributes`][s3_get_object_attributes].
#' 
#' **Directory buckets** - For directory buckets, you must make requests
#' for this API operation to the Zonal endpoint. These endpoints support
#' virtual-hosted-style requests in the format
#' `https://amzn-s3-demo-bucket.s3express-zone-id.region-code.amazonaws.com/key-name `.
#' Path-style requests are not supported. For more information about
#' endpoints in Availability Zones, see [Regional and Zonal endpoints for
#' directory buckets in Availability
#' Zones](https://docs.aws.amazon.com/AmazonS3/latest/userguide/endpoint-directory-buckets-AZ.html)
#' in the *Amazon S3 User Guide*. For more information about endpoints in
#' Local Zones, see [Concepts for directory buckets in Local
#' Zones](https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-lzs-for-directory-buckets.html)
#' in the *Amazon S3 User Guide*.
#' 
#' ### Permissions
#' 
#' -   **General purpose bucket permissions** - To use
#'     [`get_object_attributes`][s3_get_object_attributes], you must have
#'     READ access to the object.
#' 
#'     The other permissions that you need to use this operation depend on
#'     whether the bucket is versioned and if a version ID is passed in the
#'     [`get_object_attributes`][s3_get_object_attributes] request.
#' 
#'     -   If you pass a version ID in your request, you need both the
#'         `s3:GetObjectVersion` and `s3:GetObjectVersionAttributes`
#'         permissions.
#' 
#'     -   If you do not pass a version ID in your request, you need the
#'         `s3:GetObject` and `s3:GetObjectAttributes` permissions.
#' 
#'     For more information, see [Specifying Permissions in a
#'     Policy](https://docs.aws.amazon.com/AmazonS3/latest/userguide/security_iam_service-with-iam.html#security_iam_service-with-iam-id-based-policies-actions)
#'     in the *Amazon S3 User Guide*.
#' 
#'     If the object that you request does not exist, the error Amazon S3
#'     returns depends on whether you also have the `s3:ListBucket`
#'     permission.
#' 
#'     -   If you have the `s3:ListBucket` permission on the bucket, Amazon
#'         S3 returns an HTTP status code `404 Not Found` ("no such key")
#'         error.
#' 
#'     -   If you don't have the `s3:ListBucket` permission, Amazon S3
#'         returns an HTTP status code `403 Forbidden` ("access denied")
#'         error.
#' 
#' -   **Directory bucket permissions** - To grant access to this API
#'     operation on a directory bucket, we recommend that you use the
#'     [`create_session`](https://docs.aws.amazon.com/AmazonS3/latest/API/API_CreateSession.html)
#'     API operation for session-based authorization. Specifically, you
#'     grant the `s3express:CreateSession` permission to the directory
#'     bucket in a bucket policy or an IAM identity-based policy. Then, you
#'     make the [`create_session`][s3_create_session] API call on the
#'     bucket to obtain a session token. With the session token in your
#'     request header, you can make API requests to this operation. After
#'     the session token expires, you make another
#'     [`create_session`][s3_create_session] API call to generate a new
#'     session token for use. Amazon Web Services CLI or SDKs create
#'     session and refresh the session token automatically to avoid service
#'     interruptions when a session expires. For more information about
#'     authorization, see
#'     [`create_session`](https://docs.aws.amazon.com/AmazonS3/latest/API/API_CreateSession.html)
#'     .
#' 
#'     If the object is encrypted with SSE-KMS, you must also have the
#'     `kms:GenerateDataKey` and `kms:Decrypt` permissions in IAM
#'     identity-based policies and KMS key policies for the KMS key.
#' 
#' ### Encryption
#' 
#' Encryption request headers, like `x-amz-server-side-encryption`, should
#' not be sent for `HEAD` requests if your object uses server-side
#' encryption with Key Management Service (KMS) keys (SSE-KMS), dual-layer
#' server-side encryption with Amazon Web Services KMS keys (DSSE-KMS), or
#' server-side encryption with Amazon S3 managed encryption keys (SSE-S3).
#' The `x-amz-server-side-encryption` header is used when you `PUT` an
#' object to S3 and want to specify the encryption method. If you include
#' this header in a `GET` request for an object that uses these types of
#' keys, you’ll get an HTTP `400 Bad Request` error. It's because the
#' encryption method can't be changed when you retrieve the object.
#' 
#' If you encrypted an object when you stored the object in Amazon S3 by
#' using server-side encryption with customer-provided encryption keys
#' (SSE-C), then when you retrieve the metadata from the object, you must
#' use the following headers. These headers provide the server with the
#' encryption key required to retrieve the object's metadata. The headers
#' are:
#' 
#' -   `x-amz-server-side-encryption-customer-algorithm`
#' 
#' -   `x-amz-server-side-encryption-customer-key`
#' 
#' -   `x-amz-server-side-encryption-customer-key-MD5`
#' 
#' For more information about SSE-C, see [Server-Side Encryption (Using
#' Customer-Provided Encryption
#' Keys)](https://docs.aws.amazon.com/AmazonS3/latest/userguide/ServerSideEncryptionCustomerKeys.html)
#' in the *Amazon S3 User Guide*.
#' 
#' **Directory bucket permissions** - For directory buckets, there are only
#' two supported options for server-side encryption: server-side encryption
#' with Amazon S3 managed keys (SSE-S3) (`AES256`) and server-side
#' encryption with KMS keys (SSE-KMS) (`aws:kms`). We recommend that the
#' bucket's default encryption uses the desired encryption configuration
#' and you don't override the bucket default encryption in your
#' [`create_session`][s3_create_session] requests or `PUT` object requests.
#' Then, new objects are automatically encrypted with the desired
#' encryption settings. For more information, see [Protecting data with
#' server-side
#' encryption](https://docs.aws.amazon.com/AmazonS3/latest/userguide/) in
#' the *Amazon S3 User Guide*. For more information about the encryption
#' overriding behaviors in directory buckets, see [Specifying server-side
#' encryption with KMS for new object
#' uploads](https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-express-specifying-kms-encryption.html).
#' 
#' ### Versioning
#' 
#' **Directory buckets** - S3 Versioning isn't enabled and supported for
#' directory buckets. For this API operation, only the `null` value of the
#' version ID is supported by directory buckets. You can only specify
#' `null` to the `versionId` query parameter in the request.
#' 
#' ### Conditional request headers
#' 
#' Consider the following when using request headers:
#' 
#' -   If both of the `If-Match` and `If-Unmodified-Since` headers are
#'     present in the request as follows, then Amazon S3 returns the HTTP
#'     status code `200 OK` and the data requested:
#' 
#'     -   `If-Match` condition evaluates to `true`.
#' 
#'     -   `If-Unmodified-Since` condition evaluates to `false`.
#' 
#'     For more information about conditional requests, see [RFC
#'     7232](https://datatracker.ietf.org/doc/html/rfc7232).
#' 
#' -   If both of the `If-None-Match` and `If-Modified-Since` headers are
#'     present in the request as follows, then Amazon S3 returns the HTTP
#'     status code `304 Not Modified`:
#' 
#'     -   `If-None-Match` condition evaluates to `false`.
#' 
#'     -   `If-Modified-Since` condition evaluates to `true`.
#' 
#'     For more information about conditional requests, see [RFC
#'     7232](https://datatracker.ietf.org/doc/html/rfc7232).
#' 
#' ### HTTP Host header syntax
#' 
#' **Directory buckets** - The HTTP Host header syntax is
#' ` Bucket-name.s3express-zone-id.region-code.amazonaws.com`.
#' 
#' The following actions are related to
#' [`get_object_attributes`][s3_get_object_attributes]:
#' 
#' -   [`get_object`][s3_get_object]
#' 
#' -   [`get_object_acl`][s3_get_object_acl]
#' 
#' -   [`get_object_legal_hold`][s3_get_object_legal_hold]
#' 
#' -   [`get_object_lock_configuration`][s3_get_object_lock_configuration]
#' 
#' -   [`get_object_retention`][s3_get_object_retention]
#' 
#' -   [`get_object_tagging`][s3_get_object_tagging]
#' 
#' -   [`head_object`][s3_head_object]
#' 
#' -   [`list_parts`][s3_list_parts]
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#'
#' @usage
#' s3_get_object_attributes(Bucket, Key, VersionId, MaxParts,
#'   PartNumberMarker, SSECustomerAlgorithm, SSECustomerKey,
#'   SSECustomerKeyMD5, RequestPayer, ExpectedBucketOwner, ObjectAttributes)
#'
#' @param Bucket &#91;required&#93; The name of the bucket that contains the object.
#' 
#' **Directory buckets** - When you use this operation with a directory
#' bucket, you must use virtual-hosted-style requests in the format
#' ` Bucket-name.s3express-zone-id.region-code.amazonaws.com`. Path-style
#' requests are not supported. Directory bucket names must be unique in the
#' chosen Zone (Availability Zone or Local Zone). Bucket names must follow
#' the format ` bucket-base-name--zone-id--x-s3` (for example,
#' ` amzn-s3-demo-bucket--usw2-az1--x-s3`). For information about bucket
#' naming restrictions, see [Directory bucket naming
#' rules](https://docs.aws.amazon.com/AmazonS3/latest/userguide/directory-bucket-naming-rules.html)
#' in the *Amazon S3 User Guide*.
#' 
#' **Access points** - When you use this action with an access point for
#' general purpose buckets, you must provide the alias of the access point
#' in place of the bucket name or specify the access point ARN. When you
#' use this action with an access point for directory buckets, you must
#' provide the access point name in place of the bucket name. When using
#' the access point ARN, you must direct requests to the access point
#' hostname. The access point hostname takes the form
#' *AccessPointName*-*AccountId*.s3-accesspoint.*Region*.amazonaws.com.
#' When using this action with an access point through the Amazon Web
#' Services SDKs, you provide the access point ARN in place of the bucket
#' name. For more information about access point ARNs, see [Using access
#' points](https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-access-points.html)
#' in the *Amazon S3 User Guide*.
#' 
#' Object Lambda access points are not supported by directory buckets.
#' 
#' **S3 on Outposts** - When you use this action with S3 on Outposts, you
#' must direct requests to the S3 on Outposts hostname. The S3 on Outposts
#' hostname takes the form
#' ` AccessPointName-AccountId.outpostID.s3-outposts.Region.amazonaws.com`.
#' When you use this action with S3 on Outposts, the destination bucket
#' must be the Outposts access point ARN or the access point alias. For
#' more information about S3 on Outposts, see [What is S3 on
#' Outposts?](https://docs.aws.amazon.com/AmazonS3/latest/s3-outposts/S3onOutposts.html)
#' in the *Amazon S3 User Guide*.
#' @param Key &#91;required&#93; The object key.
#' @param VersionId The version ID used to reference a specific version of the object.
#' 
#' S3 Versioning isn't enabled and supported for directory buckets. For
#' this API operation, only the `null` value of the version ID is supported
#' by directory buckets. You can only specify `null` to the `versionId`
#' query parameter in the request.
#' @param MaxParts Sets the maximum number of parts to return. For more information, see
#' [Uploading and copying objects using multipart upload in Amazon
#' S3](https://docs.aws.amazon.com/AmazonS3/latest/userguide/mpuoverview.html)
#' in the *Amazon Simple Storage Service user guide*.
#' @param PartNumberMarker Specifies the part after which listing should begin. Only parts with
#' higher part numbers will be listed. For more information, see [Uploading
#' and copying objects using multipart upload in Amazon
#' S3](https://docs.aws.amazon.com/AmazonS3/latest/userguide/mpuoverview.html)
#' in the *Amazon Simple Storage Service user guide*.
#' @param SSECustomerAlgorithm Specifies the algorithm to use when encrypting the object (for example,
#' AES256).
#' 
#' This functionality is not supported for directory buckets.
#' @param SSECustomerKey Specifies the customer-provided encryption key for Amazon S3 to use in
#' encrypting data. This value is used to store the object and then it is
#' discarded; Amazon S3 does not store the encryption key. The key must be
#' appropriate for use with the algorithm specified in the
#' `x-amz-server-side-encryption-customer-algorithm` header.
#' 
#' This functionality is not supported for directory buckets.
#' @param SSECustomerKeyMD5 Specifies the 128-bit MD5 digest of the encryption key according to RFC
#' 1321. Amazon S3 uses this header for a message integrity check to ensure
#' that the encryption key was transmitted without error.
#' 
#' This functionality is not supported for directory buckets.
#' @param RequestPayer 
#' @param ExpectedBucketOwner The account ID of the expected bucket owner. If the account ID that you
#' provide does not match the actual owner of the bucket, the request fails
#' with the HTTP status code `403 Forbidden` (access denied).
#' @param ObjectAttributes &#91;required&#93; Specifies the fields at the root level that you want returned in the
#' response. Fields that you do not specify are not returned.
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   DeleteMarker = TRUE|FALSE,
#'   LastModified = as.POSIXct(
#'     "2015-01-01"
#'   ),
#'   VersionId = "string",
#'   RequestCharged = "requester",
#'   ETag = "string",
#'   Checksum = list(
#'     ChecksumCRC32 = "string",
#'     ChecksumCRC32C = "string",
#'     ChecksumCRC64NVME = "string",
#'     ChecksumSHA1 = "string",
#'     ChecksumSHA256 = "string",
#'     ChecksumType = "COMPOSITE"|"FULL_OBJECT"
#'   ),
#'   ObjectParts = list(
#'     TotalPartsCount = 123,
#'     PartNumberMarker = 123,
#'     NextPartNumberMarker = 123,
#'     MaxParts = 123,
#'     IsTruncated = TRUE|FALSE,
#'     Parts = list(
#'       list(
#'         PartNumber = 123,
#'         Size = 123,
#'         ChecksumCRC32 = "string",
#'         ChecksumCRC32C = "string",
#'         ChecksumCRC64NVME = "string",
#'         ChecksumSHA1 = "string",
#'         ChecksumSHA256 = "string"
#'       )
#'     )
#'   ),
#'   StorageClass = "STANDARD"|"REDUCED_REDUNDANCY"|"STANDARD_IA"|"ONEZONE_IA"|"INTELLIGENT_TIERING"|"GLACIER"|"DEEP_ARCHIVE"|"OUTPOSTS"|"GLACIER_IR"|"SNOW"|"EXPRESS_ONEZONE"|"FSX_OPENZFS"|"FSX_ONTAP",
#'   ObjectSize = 123
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$get_object_attributes(
#'   Bucket = "string",
#'   Key = "string",
#'   VersionId = "string",
#'   MaxParts = 123,
#'   PartNumberMarker = 123,
#'   SSECustomerAlgorithm = "string",
#'   SSECustomerKey = "string",
#'   SSECustomerKeyMD5 = "string",
#'   RequestPayer = "requester",
#'   ExpectedBucketOwner = "string",
#'   ObjectAttributes = list(
#'     "ETag"|"Checksum"|"ObjectParts"|"StorageClass"|"ObjectSize"
#'   )
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname s3_get_object_attributes
#'
#' @aliases s3_get_object_attributes
s3_get_object_attributes <- function(Bucket, Key, VersionId = NULL, MaxParts = NULL, PartNumberMarker = NULL, SSECustomerAlgorithm = NULL, SSECustomerKey = NULL, SSECustomerKeyMD5 = NULL, RequestPayer = NULL, ExpectedBucketOwner = NULL, ObjectAttributes) {
  op <- new_operation(
    name = "GetObjectAttributes",
    http_method = "GET",
    http_path = "/{Bucket}/{Key+}?attributes",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .s3$get_object_attributes_input(Bucket = Bucket, Key = Key, VersionId = VersionId, MaxParts = MaxParts, PartNumberMarker = PartNumberMarker, SSECustomerAlgorithm = SSECustomerAlgorithm, SSECustomerKey = SSECustomerKey, SSECustomerKeyMD5 = SSECustomerKeyMD5, RequestPayer = RequestPayer, ExpectedBucketOwner = ExpectedBucketOwner, ObjectAttributes = ObjectAttributes)
  output <- .s3$get_object_attributes_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$get_object_attributes <- s3_get_object_attributes

#' This operation is not supported for directory buckets
#'
#' @description
#' This operation is not supported for directory buckets.
#' 
#' Gets an object's current legal hold status. For more information, see
#' [Locking
#' Objects](https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock.html).
#' 
#' This functionality is not supported for Amazon S3 on Outposts.
#' 
#' The following action is related to
#' [`get_object_legal_hold`][s3_get_object_legal_hold]:
#' 
#' -   [`get_object_attributes`][s3_get_object_attributes]
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#'
#' @usage
#' s3_get_object_legal_hold(Bucket, Key, VersionId, RequestPayer,
#'   ExpectedBucketOwner)
#'
#' @param Bucket &#91;required&#93; The bucket name containing the object whose legal hold status you want
#' to retrieve.
#' 
#' **Access points** - When you use this action with an access point for
#' general purpose buckets, you must provide the alias of the access point
#' in place of the bucket name or specify the access point ARN. When you
#' use this action with an access point for directory buckets, you must
#' provide the access point name in place of the bucket name. When using
#' the access point ARN, you must direct requests to the access point
#' hostname. The access point hostname takes the form
#' *AccessPointName*-*AccountId*.s3-accesspoint.*Region*.amazonaws.com.
#' When using this action with an access point through the Amazon Web
#' Services SDKs, you provide the access point ARN in place of the bucket
#' name. For more information about access point ARNs, see [Using access
#' points](https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-access-points.html)
#' in the *Amazon S3 User Guide*.
#' @param Key &#91;required&#93; The key name for the object whose legal hold status you want to
#' retrieve.
#' @param VersionId The version ID of the object whose legal hold status you want to
#' retrieve.
#' @param RequestPayer 
#' @param ExpectedBucketOwner The account ID of the expected bucket owner. If the account ID that you
#' provide does not match the actual owner of the bucket, the request fails
#' with the HTTP status code `403 Forbidden` (access denied).
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   LegalHold = list(
#'     Status = "ON"|"OFF"
#'   )
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$get_object_legal_hold(
#'   Bucket = "string",
#'   Key = "string",
#'   VersionId = "string",
#'   RequestPayer = "requester",
#'   ExpectedBucketOwner = "string"
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname s3_get_object_legal_hold
#'
#' @aliases s3_get_object_legal_hold
s3_get_object_legal_hold <- function(Bucket, Key, VersionId = NULL, RequestPayer = NULL, ExpectedBucketOwner = NULL) {
  op <- new_operation(
    name = "GetObjectLegalHold",
    http_method = "GET",
    http_path = "/{Bucket}/{Key+}?legal-hold",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .s3$get_object_legal_hold_input(Bucket = Bucket, Key = Key, VersionId = VersionId, RequestPayer = RequestPayer, ExpectedBucketOwner = ExpectedBucketOwner)
  output <- .s3$get_object_legal_hold_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$get_object_legal_hold <- s3_get_object_legal_hold

#' This operation is not supported for directory buckets
#'
#' @description
#' This operation is not supported for directory buckets.
#' 
#' Gets the Object Lock configuration for a bucket. The rule specified in
#' the Object Lock configuration will be applied by default to every new
#' object placed in the specified bucket. For more information, see
#' [Locking
#' Objects](https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock.html).
#' 
#' The following action is related to
#' [`get_object_lock_configuration`][s3_get_object_lock_configuration]:
#' 
#' -   [`get_object_attributes`][s3_get_object_attributes]
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#'
#' @usage
#' s3_get_object_lock_configuration(Bucket, ExpectedBucketOwner)
#'
#' @param Bucket &#91;required&#93; The bucket whose Object Lock configuration you want to retrieve.
#' 
#' **Access points** - When you use this action with an access point for
#' general purpose buckets, you must provide the alias of the access point
#' in place of the bucket name or specify the access point ARN. When you
#' use this action with an access point for directory buckets, you must
#' provide the access point name in place of the bucket name. When using
#' the access point ARN, you must direct requests to the access point
#' hostname. The access point hostname takes the form
#' *AccessPointName*-*AccountId*.s3-accesspoint.*Region*.amazonaws.com.
#' When using this action with an access point through the Amazon Web
#' Services SDKs, you provide the access point ARN in place of the bucket
#' name. For more information about access point ARNs, see [Using access
#' points](https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-access-points.html)
#' in the *Amazon S3 User Guide*.
#' @param ExpectedBucketOwner The account ID of the expected bucket owner. If the account ID that you
#' provide does not match the actual owner of the bucket, the request fails
#' with the HTTP status code `403 Forbidden` (access denied).
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   ObjectLockConfiguration = list(
#'     ObjectLockEnabled = "Enabled",
#'     Rule = list(
#'       DefaultRetention = list(
#'         Mode = "GOVERNANCE"|"COMPLIANCE",
#'         Days = 123,
#'         Years = 123
#'       )
#'     )
#'   )
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$get_object_lock_configuration(
#'   Bucket = "string",
#'   ExpectedBucketOwner = "string"
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname s3_get_object_lock_configuration
#'
#' @aliases s3_get_object_lock_configuration
s3_get_object_lock_configuration <- function(Bucket, ExpectedBucketOwner = NULL) {
  op <- new_operation(
    name = "GetObjectLockConfiguration",
    http_method = "GET",
    http_path = "/{Bucket}?object-lock",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .s3$get_object_lock_configuration_input(Bucket = Bucket, ExpectedBucketOwner = ExpectedBucketOwner)
  output <- .s3$get_object_lock_configuration_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$get_object_lock_configuration <- s3_get_object_lock_configuration

#' This operation is not supported for directory buckets
#'
#' @description
#' This operation is not supported for directory buckets.
#' 
#' Retrieves an object's retention settings. For more information, see
#' [Locking
#' Objects](https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock.html).
#' 
#' This functionality is not supported for Amazon S3 on Outposts.
#' 
#' The following action is related to
#' [`get_object_retention`][s3_get_object_retention]:
#' 
#' -   [`get_object_attributes`][s3_get_object_attributes]
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#'
#' @usage
#' s3_get_object_retention(Bucket, Key, VersionId, RequestPayer,
#'   ExpectedBucketOwner)
#'
#' @param Bucket &#91;required&#93; The bucket name containing the object whose retention settings you want
#' to retrieve.
#' 
#' **Access points** - When you use this action with an access point for
#' general purpose buckets, you must provide the alias of the access point
#' in place of the bucket name or specify the access point ARN. When you
#' use this action with an access point for directory buckets, you must
#' provide the access point name in place of the bucket name. When using
#' the access point ARN, you must direct requests to the access point
#' hostname. The access point hostname takes the form
#' *AccessPointName*-*AccountId*.s3-accesspoint.*Region*.amazonaws.com.
#' When using this action with an access point through the Amazon Web
#' Services SDKs, you provide the access point ARN in place of the bucket
#' name. For more information about access point ARNs, see [Using access
#' points](https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-access-points.html)
#' in the *Amazon S3 User Guide*.
#' @param Key &#91;required&#93; The key name for the object whose retention settings you want to
#' retrieve.
#' @param VersionId The version ID for the object whose retention settings you want to
#' retrieve.
#' @param RequestPayer 
#' @param ExpectedBucketOwner The account ID of the expected bucket owner. If the account ID that you
#' provide does not match the actual owner of the bucket, the request fails
#' with the HTTP status code `403 Forbidden` (access denied).
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   Retention = list(
#'     Mode = "GOVERNANCE"|"COMPLIANCE",
#'     RetainUntilDate = as.POSIXct(
#'       "2015-01-01"
#'     )
#'   )
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$get_object_retention(
#'   Bucket = "string",
#'   Key = "string",
#'   VersionId = "string",
#'   RequestPayer = "requester",
#'   ExpectedBucketOwner = "string"
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname s3_get_object_retention
#'
#' @aliases s3_get_object_retention
s3_get_object_retention <- function(Bucket, Key, VersionId = NULL, RequestPayer = NULL, ExpectedBucketOwner = NULL) {
  op <- new_operation(
    name = "GetObjectRetention",
    http_method = "GET",
    http_path = "/{Bucket}/{Key+}?retention",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .s3$get_object_retention_input(Bucket = Bucket, Key = Key, VersionId = VersionId, RequestPayer = RequestPayer, ExpectedBucketOwner = ExpectedBucketOwner)
  output <- .s3$get_object_retention_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$get_object_retention <- s3_get_object_retention

#' This operation is not supported for directory buckets
#'
#' @description
#' This operation is not supported for directory buckets.
#' 
#' Returns the tag-set of an object. You send the GET request against the
#' tagging subresource associated with the object.
#' 
#' To use this operation, you must have permission to perform the
#' `s3:GetObjectTagging` action. By default, the GET action returns
#' information about current version of an object. For a versioned bucket,
#' you can have multiple versions of an object in your bucket. To retrieve
#' tags of any other version, use the versionId query parameter. You also
#' need permission for the `s3:GetObjectVersionTagging` action.
#' 
#' By default, the bucket owner has this permission and can grant this
#' permission to others.
#' 
#' For information about the Amazon S3 object tagging feature, see [Object
#' Tagging](https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-tagging.html).
#' 
#' The following actions are related to
#' [`get_object_tagging`][s3_get_object_tagging]:
#' 
#' -   [`delete_object_tagging`][s3_delete_object_tagging]
#' 
#' -   [`get_object_attributes`][s3_get_object_attributes]
#' 
#' -   [`put_object_tagging`][s3_put_object_tagging]
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#'
#' @usage
#' s3_get_object_tagging(Bucket, Key, VersionId, ExpectedBucketOwner,
#'   RequestPayer)
#'
#' @param Bucket &#91;required&#93; The bucket name containing the object for which to get the tagging
#' information.
#' 
#' **Access points** - When you use this action with an access point for
#' general purpose buckets, you must provide the alias of the access point
#' in place of the bucket name or specify the access point ARN. When you
#' use this action with an access point for directory buckets, you must
#' provide the access point name in place of the bucket name. When using
#' the access point ARN, you must direct requests to the access point
#' hostname. The access point hostname takes the form
#' *AccessPointName*-*AccountId*.s3-accesspoint.*Region*.amazonaws.com.
#' When using this action with an access point through the Amazon Web
#' Services SDKs, you provide the access point ARN in place of the bucket
#' name. For more information about access point ARNs, see [Using access
#' points](https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-access-points.html)
#' in the *Amazon S3 User Guide*.
#' 
#' **S3 on Outposts** - When you use this action with S3 on Outposts, you
#' must direct requests to the S3 on Outposts hostname. The S3 on Outposts
#' hostname takes the form
#' ` AccessPointName-AccountId.outpostID.s3-outposts.Region.amazonaws.com`.
#' When you use this action with S3 on Outposts, the destination bucket
#' must be the Outposts access point ARN or the access point alias. For
#' more information about S3 on Outposts, see [What is S3 on
#' Outposts?](https://docs.aws.amazon.com/AmazonS3/latest/s3-outposts/S3onOutposts.html)
#' in the *Amazon S3 User Guide*.
#' @param Key &#91;required&#93; Object key for which to get the tagging information.
#' @param VersionId The versionId of the object for which to get the tagging information.
#' @param ExpectedBucketOwner The account ID of the expected bucket owner. If the account ID that you
#' provide does not match the actual owner of the bucket, the request fails
#' with the HTTP status code `403 Forbidden` (access denied).
#' @param RequestPayer 
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   VersionId = "string",
#'   TagSet = list(
#'     list(
#'       Key = "string",
#'       Value = "string"
#'     )
#'   )
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$get_object_tagging(
#'   Bucket = "string",
#'   Key = "string",
#'   VersionId = "string",
#'   ExpectedBucketOwner = "string",
#'   RequestPayer = "requester"
#' )
#' ```
#'
#' @examples
#' \dontrun{
#' # The following example retrieves tag set of an object.
#' svc$get_object_tagging(
#'   Bucket = "examplebucket",
#'   Key = "HappyFace.jpg"
#' )
#' 
#' # The following example retrieves tag set of an object. The request
#' # specifies object version.
#' svc$get_object_tagging(
#'   Bucket = "examplebucket",
#'   Key = "exampleobject",
#'   VersionId = "ydlaNkwWm0SfKJR.T1b1fIdPRbldTYRI"
#' )
#' }
#'
#' @keywords internal
#'
#' @rdname s3_get_object_tagging
#'
#' @aliases s3_get_object_tagging
s3_get_object_tagging <- function(Bucket, Key, VersionId = NULL, ExpectedBucketOwner = NULL, RequestPayer = NULL) {
  op <- new_operation(
    name = "GetObjectTagging",
    http_method = "GET",
    http_path = "/{Bucket}/{Key+}?tagging",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .s3$get_object_tagging_input(Bucket = Bucket, Key = Key, VersionId = VersionId, ExpectedBucketOwner = ExpectedBucketOwner, RequestPayer = RequestPayer)
  output <- .s3$get_object_tagging_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$get_object_tagging <- s3_get_object_tagging

#' This operation is not supported for directory buckets
#'
#' @description
#' This operation is not supported for directory buckets.
#' 
#' Returns torrent files from a bucket. BitTorrent can save you bandwidth
#' when you're distributing large files.
#' 
#' You can get torrent only for objects that are less than 5 GB in size,
#' and that are not encrypted using server-side encryption with a
#' customer-provided encryption key.
#' 
#' To use GET, you must have READ access to the object.
#' 
#' This functionality is not supported for Amazon S3 on Outposts.
#' 
#' The following action is related to
#' [`get_object_torrent`][s3_get_object_torrent]:
#' 
#' -   [`get_object`][s3_get_object]
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#'
#' @usage
#' s3_get_object_torrent(Bucket, Key, RequestPayer, ExpectedBucketOwner)
#'
#' @param Bucket &#91;required&#93; The name of the bucket containing the object for which to get the
#' torrent files.
#' @param Key &#91;required&#93; The object key for which to get the information.
#' @param RequestPayer 
#' @param ExpectedBucketOwner The account ID of the expected bucket owner. If the account ID that you
#' provide does not match the actual owner of the bucket, the request fails
#' with the HTTP status code `403 Forbidden` (access denied).
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   Body = raw,
#'   RequestCharged = "requester"
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$get_object_torrent(
#'   Bucket = "string",
#'   Key = "string",
#'   RequestPayer = "requester",
#'   ExpectedBucketOwner = "string"
#' )
#' ```
#'
#' @examples
#' \dontrun{
#' # The following example retrieves torrent files of an object.
#' svc$get_object_torrent(
#'   Bucket = "examplebucket",
#'   Key = "HappyFace.jpg"
#' )
#' }
#'
#' @keywords internal
#'
#' @rdname s3_get_object_torrent
#'
#' @aliases s3_get_object_torrent
s3_get_object_torrent <- function(Bucket, Key, RequestPayer = NULL, ExpectedBucketOwner = NULL) {
  op <- new_operation(
    name = "GetObjectTorrent",
    http_method = "GET",
    http_path = "/{Bucket}/{Key+}?torrent",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .s3$get_object_torrent_input(Bucket = Bucket, Key = Key, RequestPayer = RequestPayer, ExpectedBucketOwner = ExpectedBucketOwner)
  output <- .s3$get_object_torrent_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$get_object_torrent <- s3_get_object_torrent

#' This operation is not supported for directory buckets
#'
#' @description
#' This operation is not supported for directory buckets.
#' 
#' Retrieves the `PublicAccessBlock` configuration for an Amazon S3 bucket.
#' This operation returns the bucket-level configuration only. To
#' understand the effective public access behavior, you must also consider
#' account-level settings (which may inherit from organization-level
#' policies). To use this operation, you must have the
#' `s3:GetBucketPublicAccessBlock` permission. For more information about
#' Amazon S3 permissions, see [Specifying Permissions in a
#' Policy](https://docs.aws.amazon.com/AmazonS3/latest/userguide/security_iam_service-with-iam.html#security_iam_service-with-iam-id-based-policies-actions).
#' 
#' When Amazon S3 evaluates the `PublicAccessBlock` configuration for a
#' bucket or an object, it checks the `PublicAccessBlock` configuration for
#' both the bucket (or the bucket that contains the object) and the bucket
#' owner's account. Account-level settings automatically inherit from
#' organization-level policies when present. If the `PublicAccessBlock`
#' settings are different between the bucket and the account, Amazon S3
#' uses the most restrictive combination of the bucket-level and
#' account-level settings.
#' 
#' For more information about when Amazon S3 considers a bucket or an
#' object public, see [The Meaning of
#' "Public"](https://docs.aws.amazon.com/AmazonS3/latest/userguide/access-control-block-public-access.html#access-control-block-public-access-policy-status).
#' 
#' The following operations are related to
#' [`get_public_access_block`][s3_get_public_access_block]:
#' 
#' -   [Using Amazon S3 Block Public
#'     Access](https://docs.aws.amazon.com/AmazonS3/latest/userguide/access-control-block-public-access.html)
#' 
#' -   [`put_public_access_block`][s3_put_public_access_block]
#' 
#' -   [`get_public_access_block`][s3_get_public_access_block]
#' 
#' -   [`delete_public_access_block`][s3_delete_public_access_block]
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#'
#' @usage
#' s3_get_public_access_block(Bucket, ExpectedBucketOwner)
#'
#' @param Bucket &#91;required&#93; The name of the Amazon S3 bucket whose `PublicAccessBlock` configuration
#' you want to retrieve.
#' @param ExpectedBucketOwner The account ID of the expected bucket owner. If the account ID that you
#' provide does not match the actual owner of the bucket, the request fails
#' with the HTTP status code `403 Forbidden` (access denied).
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   PublicAccessBlockConfiguration = list(
#'     BlockPublicAcls = TRUE|FALSE,
#'     IgnorePublicAcls = TRUE|FALSE,
#'     BlockPublicPolicy = TRUE|FALSE,
#'     RestrictPublicBuckets = TRUE|FALSE
#'   )
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$get_public_access_block(
#'   Bucket = "string",
#'   ExpectedBucketOwner = "string"
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname s3_get_public_access_block
#'
#' @aliases s3_get_public_access_block
s3_get_public_access_block <- function(Bucket, ExpectedBucketOwner = NULL) {
  op <- new_operation(
    name = "GetPublicAccessBlock",
    http_method = "GET",
    http_path = "/{Bucket}?publicAccessBlock",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .s3$get_public_access_block_input(Bucket = Bucket, ExpectedBucketOwner = ExpectedBucketOwner)
  output <- .s3$get_public_access_block_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$get_public_access_block <- s3_get_public_access_block

#' You can use this operation to determine if a bucket exists and if you
#' have permission to access it
#'
#' @description
#' You can use this operation to determine if a bucket exists and if you
#' have permission to access it. The action returns a `200 OK` HTTP status
#' code if the bucket exists and you have permission to access it. You can
#' make a [`head_bucket`][s3_head_bucket] call on any bucket name to any
#' Region in the partition, and regardless of the permissions on the
#' bucket, you will receive a response header with the correct bucket
#' location so that you can then make a proper, signed request to the
#' appropriate Regional endpoint.
#' 
#' If the bucket doesn't exist or you don't have permission to access it,
#' the `HEAD` request returns a generic `400 Bad Request`, `403 Forbidden`,
#' or `404 Not Found` HTTP status code. A message body isn't included, so
#' you can't determine the exception beyond these HTTP response codes.
#' 
#' ### Authentication and authorization
#' 
#' **General purpose buckets** - Request to public buckets that grant the
#' s3:ListBucket permission publicly do not need to be signed. All other
#' [`head_bucket`][s3_head_bucket] requests must be authenticated and
#' signed by using IAM credentials (access key ID and secret access key for
#' the IAM identities). All headers with the `x-amz-` prefix, including
#' `x-amz-copy-source`, must be signed. For more information, see [REST
#' Authentication](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTAuthentication.html).
#' 
#' **Directory buckets** - You must use IAM credentials to authenticate and
#' authorize your access to the [`head_bucket`][s3_head_bucket] API
#' operation, instead of using the temporary security credentials through
#' the [`create_session`][s3_create_session] API operation.
#' 
#' Amazon Web Services CLI or SDKs handles authentication and authorization
#' on your behalf.
#' 
#' ### Permissions
#' 
#' -   **General purpose bucket permissions** - To use this operation, you
#'     must have permissions to perform the `s3:ListBucket` action. The
#'     bucket owner has this permission by default and can grant this
#'     permission to others. For more information about permissions, see
#'     [Managing access permissions to your Amazon S3
#'     resources](https://docs.aws.amazon.com/AmazonS3/latest/userguide/security-iam.html)
#'     in the *Amazon S3 User Guide*.
#' 
#' -   **Directory bucket permissions** - You must have the
#'     **`s3express:CreateSession`** permission in the `Action` element of
#'     a policy. By default, the session is in the `ReadWrite` mode. If you
#'     want to restrict the access, you can explicitly set the
#'     `s3express:SessionMode` condition key to `ReadOnly` on the bucket.
#' 
#'     For more information about example bucket policies, see [Example
#'     bucket policies for S3 Express One
#'     Zone](https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-express-security-iam-example-bucket-policies.html)
#'     and [Amazon Web Services Identity and Access Management (IAM)
#'     identity-based policies for S3 Express One
#'     Zone](https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-express-security-iam-identity-policies.html)
#'     in the *Amazon S3 User Guide*.
#' 
#' ### HTTP Host header syntax
#' 
#' **Directory buckets** - The HTTP Host header syntax is
#' ` Bucket-name.s3express-zone-id.region-code.amazonaws.com`.
#' 
#' You must make requests for this API operation to the Zonal endpoint.
#' These endpoints support virtual-hosted-style requests in the format
#' `https://bucket-name.s3express-zone-id.region-code.amazonaws.com`.
#' Path-style requests are not supported. For more information about
#' endpoints in Availability Zones, see [Regional and Zonal endpoints for
#' directory buckets in Availability
#' Zones](https://docs.aws.amazon.com/AmazonS3/latest/userguide/endpoint-directory-buckets-AZ.html)
#' in the *Amazon S3 User Guide*. For more information about endpoints in
#' Local Zones, see [Concepts for directory buckets in Local
#' Zones](https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-lzs-for-directory-buckets.html)
#' in the *Amazon S3 User Guide*.
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#'
#' @usage
#' s3_head_bucket(Bucket, ExpectedBucketOwner)
#'
#' @param Bucket &#91;required&#93; The bucket name.
#' 
#' **Directory buckets** - When you use this operation with a directory
#' bucket, you must use virtual-hosted-style requests in the format
#' ` Bucket-name.s3express-zone-id.region-code.amazonaws.com`. Path-style
#' requests are not supported. Directory bucket names must be unique in the
#' chosen Zone (Availability Zone or Local Zone). Bucket names must follow
#' the format ` bucket-base-name--zone-id--x-s3` (for example,
#' ` amzn-s3-demo-bucket--usw2-az1--x-s3`). For information about bucket
#' naming restrictions, see [Directory bucket naming
#' rules](https://docs.aws.amazon.com/AmazonS3/latest/userguide/directory-bucket-naming-rules.html)
#' in the *Amazon S3 User Guide*.
#' 
#' **Access points** - When you use this action with an access point for
#' general purpose buckets, you must provide the alias of the access point
#' in place of the bucket name or specify the access point ARN. When you
#' use this action with an access point for directory buckets, you must
#' provide the access point name in place of the bucket name. When using
#' the access point ARN, you must direct requests to the access point
#' hostname. The access point hostname takes the form
#' *AccessPointName*-*AccountId*.s3-accesspoint.*Region*.amazonaws.com.
#' When using this action with an access point through the Amazon Web
#' Services SDKs, you provide the access point ARN in place of the bucket
#' name. For more information about access point ARNs, see [Using access
#' points](https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-access-points.html)
#' in the *Amazon S3 User Guide*.
#' 
#' **Object Lambda access points** - When you use this API operation with
#' an Object Lambda access point, provide the alias of the Object Lambda
#' access point in place of the bucket name. If the Object Lambda access
#' point alias in a request is not valid, the error code
#' `InvalidAccessPointAliasError` is returned. For more information about
#' `InvalidAccessPointAliasError`, see [List of Error
#' Codes](https://docs.aws.amazon.com/AmazonS3/latest/API/ErrorResponses.html#ErrorCodeList).
#' 
#' Object Lambda access points are not supported by directory buckets.
#' 
#' **S3 on Outposts** - When you use this action with S3 on Outposts, you
#' must direct requests to the S3 on Outposts hostname. The S3 on Outposts
#' hostname takes the form
#' ` AccessPointName-AccountId.outpostID.s3-outposts.Region.amazonaws.com`.
#' When you use this action with S3 on Outposts, the destination bucket
#' must be the Outposts access point ARN or the access point alias. For
#' more information about S3 on Outposts, see [What is S3 on
#' Outposts?](https://docs.aws.amazon.com/AmazonS3/latest/s3-outposts/S3onOutposts.html)
#' in the *Amazon S3 User Guide*.
#' @param ExpectedBucketOwner The account ID of the expected bucket owner. If the account ID that you
#' provide does not match the actual owner of the bucket, the request fails
#' with the HTTP status code `403 Forbidden` (access denied).
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   BucketArn = "string",
#'   BucketLocationType = "AvailabilityZone"|"LocalZone",
#'   BucketLocationName = "string",
#'   BucketRegion = "string",
#'   AccessPointAlias = TRUE|FALSE
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$head_bucket(
#'   Bucket = "string",
#'   ExpectedBucketOwner = "string"
#' )
#' ```
#'
#' @examples
#' \dontrun{
#' # This operation checks to see if a bucket exists.
#' svc$head_bucket(
#'   Bucket = "acl1"
#' )
#' }
#'
#' @keywords internal
#'
#' @rdname s3_head_bucket
#'
#' @aliases s3_head_bucket
s3_head_bucket <- function(Bucket, ExpectedBucketOwner = NULL) {
  op <- new_operation(
    name = "HeadBucket",
    http_method = "HEAD",
    http_path = "/{Bucket}",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .s3$head_bucket_input(Bucket = Bucket, ExpectedBucketOwner = ExpectedBucketOwner)
  output <- .s3$head_bucket_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$head_bucket <- s3_head_bucket

#' The HEAD operation retrieves metadata from an object without returning
#' the object itself
#'
#' @description
#' The `HEAD` operation retrieves metadata from an object without returning
#' the object itself. This operation is useful if you're interested only in
#' an object's metadata.
#' 
#' A `HEAD` request has the same options as a `GET` operation on an object.
#' The response is identical to the `GET` response except that there is no
#' response body. Because of this, if the `HEAD` request generates an
#' error, it returns a generic code, such as `400 Bad Request`,
#' `403 Forbidden`, `404 Not Found`, `405 Method Not Allowed`,
#' `412 Precondition Failed`, or `304 Not Modified`. It's not possible to
#' retrieve the exact exception of these error codes.
#' 
#' Request headers are limited to 8 KB in size. For more information, see
#' [Common Request
#' Headers](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTCommonRequestHeaders.html).
#' 
#' ### Permissions
#' 
#' -   **General purpose bucket permissions** - To use `HEAD`, you must
#'     have the `s3:GetObject` permission. You need the relevant read
#'     object (or version) permission for this operation. For more
#'     information, see [Actions, resources, and condition keys for Amazon
#'     S3](https://docs.aws.amazon.com/service-authorization/latest/reference/list_amazons3.html)
#'     in the *Amazon S3 User Guide*. For more information about the
#'     permissions to S3 API operations by S3 resource types, see Required
#'     permissions for Amazon S3 API operations in the *Amazon S3 User
#'     Guide*.
#' 
#'     If the object you request doesn't exist, the error that Amazon S3
#'     returns depends on whether you also have the `s3:ListBucket`
#'     permission.
#' 
#'     -   If you have the `s3:ListBucket` permission on the bucket, Amazon
#'         S3 returns an HTTP status code `404 Not Found` error.
#' 
#'     -   If you don’t have the `s3:ListBucket` permission, Amazon S3
#'         returns an HTTP status code `403 Forbidden` error.
#' 
#' -   **Directory bucket permissions** - To grant access to this API
#'     operation on a directory bucket, we recommend that you use the
#'     [`create_session`](https://docs.aws.amazon.com/AmazonS3/latest/API/API_CreateSession.html)
#'     API operation for session-based authorization. Specifically, you
#'     grant the `s3express:CreateSession` permission to the directory
#'     bucket in a bucket policy or an IAM identity-based policy. Then, you
#'     make the [`create_session`][s3_create_session] API call on the
#'     bucket to obtain a session token. With the session token in your
#'     request header, you can make API requests to this operation. After
#'     the session token expires, you make another
#'     [`create_session`][s3_create_session] API call to generate a new
#'     session token for use. Amazon Web Services CLI or SDKs create
#'     session and refresh the session token automatically to avoid service
#'     interruptions when a session expires. For more information about
#'     authorization, see
#'     [`create_session`](https://docs.aws.amazon.com/AmazonS3/latest/API/API_CreateSession.html)
#'     .
#' 
#'     If you enable `x-amz-checksum-mode` in the request and the object is
#'     encrypted with Amazon Web Services Key Management Service (Amazon
#'     Web Services KMS), you must also have the `kms:GenerateDataKey` and
#'     `kms:Decrypt` permissions in IAM identity-based policies and KMS key
#'     policies for the KMS key to retrieve the checksum of the object.
#' 
#' ### Encryption
#' 
#' Encryption request headers, like `x-amz-server-side-encryption`, should
#' not be sent for `HEAD` requests if your object uses server-side
#' encryption with Key Management Service (KMS) keys (SSE-KMS), dual-layer
#' server-side encryption with Amazon Web Services KMS keys (DSSE-KMS), or
#' server-side encryption with Amazon S3 managed encryption keys (SSE-S3).
#' The `x-amz-server-side-encryption` header is used when you `PUT` an
#' object to S3 and want to specify the encryption method. If you include
#' this header in a `HEAD` request for an object that uses these types of
#' keys, you’ll get an HTTP `400 Bad Request` error. It's because the
#' encryption method can't be changed when you retrieve the object.
#' 
#' If you encrypt an object by using server-side encryption with
#' customer-provided encryption keys (SSE-C) when you store the object in
#' Amazon S3, then when you retrieve the metadata from the object, you must
#' use the following headers to provide the encryption key for the server
#' to be able to retrieve the object's metadata. The headers are:
#' 
#' -   `x-amz-server-side-encryption-customer-algorithm`
#' 
#' -   `x-amz-server-side-encryption-customer-key`
#' 
#' -   `x-amz-server-side-encryption-customer-key-MD5`
#' 
#' For more information about SSE-C, see [Server-Side Encryption (Using
#' Customer-Provided Encryption
#' Keys)](https://docs.aws.amazon.com/AmazonS3/latest/userguide/ServerSideEncryptionCustomerKeys.html)
#' in the *Amazon S3 User Guide*.
#' 
#' **Directory bucket** - For directory buckets, there are only two
#' supported options for server-side encryption: SSE-S3 and SSE-KMS. SSE-C
#' isn't supported. For more information, see [Protecting data with
#' server-side
#' encryption](https://docs.aws.amazon.com/AmazonS3/latest/userguide/) in
#' the *Amazon S3 User Guide*.
#' 
#' ### Versioning
#' 
#' -   If the current version of the object is a delete marker, Amazon S3
#'     behaves as if the object was deleted and includes
#'     `x-amz-delete-marker: true` in the response.
#' 
#' -   If the specified version is a delete marker, the response returns a
#'     `405 Method Not Allowed` error and the `Last-Modified: timestamp`
#'     response header.
#' 
#' 
#' -   **Directory buckets** - Delete marker is not supported for directory
#'     buckets.
#' 
#' -   **Directory buckets** - S3 Versioning isn't enabled and supported
#'     for directory buckets. For this API operation, only the `null` value
#'     of the version ID is supported by directory buckets. You can only
#'     specify `null` to the `versionId` query parameter in the request.
#' 
#' ### HTTP Host header syntax
#' 
#' **Directory buckets** - The HTTP Host header syntax is
#' ` Bucket-name.s3express-zone-id.region-code.amazonaws.com`.
#' 
#' For directory buckets, you must make requests for this API operation to
#' the Zonal endpoint. These endpoints support virtual-hosted-style
#' requests in the format
#' `https://amzn-s3-demo-bucket.s3express-zone-id.region-code.amazonaws.com/key-name `.
#' Path-style requests are not supported. For more information about
#' endpoints in Availability Zones, see [Regional and Zonal endpoints for
#' directory buckets in Availability
#' Zones](https://docs.aws.amazon.com/AmazonS3/latest/userguide/endpoint-directory-buckets-AZ.html)
#' in the *Amazon S3 User Guide*. For more information about endpoints in
#' Local Zones, see [Concepts for directory buckets in Local
#' Zones](https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-lzs-for-directory-buckets.html)
#' in the *Amazon S3 User Guide*.
#' 
#' The following actions are related to [`head_object`][s3_head_object]:
#' 
#' -   [`get_object`][s3_get_object]
#' 
#' -   [`get_object_attributes`][s3_get_object_attributes]
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#'
#' @usage
#' s3_head_object(Bucket, IfMatch, IfModifiedSince, IfNoneMatch,
#'   IfUnmodifiedSince, Key, Range, ResponseCacheControl,
#'   ResponseContentDisposition, ResponseContentEncoding,
#'   ResponseContentLanguage, ResponseContentType, ResponseExpires,
#'   VersionId, SSECustomerAlgorithm, SSECustomerKey, SSECustomerKeyMD5,
#'   RequestPayer, PartNumber, ExpectedBucketOwner, ChecksumMode)
#'
#' @param Bucket &#91;required&#93; The name of the bucket that contains the object.
#' 
#' **Directory buckets** - When you use this operation with a directory
#' bucket, you must use virtual-hosted-style requests in the format
#' ` Bucket-name.s3express-zone-id.region-code.amazonaws.com`. Path-style
#' requests are not supported. Directory bucket names must be unique in the
#' chosen Zone (Availability Zone or Local Zone). Bucket names must follow
#' the format ` bucket-base-name--zone-id--x-s3` (for example,
#' ` amzn-s3-demo-bucket--usw2-az1--x-s3`). For information about bucket
#' naming restrictions, see [Directory bucket naming
#' rules](https://docs.aws.amazon.com/AmazonS3/latest/userguide/directory-bucket-naming-rules.html)
#' in the *Amazon S3 User Guide*.
#' 
#' **Access points** - When you use this action with an access point for
#' general purpose buckets, you must provide the alias of the access point
#' in place of the bucket name or specify the access point ARN. When you
#' use this action with an access point for directory buckets, you must
#' provide the access point name in place of the bucket name. When using
#' the access point ARN, you must direct requests to the access point
#' hostname. The access point hostname takes the form
#' *AccessPointName*-*AccountId*.s3-accesspoint.*Region*.amazonaws.com.
#' When using this action with an access point through the Amazon Web
#' Services SDKs, you provide the access point ARN in place of the bucket
#' name. For more information about access point ARNs, see [Using access
#' points](https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-access-points.html)
#' in the *Amazon S3 User Guide*.
#' 
#' Object Lambda access points are not supported by directory buckets.
#' 
#' **S3 on Outposts** - When you use this action with S3 on Outposts, you
#' must direct requests to the S3 on Outposts hostname. The S3 on Outposts
#' hostname takes the form
#' ` AccessPointName-AccountId.outpostID.s3-outposts.Region.amazonaws.com`.
#' When you use this action with S3 on Outposts, the destination bucket
#' must be the Outposts access point ARN or the access point alias. For
#' more information about S3 on Outposts, see [What is S3 on
#' Outposts?](https://docs.aws.amazon.com/AmazonS3/latest/s3-outposts/S3onOutposts.html)
#' in the *Amazon S3 User Guide*.
#' @param IfMatch Return the object only if its entity tag (ETag) is the same as the one
#' specified; otherwise, return a 412 (precondition failed) error.
#' 
#' If both of the `If-Match` and `If-Unmodified-Since` headers are present
#' in the request as follows:
#' 
#' -   `If-Match` condition evaluates to `true`, and;
#' 
#' -   `If-Unmodified-Since` condition evaluates to `false`;
#' 
#' Then Amazon S3 returns `200 OK` and the data requested.
#' 
#' For more information about conditional requests, see [RFC
#' 7232](https://datatracker.ietf.org/doc/html/rfc7232).
#' @param IfModifiedSince Return the object only if it has been modified since the specified time;
#' otherwise, return a 304 (not modified) error.
#' 
#' If both of the `If-None-Match` and `If-Modified-Since` headers are
#' present in the request as follows:
#' 
#' -   `If-None-Match` condition evaluates to `false`, and;
#' 
#' -   `If-Modified-Since` condition evaluates to `true`;
#' 
#' Then Amazon S3 returns the `304 Not Modified` response code.
#' 
#' For more information about conditional requests, see [RFC
#' 7232](https://datatracker.ietf.org/doc/html/rfc7232).
#' @param IfNoneMatch Return the object only if its entity tag (ETag) is different from the
#' one specified; otherwise, return a 304 (not modified) error.
#' 
#' If both of the `If-None-Match` and `If-Modified-Since` headers are
#' present in the request as follows:
#' 
#' -   `If-None-Match` condition evaluates to `false`, and;
#' 
#' -   `If-Modified-Since` condition evaluates to `true`;
#' 
#' Then Amazon S3 returns the `304 Not Modified` response code.
#' 
#' For more information about conditional requests, see [RFC
#' 7232](https://datatracker.ietf.org/doc/html/rfc7232).
#' @param IfUnmodifiedSince Return the object only if it has not been modified since the specified
#' time; otherwise, return a 412 (precondition failed) error.
#' 
#' If both of the `If-Match` and `If-Unmodified-Since` headers are present
#' in the request as follows:
#' 
#' -   `If-Match` condition evaluates to `true`, and;
#' 
#' -   `If-Unmodified-Since` condition evaluates to `false`;
#' 
#' Then Amazon S3 returns `200 OK` and the data requested.
#' 
#' For more information about conditional requests, see [RFC
#' 7232](https://datatracker.ietf.org/doc/html/rfc7232).
#' @param Key &#91;required&#93; The object key.
#' @param Range HeadObject returns only the metadata for an object. If the Range is
#' satisfiable, only the `ContentLength` is affected in the response. If
#' the Range is not satisfiable, S3 returns a
#' `416 - Requested Range Not Satisfiable` error.
#' @param ResponseCacheControl Sets the `Cache-Control` header of the response.
#' @param ResponseContentDisposition Sets the `Content-Disposition` header of the response.
#' @param ResponseContentEncoding Sets the `Content-Encoding` header of the response.
#' @param ResponseContentLanguage Sets the `Content-Language` header of the response.
#' @param ResponseContentType Sets the `Content-Type` header of the response.
#' @param ResponseExpires Sets the `Expires` header of the response.
#' @param VersionId Version ID used to reference a specific version of the object.
#' 
#' For directory buckets in this API operation, only the `null` value of
#' the version ID is supported.
#' @param SSECustomerAlgorithm Specifies the algorithm to use when encrypting the object (for example,
#' AES256).
#' 
#' This functionality is not supported for directory buckets.
#' @param SSECustomerKey Specifies the customer-provided encryption key for Amazon S3 to use in
#' encrypting data. This value is used to store the object and then it is
#' discarded; Amazon S3 does not store the encryption key. The key must be
#' appropriate for use with the algorithm specified in the
#' `x-amz-server-side-encryption-customer-algorithm` header.
#' 
#' This functionality is not supported for directory buckets.
#' @param SSECustomerKeyMD5 Specifies the 128-bit MD5 digest of the encryption key according to RFC
#' 1321. Amazon S3 uses this header for a message integrity check to ensure
#' that the encryption key was transmitted without error.
#' 
#' This functionality is not supported for directory buckets.
#' @param RequestPayer 
#' @param PartNumber Part number of the object being read. This is a positive integer between
#' 1 and 10,000. Effectively performs a 'ranged' HEAD request for the part
#' specified. Useful querying about the size of the part and the number of
#' parts in this object.
#' @param ExpectedBucketOwner The account ID of the expected bucket owner. If the account ID that you
#' provide does not match the actual owner of the bucket, the request fails
#' with the HTTP status code `403 Forbidden` (access denied).
#' @param ChecksumMode To retrieve the checksum, this parameter must be enabled.
#' 
#' **General purpose buckets** - If you enable checksum mode and the object
#' is uploaded with a
#' [checksum](https://docs.aws.amazon.com/AmazonS3/latest/API/API_Checksum.html)
#' and encrypted with an Key Management Service (KMS) key, you must have
#' permission to use the `kms:Decrypt` action to retrieve the checksum.
#' 
#' **Directory buckets** - If you enable `ChecksumMode` and the object is
#' encrypted with Amazon Web Services Key Management Service (Amazon Web
#' Services KMS), you must also have the `kms:GenerateDataKey` and
#' `kms:Decrypt` permissions in IAM identity-based policies and KMS key
#' policies for the KMS key to retrieve the checksum of the object.
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   DeleteMarker = TRUE|FALSE,
#'   AcceptRanges = "string",
#'   Expiration = "string",
#'   Restore = "string",
#'   ArchiveStatus = "ARCHIVE_ACCESS"|"DEEP_ARCHIVE_ACCESS",
#'   LastModified = as.POSIXct(
#'     "2015-01-01"
#'   ),
#'   ContentLength = 123,
#'   ChecksumCRC32 = "string",
#'   ChecksumCRC32C = "string",
#'   ChecksumCRC64NVME = "string",
#'   ChecksumSHA1 = "string",
#'   ChecksumSHA256 = "string",
#'   ChecksumType = "COMPOSITE"|"FULL_OBJECT",
#'   ETag = "string",
#'   MissingMeta = 123,
#'   VersionId = "string",
#'   CacheControl = "string",
#'   ContentDisposition = "string",
#'   ContentEncoding = "string",
#'   ContentLanguage = "string",
#'   ContentType = "string",
#'   ContentRange = "string",
#'   Expires = "string",
#'   WebsiteRedirectLocation = "string",
#'   ServerSideEncryption = "AES256"|"aws:fsx"|"aws:kms"|"aws:kms:dsse",
#'   Metadata = list(
#'     "string"
#'   ),
#'   SSECustomerAlgorithm = "string",
#'   SSECustomerKeyMD5 = "string",
#'   SSEKMSKeyId = "string",
#'   BucketKeyEnabled = TRUE|FALSE,
#'   StorageClass = "STANDARD"|"REDUCED_REDUNDANCY"|"STANDARD_IA"|"ONEZONE_IA"|"INTELLIGENT_TIERING"|"GLACIER"|"DEEP_ARCHIVE"|"OUTPOSTS"|"GLACIER_IR"|"SNOW"|"EXPRESS_ONEZONE"|"FSX_OPENZFS"|"FSX_ONTAP",
#'   RequestCharged = "requester",
#'   ReplicationStatus = "COMPLETE"|"PENDING"|"FAILED"|"REPLICA"|"COMPLETED",
#'   PartsCount = 123,
#'   TagCount = 123,
#'   ObjectLockMode = "GOVERNANCE"|"COMPLIANCE",
#'   ObjectLockRetainUntilDate = as.POSIXct(
#'     "2015-01-01"
#'   ),
#'   ObjectLockLegalHoldStatus = "ON"|"OFF"
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$head_object(
#'   Bucket = "string",
#'   IfMatch = "string",
#'   IfModifiedSince = as.POSIXct(
#'     "2015-01-01"
#'   ),
#'   IfNoneMatch = "string",
#'   IfUnmodifiedSince = as.POSIXct(
#'     "2015-01-01"
#'   ),
#'   Key = "string",
#'   Range = "string",
#'   ResponseCacheControl = "string",
#'   ResponseContentDisposition = "string",
#'   ResponseContentEncoding = "string",
#'   ResponseContentLanguage = "string",
#'   ResponseContentType = "string",
#'   ResponseExpires = as.POSIXct(
#'     "2015-01-01"
#'   ),
#'   VersionId = "string",
#'   SSECustomerAlgorithm = "string",
#'   SSECustomerKey = "string",
#'   SSECustomerKeyMD5 = "string",
#'   RequestPayer = "requester",
#'   PartNumber = 123,
#'   ExpectedBucketOwner = "string",
#'   ChecksumMode = "ENABLED"
#' )
#' ```
#'
#' @examples
#' \dontrun{
#' # The following example retrieves an object metadata.
#' svc$head_object(
#'   Bucket = "examplebucket",
#'   Key = "HappyFace.jpg"
#' )
#' }
#'
#' @keywords internal
#'
#' @rdname s3_head_object
#'
#' @aliases s3_head_object
s3_head_object <- function(Bucket, IfMatch = NULL, IfModifiedSince = NULL, IfNoneMatch = NULL, IfUnmodifiedSince = NULL, Key, Range = NULL, ResponseCacheControl = NULL, ResponseContentDisposition = NULL, ResponseContentEncoding = NULL, ResponseContentLanguage = NULL, ResponseContentType = NULL, ResponseExpires = NULL, VersionId = NULL, SSECustomerAlgorithm = NULL, SSECustomerKey = NULL, SSECustomerKeyMD5 = NULL, RequestPayer = NULL, PartNumber = NULL, ExpectedBucketOwner = NULL, ChecksumMode = NULL) {
  op <- new_operation(
    name = "HeadObject",
    http_method = "HEAD",
    http_path = "/{Bucket}/{Key+}",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .s3$head_object_input(Bucket = Bucket, IfMatch = IfMatch, IfModifiedSince = IfModifiedSince, IfNoneMatch = IfNoneMatch, IfUnmodifiedSince = IfUnmodifiedSince, Key = Key, Range = Range, ResponseCacheControl = ResponseCacheControl, ResponseContentDisposition = ResponseContentDisposition, ResponseContentEncoding = ResponseContentEncoding, ResponseContentLanguage = ResponseContentLanguage, ResponseContentType = ResponseContentType, ResponseExpires = ResponseExpires, VersionId = VersionId, SSECustomerAlgorithm = SSECustomerAlgorithm, SSECustomerKey = SSECustomerKey, SSECustomerKeyMD5 = SSECustomerKeyMD5, RequestPayer = RequestPayer, PartNumber = PartNumber, ExpectedBucketOwner = ExpectedBucketOwner, ChecksumMode = ChecksumMode)
  output <- .s3$head_object_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$head_object <- s3_head_object

#' This operation is not supported for directory buckets
#'
#' @description
#' This operation is not supported for directory buckets.
#' 
#' Lists the analytics configurations for the bucket. You can have up to
#' 1,000 analytics configurations per bucket.
#' 
#' This action supports list pagination and does not return more than 100
#' configurations at a time. You should always check the `IsTruncated`
#' element in the response. If there are no more configurations to list,
#' `IsTruncated` is set to false. If there are more configurations to list,
#' `IsTruncated` is set to true, and there will be a value in
#' `NextContinuationToken`. You use the `NextContinuationToken` value to
#' continue the pagination of the list by passing the value in
#' continuation-token in the request to `GET` the next page.
#' 
#' To use this operation, you must have permissions to perform the
#' `s3:GetAnalyticsConfiguration` action. The bucket owner has this
#' permission by default. The bucket owner can grant this permission to
#' others. For more information about permissions, see [Permissions Related
#' to Bucket Subresource
#' Operations](https://docs.aws.amazon.com/AmazonS3/latest/userguide/security_iam_service-with-iam.html#security_iam_service-with-iam-id-based-policies-actions)
#' and [Managing Access Permissions to Your Amazon S3
#' Resources](https://docs.aws.amazon.com/AmazonS3/latest/userguide/security-iam.html).
#' 
#' For information about Amazon S3 analytics feature, see [Amazon S3
#' Analytics – Storage Class
#' Analysis](https://docs.aws.amazon.com/AmazonS3/latest/userguide/analytics-storage-class.html).
#' 
#' The following operations are related to
#' [`list_bucket_analytics_configurations`][s3_list_bucket_analytics_configurations]:
#' 
#' -   [`get_bucket_analytics_configuration`][s3_get_bucket_analytics_configuration]
#' 
#' -   [`delete_bucket_analytics_configuration`][s3_delete_bucket_analytics_configuration]
#' 
#' -   [`put_bucket_analytics_configuration`][s3_put_bucket_analytics_configuration]
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#'
#' @usage
#' s3_list_bucket_analytics_configurations(Bucket, ContinuationToken,
#'   ExpectedBucketOwner)
#'
#' @param Bucket &#91;required&#93; The name of the bucket from which analytics configurations are
#' retrieved.
#' @param ContinuationToken The `ContinuationToken` that represents a placeholder from where this
#' request should begin.
#' @param ExpectedBucketOwner The account ID of the expected bucket owner. If the account ID that you
#' provide does not match the actual owner of the bucket, the request fails
#' with the HTTP status code `403 Forbidden` (access denied).
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   IsTruncated = TRUE|FALSE,
#'   ContinuationToken = "string",
#'   NextContinuationToken = "string",
#'   AnalyticsConfigurationList = list(
#'     list(
#'       Id = "string",
#'       Filter = list(
#'         Prefix = "string",
#'         Tag = list(
#'           Key = "string",
#'           Value = "string"
#'         ),
#'         And = list(
#'           Prefix = "string",
#'           Tags = list(
#'             list(
#'               Key = "string",
#'               Value = "string"
#'             )
#'           )
#'         )
#'       ),
#'       StorageClassAnalysis = list(
#'         DataExport = list(
#'           OutputSchemaVersion = "V_1",
#'           Destination = list(
#'             S3BucketDestination = list(
#'               Format = "CSV",
#'               BucketAccountId = "string",
#'               Bucket = "string",
#'               Prefix = "string"
#'             )
#'           )
#'         )
#'       )
#'     )
#'   )
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$list_bucket_analytics_configurations(
#'   Bucket = "string",
#'   ContinuationToken = "string",
#'   ExpectedBucketOwner = "string"
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname s3_list_bucket_analytics_configurations
#'
#' @aliases s3_list_bucket_analytics_configurations
s3_list_bucket_analytics_configurations <- function(Bucket, ContinuationToken = NULL, ExpectedBucketOwner = NULL) {
  op <- new_operation(
    name = "ListBucketAnalyticsConfigurations",
    http_method = "GET",
    http_path = "/{Bucket}?analytics",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .s3$list_bucket_analytics_configurations_input(Bucket = Bucket, ContinuationToken = ContinuationToken, ExpectedBucketOwner = ExpectedBucketOwner)
  output <- .s3$list_bucket_analytics_configurations_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$list_bucket_analytics_configurations <- s3_list_bucket_analytics_configurations

#' This operation is not supported for directory buckets
#'
#' @description
#' This operation is not supported for directory buckets.
#' 
#' Lists the S3 Intelligent-Tiering configuration from the specified
#' bucket.
#' 
#' The S3 Intelligent-Tiering storage class is designed to optimize storage
#' costs by automatically moving data to the most cost-effective storage
#' access tier, without performance impact or operational overhead. S3
#' Intelligent-Tiering delivers automatic cost savings in three low latency
#' and high throughput access tiers. To get the lowest storage cost on data
#' that can be accessed in minutes to hours, you can choose to activate
#' additional archiving capabilities.
#' 
#' The S3 Intelligent-Tiering storage class is the ideal storage class for
#' data with unknown, changing, or unpredictable access patterns,
#' independent of object size or retention period. If the size of an object
#' is less than 128 KB, it is not monitored and not eligible for
#' auto-tiering. Smaller objects can be stored, but they are always charged
#' at the Frequent Access tier rates in the S3 Intelligent-Tiering storage
#' class.
#' 
#' For more information, see [Storage class for automatically optimizing
#' frequently and infrequently accessed
#' objects](https://docs.aws.amazon.com/AmazonS3/latest/userguide/storage-class-intro.html#sc-dynamic-data-access).
#' 
#' Operations related to
#' [`list_bucket_intelligent_tiering_configurations`][s3_list_bucket_intelligent_tiering_configurations]
#' include:
#' 
#' -   [`delete_bucket_intelligent_tiering_configuration`][s3_delete_bucket_intelligent_tiering_configuration]
#' 
#' -   [`put_bucket_intelligent_tiering_configuration`][s3_put_bucket_intelligent_tiering_configuration]
#' 
#' -   [`get_bucket_intelligent_tiering_configuration`][s3_get_bucket_intelligent_tiering_configuration]
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#'
#' @usage
#' s3_list_bucket_intelligent_tiering_configurations(Bucket,
#'   ContinuationToken, ExpectedBucketOwner)
#'
#' @param Bucket &#91;required&#93; The name of the Amazon S3 bucket whose configuration you want to modify
#' or retrieve.
#' @param ContinuationToken The `ContinuationToken` that represents a placeholder from where this
#' request should begin.
#' @param ExpectedBucketOwner The account ID of the expected bucket owner. If the account ID that you
#' provide does not match the actual owner of the bucket, the request fails
#' with the HTTP status code `403 Forbidden` (access denied).
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   IsTruncated = TRUE|FALSE,
#'   ContinuationToken = "string",
#'   NextContinuationToken = "string",
#'   IntelligentTieringConfigurationList = list(
#'     list(
#'       Id = "string",
#'       Filter = list(
#'         Prefix = "string",
#'         Tag = list(
#'           Key = "string",
#'           Value = "string"
#'         ),
#'         And = list(
#'           Prefix = "string",
#'           Tags = list(
#'             list(
#'               Key = "string",
#'               Value = "string"
#'             )
#'           )
#'         )
#'       ),
#'       Status = "Enabled"|"Disabled",
#'       Tierings = list(
#'         list(
#'           Days = 123,
#'           AccessTier = "ARCHIVE_ACCESS"|"DEEP_ARCHIVE_ACCESS"
#'         )
#'       )
#'     )
#'   )
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$list_bucket_intelligent_tiering_configurations(
#'   Bucket = "string",
#'   ContinuationToken = "string",
#'   ExpectedBucketOwner = "string"
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname s3_list_bucket_intelligent_tiering_configurations
#'
#' @aliases s3_list_bucket_intelligent_tiering_configurations
s3_list_bucket_intelligent_tiering_configurations <- function(Bucket, ContinuationToken = NULL, ExpectedBucketOwner = NULL) {
  op <- new_operation(
    name = "ListBucketIntelligentTieringConfigurations",
    http_method = "GET",
    http_path = "/{Bucket}?intelligent-tiering",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .s3$list_bucket_intelligent_tiering_configurations_input(Bucket = Bucket, ContinuationToken = ContinuationToken, ExpectedBucketOwner = ExpectedBucketOwner)
  output <- .s3$list_bucket_intelligent_tiering_configurations_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$list_bucket_intelligent_tiering_configurations <- s3_list_bucket_intelligent_tiering_configurations

#' This operation is not supported for directory buckets
#'
#' @description
#' This operation is not supported for directory buckets.
#' 
#' Returns a list of S3 Inventory configurations for the bucket. You can
#' have up to 1,000 inventory configurations per bucket.
#' 
#' This action supports list pagination and does not return more than 100
#' configurations at a time. Always check the `IsTruncated` element in the
#' response. If there are no more configurations to list, `IsTruncated` is
#' set to false. If there are more configurations to list, `IsTruncated` is
#' set to true, and there is a value in `NextContinuationToken`. You use
#' the `NextContinuationToken` value to continue the pagination of the list
#' by passing the value in continuation-token in the request to `GET` the
#' next page.
#' 
#' To use this operation, you must have permissions to perform the
#' `s3:GetInventoryConfiguration` action. The bucket owner has this
#' permission by default. The bucket owner can grant this permission to
#' others. For more information about permissions, see [Permissions Related
#' to Bucket Subresource
#' Operations](https://docs.aws.amazon.com/AmazonS3/latest/userguide/security_iam_service-with-iam.html#security_iam_service-with-iam-id-based-policies-actions)
#' and [Managing Access Permissions to Your Amazon S3
#' Resources](https://docs.aws.amazon.com/AmazonS3/latest/userguide/security-iam.html).
#' 
#' For information about the Amazon S3 inventory feature, see [Amazon S3
#' Inventory](https://docs.aws.amazon.com/AmazonS3/latest/userguide/storage-inventory.html)
#' 
#' The following operations are related to
#' [`list_bucket_inventory_configurations`][s3_list_bucket_inventory_configurations]:
#' 
#' -   [`get_bucket_inventory_configuration`][s3_get_bucket_inventory_configuration]
#' 
#' -   [`delete_bucket_inventory_configuration`][s3_delete_bucket_inventory_configuration]
#' 
#' -   [`put_bucket_inventory_configuration`][s3_put_bucket_inventory_configuration]
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#'
#' @usage
#' s3_list_bucket_inventory_configurations(Bucket, ContinuationToken,
#'   ExpectedBucketOwner)
#'
#' @param Bucket &#91;required&#93; The name of the bucket containing the inventory configurations to
#' retrieve.
#' @param ContinuationToken The marker used to continue an inventory configuration listing that has
#' been truncated. Use the `NextContinuationToken` from a previously
#' truncated list response to continue the listing. The continuation token
#' is an opaque value that Amazon S3 understands.
#' @param ExpectedBucketOwner The account ID of the expected bucket owner. If the account ID that you
#' provide does not match the actual owner of the bucket, the request fails
#' with the HTTP status code `403 Forbidden` (access denied).
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   ContinuationToken = "string",
#'   InventoryConfigurationList = list(
#'     list(
#'       Destination = list(
#'         S3BucketDestination = list(
#'           AccountId = "string",
#'           Bucket = "string",
#'           Format = "CSV"|"ORC"|"Parquet",
#'           Prefix = "string",
#'           Encryption = list(
#'             SSES3 = list(),
#'             SSEKMS = list(
#'               KeyId = "string"
#'             )
#'           )
#'         )
#'       ),
#'       IsEnabled = TRUE|FALSE,
#'       Filter = list(
#'         Prefix = "string"
#'       ),
#'       Id = "string",
#'       IncludedObjectVersions = "All"|"Current",
#'       OptionalFields = list(
#'         "Size"|"LastModifiedDate"|"StorageClass"|"ETag"|"IsMultipartUploaded"|"ReplicationStatus"|"EncryptionStatus"|"ObjectLockRetainUntilDate"|"ObjectLockMode"|"ObjectLockLegalHoldStatus"|"IntelligentTieringAccessTier"|"BucketKeyStatus"|"ChecksumAlgorithm"|"ObjectAccessControlList"|"ObjectOwner"|"LifecycleExpirationDate"
#'       ),
#'       Schedule = list(
#'         Frequency = "Daily"|"Weekly"
#'       )
#'     )
#'   ),
#'   IsTruncated = TRUE|FALSE,
#'   NextContinuationToken = "string"
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$list_bucket_inventory_configurations(
#'   Bucket = "string",
#'   ContinuationToken = "string",
#'   ExpectedBucketOwner = "string"
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname s3_list_bucket_inventory_configurations
#'
#' @aliases s3_list_bucket_inventory_configurations
s3_list_bucket_inventory_configurations <- function(Bucket, ContinuationToken = NULL, ExpectedBucketOwner = NULL) {
  op <- new_operation(
    name = "ListBucketInventoryConfigurations",
    http_method = "GET",
    http_path = "/{Bucket}?inventory",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .s3$list_bucket_inventory_configurations_input(Bucket = Bucket, ContinuationToken = ContinuationToken, ExpectedBucketOwner = ExpectedBucketOwner)
  output <- .s3$list_bucket_inventory_configurations_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$list_bucket_inventory_configurations <- s3_list_bucket_inventory_configurations

#' This operation is not supported for directory buckets
#'
#' @description
#' This operation is not supported for directory buckets.
#' 
#' Lists the metrics configurations for the bucket. The metrics
#' configurations are only for the request metrics of the bucket and do not
#' provide information on daily storage metrics. You can have up to 1,000
#' configurations per bucket.
#' 
#' This action supports list pagination and does not return more than 100
#' configurations at a time. Always check the `IsTruncated` element in the
#' response. If there are no more configurations to list, `IsTruncated` is
#' set to false. If there are more configurations to list, `IsTruncated` is
#' set to true, and there is a value in `NextContinuationToken`. You use
#' the `NextContinuationToken` value to continue the pagination of the list
#' by passing the value in `continuation-token` in the request to `GET` the
#' next page.
#' 
#' To use this operation, you must have permissions to perform the
#' `s3:GetMetricsConfiguration` action. The bucket owner has this
#' permission by default. The bucket owner can grant this permission to
#' others. For more information about permissions, see [Permissions Related
#' to Bucket Subresource
#' Operations](https://docs.aws.amazon.com/AmazonS3/latest/userguide/security_iam_service-with-iam.html#security_iam_service-with-iam-id-based-policies-actions)
#' and [Managing Access Permissions to Your Amazon S3
#' Resources](https://docs.aws.amazon.com/AmazonS3/latest/userguide/security-iam.html).
#' 
#' For more information about metrics configurations and CloudWatch request
#' metrics, see [Monitoring Metrics with Amazon
#' CloudWatch](https://docs.aws.amazon.com/AmazonS3/latest/userguide/cloudwatch-monitoring.html).
#' 
#' The following operations are related to
#' [`list_bucket_metrics_configurations`][s3_list_bucket_metrics_configurations]:
#' 
#' -   [`put_bucket_metrics_configuration`][s3_put_bucket_metrics_configuration]
#' 
#' -   [`get_bucket_metrics_configuration`][s3_get_bucket_metrics_configuration]
#' 
#' -   [`delete_bucket_metrics_configuration`][s3_delete_bucket_metrics_configuration]
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#'
#' @usage
#' s3_list_bucket_metrics_configurations(Bucket, ContinuationToken,
#'   ExpectedBucketOwner)
#'
#' @param Bucket &#91;required&#93; The name of the bucket containing the metrics configurations to
#' retrieve.
#' @param ContinuationToken The marker that is used to continue a metrics configuration listing that
#' has been truncated. Use the `NextContinuationToken` from a previously
#' truncated list response to continue the listing. The continuation token
#' is an opaque value that Amazon S3 understands.
#' @param ExpectedBucketOwner The account ID of the expected bucket owner. If the account ID that you
#' provide does not match the actual owner of the bucket, the request fails
#' with the HTTP status code `403 Forbidden` (access denied).
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   IsTruncated = TRUE|FALSE,
#'   ContinuationToken = "string",
#'   NextContinuationToken = "string",
#'   MetricsConfigurationList = list(
#'     list(
#'       Id = "string",
#'       Filter = list(
#'         Prefix = "string",
#'         Tag = list(
#'           Key = "string",
#'           Value = "string"
#'         ),
#'         AccessPointArn = "string",
#'         And = list(
#'           Prefix = "string",
#'           Tags = list(
#'             list(
#'               Key = "string",
#'               Value = "string"
#'             )
#'           ),
#'           AccessPointArn = "string"
#'         )
#'       )
#'     )
#'   )
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$list_bucket_metrics_configurations(
#'   Bucket = "string",
#'   ContinuationToken = "string",
#'   ExpectedBucketOwner = "string"
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname s3_list_bucket_metrics_configurations
#'
#' @aliases s3_list_bucket_metrics_configurations
s3_list_bucket_metrics_configurations <- function(Bucket, ContinuationToken = NULL, ExpectedBucketOwner = NULL) {
  op <- new_operation(
    name = "ListBucketMetricsConfigurations",
    http_method = "GET",
    http_path = "/{Bucket}?metrics",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .s3$list_bucket_metrics_configurations_input(Bucket = Bucket, ContinuationToken = ContinuationToken, ExpectedBucketOwner = ExpectedBucketOwner)
  output <- .s3$list_bucket_metrics_configurations_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$list_bucket_metrics_configurations <- s3_list_bucket_metrics_configurations

#' This operation is not supported for directory buckets
#'
#' @description
#' This operation is not supported for directory buckets.
#' 
#' Returns a list of all buckets owned by the authenticated sender of the
#' request. To grant IAM permission to use this operation, you must add the
#' `s3:ListAllMyBuckets` policy action.
#' 
#' For information about Amazon S3 buckets, see [Creating, configuring, and
#' working with Amazon S3
#' buckets](https://docs.aws.amazon.com/AmazonS3/latest/userguide/creating-buckets-s3.html).
#' 
#' We strongly recommend using only paginated
#' [`list_buckets`][s3_list_buckets] requests. Unpaginated
#' [`list_buckets`][s3_list_buckets] requests are only supported for Amazon
#' Web Services accounts set to the default general purpose bucket quota of
#' 10,000. If you have an approved general purpose bucket quota above
#' 10,000, you must send paginated [`list_buckets`][s3_list_buckets]
#' requests to list your account’s buckets. All unpaginated
#' [`list_buckets`][s3_list_buckets] requests will be rejected for Amazon
#' Web Services accounts with a general purpose bucket quota greater than
#' 10,000.
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#'
#' @usage
#' s3_list_buckets(MaxBuckets, ContinuationToken, Prefix, BucketRegion)
#'
#' @param MaxBuckets Maximum number of buckets to be returned in response. When the number is
#' more than the count of buckets that are owned by an Amazon Web Services
#' account, return all the buckets in response.
#' @param ContinuationToken `ContinuationToken` indicates to Amazon S3 that the list is being
#' continued on this bucket with a token. `ContinuationToken` is obfuscated
#' and is not a real key. You can use this `ContinuationToken` for
#' pagination of the list results.
#' 
#' Length Constraints: Minimum length of 0. Maximum length of 1024.
#' 
#' Required: No.
#' 
#' If you specify the `bucket-region`, `prefix`, or `continuation-token`
#' query parameters without using `max-buckets` to set the maximum number
#' of buckets returned in the response, Amazon S3 applies a default page
#' size of 10,000 and provides a continuation token if there are more
#' buckets.
#' @param Prefix Limits the response to bucket names that begin with the specified bucket
#' name prefix.
#' @param BucketRegion Limits the response to buckets that are located in the specified Amazon
#' Web Services Region. The Amazon Web Services Region must be expressed
#' according to the Amazon Web Services Region code, such as `us-west-2`
#' for the US West (Oregon) Region. For a list of the valid values for all
#' of the Amazon Web Services Regions, see [Regions and
#' Endpoints](https://docs.aws.amazon.com/general/latest/gr/rande.html#s3_region).
#' 
#' Requests made to a Regional endpoint that is different from the
#' `bucket-region` parameter are not supported. For example, if you want to
#' limit the response to your buckets in Region `us-west-2`, the request
#' must be made to an endpoint in Region `us-west-2`.
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   Buckets = list(
#'     list(
#'       Name = "string",
#'       CreationDate = as.POSIXct(
#'         "2015-01-01"
#'       ),
#'       BucketRegion = "string",
#'       BucketArn = "string"
#'     )
#'   ),
#'   Owner = list(
#'     DisplayName = "string",
#'     ID = "string"
#'   ),
#'   ContinuationToken = "string",
#'   Prefix = "string"
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$list_buckets(
#'   MaxBuckets = 123,
#'   ContinuationToken = "string",
#'   Prefix = "string",
#'   BucketRegion = "string"
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname s3_list_buckets
#'
#' @aliases s3_list_buckets
s3_list_buckets <- function(MaxBuckets = NULL, ContinuationToken = NULL, Prefix = NULL, BucketRegion = NULL) {
  op <- new_operation(
    name = "ListBuckets",
    http_method = "GET",
    http_path = "/",
    host_prefix = "",
    paginator = list(input_token = "ContinuationToken", limit_key = "MaxBuckets", output_token = "ContinuationToken", result_key = "Buckets"),
    stream_api = FALSE
  )
  input <- .s3$list_buckets_input(MaxBuckets = MaxBuckets, ContinuationToken = ContinuationToken, Prefix = Prefix, BucketRegion = BucketRegion)
  output <- .s3$list_buckets_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$list_buckets <- s3_list_buckets

#' Returns a list of all Amazon S3 directory buckets owned by the
#' authenticated sender of the request
#'
#' @description
#' Returns a list of all Amazon S3 directory buckets owned by the
#' authenticated sender of the request. For more information about
#' directory buckets, see [Directory
#' buckets](https://docs.aws.amazon.com/AmazonS3/latest/userguide/directory-buckets-overview.html)
#' in the *Amazon S3 User Guide*.
#' 
#' **Directory buckets** - For directory buckets, you must make requests
#' for this API operation to the Regional endpoint. These endpoints support
#' path-style requests in the format
#' `https://s3express-control.region-code.amazonaws.com/bucket-name `.
#' Virtual-hosted-style requests aren't supported. For more information
#' about endpoints in Availability Zones, see [Regional and Zonal endpoints
#' for directory buckets in Availability
#' Zones](https://docs.aws.amazon.com/AmazonS3/latest/userguide/endpoint-directory-buckets-AZ.html)
#' in the *Amazon S3 User Guide*. For more information about endpoints in
#' Local Zones, see [Concepts for directory buckets in Local
#' Zones](https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-lzs-for-directory-buckets.html)
#' in the *Amazon S3 User Guide*.
#' 
#' ### Permissions
#' 
#' You must have the `s3express:ListAllMyDirectoryBuckets` permission in an
#' IAM identity-based policy instead of a bucket policy. Cross-account
#' access to this API operation isn't supported. This operation can only be
#' performed by the Amazon Web Services account that owns the resource. For
#' more information about directory bucket policies and permissions, see
#' [Amazon Web Services Identity and Access Management (IAM) for S3 Express
#' One
#' Zone](https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-express-security-iam.html)
#' in the *Amazon S3 User Guide*.
#' 
#' ### HTTP Host header syntax
#' 
#' **Directory buckets** - The HTTP Host header syntax is
#' `s3express-control.region.amazonaws.com`.
#' 
#' The `BucketRegion` response element is not part of the
#' [`list_directory_buckets`][s3_list_directory_buckets] Response Syntax.
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#'
#' @usage
#' s3_list_directory_buckets(ContinuationToken, MaxDirectoryBuckets)
#'
#' @param ContinuationToken `ContinuationToken` indicates to Amazon S3 that the list is being
#' continued on buckets in this account with a token. `ContinuationToken`
#' is obfuscated and is not a real bucket name. You can use this
#' `ContinuationToken` for the pagination of the list results.
#' @param MaxDirectoryBuckets Maximum number of buckets to be returned in response. When the number is
#' more than the count of buckets that are owned by an Amazon Web Services
#' account, return all the buckets in response.
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   Buckets = list(
#'     list(
#'       Name = "string",
#'       CreationDate = as.POSIXct(
#'         "2015-01-01"
#'       ),
#'       BucketRegion = "string",
#'       BucketArn = "string"
#'     )
#'   ),
#'   ContinuationToken = "string"
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$list_directory_buckets(
#'   ContinuationToken = "string",
#'   MaxDirectoryBuckets = 123
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname s3_list_directory_buckets
#'
#' @aliases s3_list_directory_buckets
s3_list_directory_buckets <- function(ContinuationToken = NULL, MaxDirectoryBuckets = NULL) {
  op <- new_operation(
    name = "ListDirectoryBuckets",
    http_method = "GET",
    http_path = "/",
    host_prefix = "",
    paginator = list(input_token = "ContinuationToken", limit_key = "MaxDirectoryBuckets", output_token = "ContinuationToken", result_key = "Buckets"),
    stream_api = FALSE
  )
  input <- .s3$list_directory_buckets_input(ContinuationToken = ContinuationToken, MaxDirectoryBuckets = MaxDirectoryBuckets)
  output <- .s3$list_directory_buckets_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$list_directory_buckets <- s3_list_directory_buckets

#' This operation lists in-progress multipart uploads in a bucket
#'
#' @description
#' This operation lists in-progress multipart uploads in a bucket. An
#' in-progress multipart upload is a multipart upload that has been
#' initiated by the [`create_multipart_upload`][s3_create_multipart_upload]
#' request, but has not yet been completed or aborted.
#' 
#' **Directory buckets** - If multipart uploads in a directory bucket are
#' in progress, you can't delete the bucket until all the in-progress
#' multipart uploads are aborted or completed. To delete these in-progress
#' multipart uploads, use the
#' [`list_multipart_uploads`][s3_list_multipart_uploads] operation to list
#' the in-progress multipart uploads in the bucket and use the
#' [`abort_multipart_upload`][s3_abort_multipart_upload] operation to abort
#' all the in-progress multipart uploads.
#' 
#' The [`list_multipart_uploads`][s3_list_multipart_uploads] operation
#' returns a maximum of 1,000 multipart uploads in the response. The limit
#' of 1,000 multipart uploads is also the default value. You can further
#' limit the number of uploads in a response by specifying the
#' `max-uploads` request parameter. If there are more than 1,000 multipart
#' uploads that satisfy your
#' [`list_multipart_uploads`][s3_list_multipart_uploads] request, the
#' response returns an `IsTruncated` element with the value of `true`, a
#' `NextKeyMarker` element, and a `NextUploadIdMarker` element. To list the
#' remaining multipart uploads, you need to make subsequent
#' [`list_multipart_uploads`][s3_list_multipart_uploads] requests. In these
#' requests, include two query parameters: `key-marker` and
#' `upload-id-marker`. Set the value of `key-marker` to the `NextKeyMarker`
#' value from the previous response. Similarly, set the value of
#' `upload-id-marker` to the `NextUploadIdMarker` value from the previous
#' response.
#' 
#' **Directory buckets** - The `upload-id-marker` element and the
#' `NextUploadIdMarker` element aren't supported by directory buckets. To
#' list the additional multipart uploads, you only need to set the value of
#' `key-marker` to the `NextKeyMarker` value from the previous response.
#' 
#' For more information about multipart uploads, see [Uploading Objects
#' Using Multipart
#' Upload](https://docs.aws.amazon.com/AmazonS3/latest/userguide/mpuoverview.html)
#' in the *Amazon S3 User Guide*.
#' 
#' **Directory buckets** - For directory buckets, you must make requests
#' for this API operation to the Zonal endpoint. These endpoints support
#' virtual-hosted-style requests in the format
#' `https://amzn-s3-demo-bucket.s3express-zone-id.region-code.amazonaws.com/key-name `.
#' Path-style requests are not supported. For more information about
#' endpoints in Availability Zones, see [Regional and Zonal endpoints for
#' directory buckets in Availability
#' Zones](https://docs.aws.amazon.com/AmazonS3/latest/userguide/endpoint-directory-buckets-AZ.html)
#' in the *Amazon S3 User Guide*. For more information about endpoints in
#' Local Zones, see [Concepts for directory buckets in Local
#' Zones](https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-lzs-for-directory-buckets.html)
#' in the *Amazon S3 User Guide*.
#' 
#' ### Permissions
#' 
#' -   **General purpose bucket permissions** - For information about
#'     permissions required to use the multipart upload API, see [Multipart
#'     Upload and
#'     Permissions](https://docs.aws.amazon.com/AmazonS3/latest/userguide/mpuoverview.html)
#'     in the *Amazon S3 User Guide*.
#' 
#' -   **Directory bucket permissions** - To grant access to this API
#'     operation on a directory bucket, we recommend that you use the
#'     [`create_session`](https://docs.aws.amazon.com/AmazonS3/latest/API/API_CreateSession.html)
#'     API operation for session-based authorization. Specifically, you
#'     grant the `s3express:CreateSession` permission to the directory
#'     bucket in a bucket policy or an IAM identity-based policy. Then, you
#'     make the [`create_session`][s3_create_session] API call on the
#'     bucket to obtain a session token. With the session token in your
#'     request header, you can make API requests to this operation. After
#'     the session token expires, you make another
#'     [`create_session`][s3_create_session] API call to generate a new
#'     session token for use. Amazon Web Services CLI or SDKs create
#'     session and refresh the session token automatically to avoid service
#'     interruptions when a session expires. For more information about
#'     authorization, see
#'     [`create_session`](https://docs.aws.amazon.com/AmazonS3/latest/API/API_CreateSession.html)
#'     .
#' 
#' ### Sorting of multipart uploads in response
#' 
#' -   **General purpose bucket** - In the
#'     [`list_multipart_uploads`][s3_list_multipart_uploads] response, the
#'     multipart uploads are sorted based on two criteria:
#' 
#'     -   Key-based sorting - Multipart uploads are initially sorted in
#'         ascending order based on their object keys.
#' 
#'     -   Time-based sorting - For uploads that share the same object key,
#'         they are further sorted in ascending order based on the upload
#'         initiation time. Among uploads with the same key, the one that
#'         was initiated first will appear before the ones that were
#'         initiated later.
#' 
#' -   **Directory bucket** - In the
#'     [`list_multipart_uploads`][s3_list_multipart_uploads] response, the
#'     multipart uploads aren't sorted lexicographically based on the
#'     object keys.
#' 
#' ### HTTP Host header syntax
#' 
#' **Directory buckets** - The HTTP Host header syntax is
#' ` Bucket-name.s3express-zone-id.region-code.amazonaws.com`.
#' 
#' The following operations are related to
#' [`list_multipart_uploads`][s3_list_multipart_uploads]:
#' 
#' -   [`create_multipart_upload`][s3_create_multipart_upload]
#' 
#' -   [`upload_part`][s3_upload_part]
#' 
#' -   [`complete_multipart_upload`][s3_complete_multipart_upload]
#' 
#' -   [`list_parts`][s3_list_parts]
#' 
#' -   [`abort_multipart_upload`][s3_abort_multipart_upload]
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#'
#' @usage
#' s3_list_multipart_uploads(Bucket, Delimiter, EncodingType, KeyMarker,
#'   MaxUploads, Prefix, UploadIdMarker, ExpectedBucketOwner, RequestPayer)
#'
#' @param Bucket &#91;required&#93; The name of the bucket to which the multipart upload was initiated.
#' 
#' **Directory buckets** - When you use this operation with a directory
#' bucket, you must use virtual-hosted-style requests in the format
#' ` Bucket-name.s3express-zone-id.region-code.amazonaws.com`. Path-style
#' requests are not supported. Directory bucket names must be unique in the
#' chosen Zone (Availability Zone or Local Zone). Bucket names must follow
#' the format ` bucket-base-name--zone-id--x-s3` (for example,
#' ` amzn-s3-demo-bucket--usw2-az1--x-s3`). For information about bucket
#' naming restrictions, see [Directory bucket naming
#' rules](https://docs.aws.amazon.com/AmazonS3/latest/userguide/directory-bucket-naming-rules.html)
#' in the *Amazon S3 User Guide*.
#' 
#' **Access points** - When you use this action with an access point for
#' general purpose buckets, you must provide the alias of the access point
#' in place of the bucket name or specify the access point ARN. When you
#' use this action with an access point for directory buckets, you must
#' provide the access point name in place of the bucket name. When using
#' the access point ARN, you must direct requests to the access point
#' hostname. The access point hostname takes the form
#' *AccessPointName*-*AccountId*.s3-accesspoint.*Region*.amazonaws.com.
#' When using this action with an access point through the Amazon Web
#' Services SDKs, you provide the access point ARN in place of the bucket
#' name. For more information about access point ARNs, see [Using access
#' points](https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-access-points.html)
#' in the *Amazon S3 User Guide*.
#' 
#' Object Lambda access points are not supported by directory buckets.
#' 
#' **S3 on Outposts** - When you use this action with S3 on Outposts, you
#' must direct requests to the S3 on Outposts hostname. The S3 on Outposts
#' hostname takes the form
#' ` AccessPointName-AccountId.outpostID.s3-outposts.Region.amazonaws.com`.
#' When you use this action with S3 on Outposts, the destination bucket
#' must be the Outposts access point ARN or the access point alias. For
#' more information about S3 on Outposts, see [What is S3 on
#' Outposts?](https://docs.aws.amazon.com/AmazonS3/latest/s3-outposts/S3onOutposts.html)
#' in the *Amazon S3 User Guide*.
#' @param Delimiter Character you use to group keys.
#' 
#' All keys that contain the same string between the prefix, if specified,
#' and the first occurrence of the delimiter after the prefix are grouped
#' under a single result element, `CommonPrefixes`. If you don't specify
#' the prefix parameter, then the substring starts at the beginning of the
#' key. The keys that are grouped under `CommonPrefixes` result element are
#' not returned elsewhere in the response.
#' 
#' `CommonPrefixes` is filtered out from results if it is not
#' lexicographically greater than the key-marker.
#' 
#' **Directory buckets** - For directory buckets, `/` is the only supported
#' delimiter.
#' @param EncodingType 
#' @param KeyMarker Specifies the multipart upload after which listing should begin.
#' 
#' -   **General purpose buckets** - For general purpose buckets,
#'     `key-marker` is an object key. Together with `upload-id-marker`,
#'     this parameter specifies the multipart upload after which listing
#'     should begin.
#' 
#'     If `upload-id-marker` is not specified, only the keys
#'     lexicographically greater than the specified `key-marker` will be
#'     included in the list.
#' 
#'     If `upload-id-marker` is specified, any multipart uploads for a key
#'     equal to the `key-marker` might also be included, provided those
#'     multipart uploads have upload IDs lexicographically greater than the
#'     specified `upload-id-marker`.
#' 
#' -   **Directory buckets** - For directory buckets, `key-marker` is
#'     obfuscated and isn't a real object key. The `upload-id-marker`
#'     parameter isn't supported by directory buckets. To list the
#'     additional multipart uploads, you only need to set the value of
#'     `key-marker` to the `NextKeyMarker` value from the previous
#'     response.
#' 
#'     In the [`list_multipart_uploads`][s3_list_multipart_uploads]
#'     response, the multipart uploads aren't sorted lexicographically
#'     based on the object keys.
#' @param MaxUploads Sets the maximum number of multipart uploads, from 1 to 1,000, to return
#' in the response body. 1,000 is the maximum number of uploads that can be
#' returned in a response.
#' @param Prefix Lists in-progress uploads only for those keys that begin with the
#' specified prefix. You can use prefixes to separate a bucket into
#' different grouping of keys. (You can think of using `prefix` to make
#' groups in the same way that you'd use a folder in a file system.)
#' 
#' **Directory buckets** - For directory buckets, only prefixes that end in
#' a delimiter (`/`) are supported.
#' @param UploadIdMarker Together with key-marker, specifies the multipart upload after which
#' listing should begin. If key-marker is not specified, the
#' upload-id-marker parameter is ignored. Otherwise, any multipart uploads
#' for a key equal to the key-marker might be included in the list only if
#' they have an upload ID lexicographically greater than the specified
#' `upload-id-marker`.
#' 
#' This functionality is not supported for directory buckets.
#' @param ExpectedBucketOwner The account ID of the expected bucket owner. If the account ID that you
#' provide does not match the actual owner of the bucket, the request fails
#' with the HTTP status code `403 Forbidden` (access denied).
#' @param RequestPayer 
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   Bucket = "string",
#'   KeyMarker = "string",
#'   UploadIdMarker = "string",
#'   NextKeyMarker = "string",
#'   Prefix = "string",
#'   Delimiter = "string",
#'   NextUploadIdMarker = "string",
#'   MaxUploads = 123,
#'   IsTruncated = TRUE|FALSE,
#'   Uploads = list(
#'     list(
#'       UploadId = "string",
#'       Key = "string",
#'       Initiated = as.POSIXct(
#'         "2015-01-01"
#'       ),
#'       StorageClass = "STANDARD"|"REDUCED_REDUNDANCY"|"STANDARD_IA"|"ONEZONE_IA"|"INTELLIGENT_TIERING"|"GLACIER"|"DEEP_ARCHIVE"|"OUTPOSTS"|"GLACIER_IR"|"SNOW"|"EXPRESS_ONEZONE"|"FSX_OPENZFS"|"FSX_ONTAP",
#'       Owner = list(
#'         DisplayName = "string",
#'         ID = "string"
#'       ),
#'       Initiator = list(
#'         ID = "string",
#'         DisplayName = "string"
#'       ),
#'       ChecksumAlgorithm = "CRC32"|"CRC32C"|"SHA1"|"SHA256"|"CRC64NVME",
#'       ChecksumType = "COMPOSITE"|"FULL_OBJECT"
#'     )
#'   ),
#'   CommonPrefixes = list(
#'     list(
#'       Prefix = "string"
#'     )
#'   ),
#'   EncodingType = "url",
#'   RequestCharged = "requester"
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$list_multipart_uploads(
#'   Bucket = "string",
#'   Delimiter = "string",
#'   EncodingType = "url",
#'   KeyMarker = "string",
#'   MaxUploads = 123,
#'   Prefix = "string",
#'   UploadIdMarker = "string",
#'   ExpectedBucketOwner = "string",
#'   RequestPayer = "requester"
#' )
#' ```
#'
#' @examples
#' \dontrun{
#' # The following example lists in-progress multipart uploads on a specific
#' # bucket.
#' svc$list_multipart_uploads(
#'   Bucket = "examplebucket"
#' )
#' 
#' # The following example specifies the upload-id-marker and key-marker from
#' # previous truncated response to retrieve next setup of multipart uploads.
#' svc$list_multipart_uploads(
#'   Bucket = "examplebucket",
#'   KeyMarker = "nextkeyfrompreviousresponse",
#'   MaxUploads = "2",
#'   UploadIdMarker = "valuefrompreviousresponse"
#' )
#' }
#'
#' @keywords internal
#'
#' @rdname s3_list_multipart_uploads
#'
#' @aliases s3_list_multipart_uploads
s3_list_multipart_uploads <- function(Bucket, Delimiter = NULL, EncodingType = NULL, KeyMarker = NULL, MaxUploads = NULL, Prefix = NULL, UploadIdMarker = NULL, ExpectedBucketOwner = NULL, RequestPayer = NULL) {
  op <- new_operation(
    name = "ListMultipartUploadsRequest",
    http_method = "GET",
    http_path = "/{Bucket}?uploads",
    host_prefix = "",
    paginator = list(limit_key = "MaxUploads", more_results = "IsTruncated", output_token = c("NextKeyMarker", "NextUploadIdMarker"), input_token = list("KeyMarker", "UploadIdMarker"), result_key = list( "Uploads", "CommonPrefixes")),
    stream_api = FALSE
  )
  input <- .s3$list_multipart_uploads_input(Bucket = Bucket, Delimiter = Delimiter, EncodingType = EncodingType, KeyMarker = KeyMarker, MaxUploads = MaxUploads, Prefix = Prefix, UploadIdMarker = UploadIdMarker, ExpectedBucketOwner = ExpectedBucketOwner, RequestPayer = RequestPayer)
  output <- .s3$list_multipart_uploads_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$list_multipart_uploads <- s3_list_multipart_uploads

#' This operation is not supported for directory buckets
#'
#' @description
#' This operation is not supported for directory buckets.
#' 
#' Returns metadata about all versions of the objects in a bucket. You can
#' also use request parameters as selection criteria to return metadata
#' about a subset of all the object versions.
#' 
#' To use this operation, you must have permission to perform the
#' `s3:ListBucketVersions` action. Be aware of the name difference.
#' 
#' A `200 OK` response can contain valid or invalid XML. Make sure to
#' design your application to parse the contents of the response and handle
#' it appropriately.
#' 
#' To use this operation, you must have READ access to the bucket.
#' 
#' The following operations are related to
#' [`list_object_versions`][s3_list_object_versions]:
#' 
#' -   [`list_objects_v2`][s3_list_objects_v2]
#' 
#' -   [`get_object`][s3_get_object]
#' 
#' -   [`put_object`][s3_put_object]
#' 
#' -   [`delete_object`][s3_delete_object]
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#'
#' @usage
#' s3_list_object_versions(Bucket, Delimiter, EncodingType, KeyMarker,
#'   MaxKeys, Prefix, VersionIdMarker, ExpectedBucketOwner, RequestPayer,
#'   OptionalObjectAttributes)
#'
#' @param Bucket &#91;required&#93; The bucket name that contains the objects.
#' @param Delimiter A delimiter is a character that you specify to group keys. All keys that
#' contain the same string between the `prefix` and the first occurrence of
#' the delimiter are grouped under a single result element in
#' `CommonPrefixes`. These groups are counted as one result against the
#' `max-keys` limitation. These keys are not returned elsewhere in the
#' response.
#' 
#' `CommonPrefixes` is filtered out from results if it is not
#' lexicographically greater than the key-marker.
#' @param EncodingType 
#' @param KeyMarker Specifies the key to start with when listing objects in a bucket.
#' @param MaxKeys Sets the maximum number of keys returned in the response. By default,
#' the action returns up to 1,000 key names. The response might contain
#' fewer keys but will never contain more. If additional keys satisfy the
#' search criteria, but were not returned because `max-keys` was exceeded,
#' the response contains `<isTruncated>true</isTruncated>`. To return the
#' additional keys, see `key-marker` and `version-id-marker`.
#' @param Prefix Use this parameter to select only those keys that begin with the
#' specified prefix. You can use prefixes to separate a bucket into
#' different groupings of keys. (You can think of using `prefix` to make
#' groups in the same way that you'd use a folder in a file system.) You
#' can use `prefix` with `delimiter` to roll up numerous objects into a
#' single result under `CommonPrefixes`.
#' @param VersionIdMarker Specifies the object version you want to start listing from.
#' @param ExpectedBucketOwner The account ID of the expected bucket owner. If the account ID that you
#' provide does not match the actual owner of the bucket, the request fails
#' with the HTTP status code `403 Forbidden` (access denied).
#' @param RequestPayer 
#' @param OptionalObjectAttributes Specifies the optional fields that you want returned in the response.
#' Fields that you do not specify are not returned.
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   IsTruncated = TRUE|FALSE,
#'   KeyMarker = "string",
#'   VersionIdMarker = "string",
#'   NextKeyMarker = "string",
#'   NextVersionIdMarker = "string",
#'   Versions = list(
#'     list(
#'       ETag = "string",
#'       ChecksumAlgorithm = list(
#'         "CRC32"|"CRC32C"|"SHA1"|"SHA256"|"CRC64NVME"
#'       ),
#'       ChecksumType = "COMPOSITE"|"FULL_OBJECT",
#'       Size = 123,
#'       StorageClass = "STANDARD",
#'       Key = "string",
#'       VersionId = "string",
#'       IsLatest = TRUE|FALSE,
#'       LastModified = as.POSIXct(
#'         "2015-01-01"
#'       ),
#'       Owner = list(
#'         DisplayName = "string",
#'         ID = "string"
#'       ),
#'       RestoreStatus = list(
#'         IsRestoreInProgress = TRUE|FALSE,
#'         RestoreExpiryDate = as.POSIXct(
#'           "2015-01-01"
#'         )
#'       )
#'     )
#'   ),
#'   DeleteMarkers = list(
#'     list(
#'       Owner = list(
#'         DisplayName = "string",
#'         ID = "string"
#'       ),
#'       Key = "string",
#'       VersionId = "string",
#'       IsLatest = TRUE|FALSE,
#'       LastModified = as.POSIXct(
#'         "2015-01-01"
#'       )
#'     )
#'   ),
#'   Name = "string",
#'   Prefix = "string",
#'   Delimiter = "string",
#'   MaxKeys = 123,
#'   CommonPrefixes = list(
#'     list(
#'       Prefix = "string"
#'     )
#'   ),
#'   EncodingType = "url",
#'   RequestCharged = "requester"
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$list_object_versions(
#'   Bucket = "string",
#'   Delimiter = "string",
#'   EncodingType = "url",
#'   KeyMarker = "string",
#'   MaxKeys = 123,
#'   Prefix = "string",
#'   VersionIdMarker = "string",
#'   ExpectedBucketOwner = "string",
#'   RequestPayer = "requester",
#'   OptionalObjectAttributes = list(
#'     "RestoreStatus"
#'   )
#' )
#' ```
#'
#' @examples
#' \dontrun{
#' # The following example return versions of an object with specific key
#' # name prefix. The request limits the number of items returned to two. If
#' # there are are more than two object version, S3 returns NextToken in the
#' # response. You can specify this token value in your next request to fetch
#' # next set of object versions.
#' svc$list_object_versions(
#'   Bucket = "examplebucket",
#'   Prefix = "HappyFace.jpg"
#' )
#' }
#'
#' @keywords internal
#'
#' @rdname s3_list_object_versions
#'
#' @aliases s3_list_object_versions
s3_list_object_versions <- function(Bucket, Delimiter = NULL, EncodingType = NULL, KeyMarker = NULL, MaxKeys = NULL, Prefix = NULL, VersionIdMarker = NULL, ExpectedBucketOwner = NULL, RequestPayer = NULL, OptionalObjectAttributes = NULL) {
  op <- new_operation(
    name = "ListObjectVersions",
    http_method = "GET",
    http_path = "/{Bucket}?versions",
    host_prefix = "",
    paginator = list(more_results = "IsTruncated", limit_key = "MaxKeys", output_token = c("NextKeyMarker", "NextVersionIdMarker"), input_token = list("KeyMarker", "VersionIdMarker"), result_key = list("Versions", "DeleteMarkers", "CommonPrefixes")),
    stream_api = FALSE
  )
  input <- .s3$list_object_versions_input(Bucket = Bucket, Delimiter = Delimiter, EncodingType = EncodingType, KeyMarker = KeyMarker, MaxKeys = MaxKeys, Prefix = Prefix, VersionIdMarker = VersionIdMarker, ExpectedBucketOwner = ExpectedBucketOwner, RequestPayer = RequestPayer, OptionalObjectAttributes = OptionalObjectAttributes)
  output <- .s3$list_object_versions_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$list_object_versions <- s3_list_object_versions

#' This operation is not supported for directory buckets
#'
#' @description
#' This operation is not supported for directory buckets.
#' 
#' Returns some or all (up to 1,000) of the objects in a bucket. You can
#' use the request parameters as selection criteria to return a subset of
#' the objects in a bucket. A 200 OK response can contain valid or invalid
#' XML. Be sure to design your application to parse the contents of the
#' response and handle it appropriately.
#' 
#' This action has been revised. We recommend that you use the newer
#' version, [`list_objects_v2`][s3_list_objects_v2], when developing
#' applications. For backward compatibility, Amazon S3 continues to support
#' [`list_objects`][s3_list_objects].
#' 
#' The following operations are related to
#' [`list_objects`][s3_list_objects]:
#' 
#' -   [`list_objects_v2`][s3_list_objects_v2]
#' 
#' -   [`get_object`][s3_get_object]
#' 
#' -   [`put_object`][s3_put_object]
#' 
#' -   [`create_bucket`][s3_create_bucket]
#' 
#' -   [`list_buckets`][s3_list_buckets]
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#'
#' @usage
#' s3_list_objects(Bucket, Delimiter, EncodingType, Marker, MaxKeys,
#'   Prefix, RequestPayer, ExpectedBucketOwner, OptionalObjectAttributes)
#'
#' @param Bucket &#91;required&#93; The name of the bucket containing the objects.
#' 
#' **Directory buckets** - When you use this operation with a directory
#' bucket, you must use virtual-hosted-style requests in the format
#' ` Bucket-name.s3express-zone-id.region-code.amazonaws.com`. Path-style
#' requests are not supported. Directory bucket names must be unique in the
#' chosen Zone (Availability Zone or Local Zone). Bucket names must follow
#' the format ` bucket-base-name--zone-id--x-s3` (for example,
#' ` amzn-s3-demo-bucket--usw2-az1--x-s3`). For information about bucket
#' naming restrictions, see [Directory bucket naming
#' rules](https://docs.aws.amazon.com/AmazonS3/latest/userguide/directory-bucket-naming-rules.html)
#' in the *Amazon S3 User Guide*.
#' 
#' **Access points** - When you use this action with an access point for
#' general purpose buckets, you must provide the alias of the access point
#' in place of the bucket name or specify the access point ARN. When you
#' use this action with an access point for directory buckets, you must
#' provide the access point name in place of the bucket name. When using
#' the access point ARN, you must direct requests to the access point
#' hostname. The access point hostname takes the form
#' *AccessPointName*-*AccountId*.s3-accesspoint.*Region*.amazonaws.com.
#' When using this action with an access point through the Amazon Web
#' Services SDKs, you provide the access point ARN in place of the bucket
#' name. For more information about access point ARNs, see [Using access
#' points](https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-access-points.html)
#' in the *Amazon S3 User Guide*.
#' 
#' Object Lambda access points are not supported by directory buckets.
#' 
#' **S3 on Outposts** - When you use this action with S3 on Outposts, you
#' must direct requests to the S3 on Outposts hostname. The S3 on Outposts
#' hostname takes the form
#' ` AccessPointName-AccountId.outpostID.s3-outposts.Region.amazonaws.com`.
#' When you use this action with S3 on Outposts, the destination bucket
#' must be the Outposts access point ARN or the access point alias. For
#' more information about S3 on Outposts, see [What is S3 on
#' Outposts?](https://docs.aws.amazon.com/AmazonS3/latest/s3-outposts/S3onOutposts.html)
#' in the *Amazon S3 User Guide*.
#' @param Delimiter A delimiter is a character that you use to group keys.
#' 
#' `CommonPrefixes` is filtered out from results if it is not
#' lexicographically greater than the key-marker.
#' @param EncodingType 
#' @param Marker Marker is where you want Amazon S3 to start listing from. Amazon S3
#' starts listing after this specified key. Marker can be any key in the
#' bucket.
#' @param MaxKeys Sets the maximum number of keys returned in the response. By default,
#' the action returns up to 1,000 key names. The response might contain
#' fewer keys but will never contain more.
#' @param Prefix Limits the response to keys that begin with the specified prefix.
#' @param RequestPayer Confirms that the requester knows that she or he will be charged for the
#' list objects request. Bucket owners need not specify this parameter in
#' their requests.
#' @param ExpectedBucketOwner The account ID of the expected bucket owner. If the account ID that you
#' provide does not match the actual owner of the bucket, the request fails
#' with the HTTP status code `403 Forbidden` (access denied).
#' @param OptionalObjectAttributes Specifies the optional fields that you want returned in the response.
#' Fields that you do not specify are not returned.
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   IsTruncated = TRUE|FALSE,
#'   Marker = "string",
#'   NextMarker = "string",
#'   Contents = list(
#'     list(
#'       Key = "string",
#'       LastModified = as.POSIXct(
#'         "2015-01-01"
#'       ),
#'       ETag = "string",
#'       ChecksumAlgorithm = list(
#'         "CRC32"|"CRC32C"|"SHA1"|"SHA256"|"CRC64NVME"
#'       ),
#'       ChecksumType = "COMPOSITE"|"FULL_OBJECT",
#'       Size = 123,
#'       StorageClass = "STANDARD"|"REDUCED_REDUNDANCY"|"GLACIER"|"STANDARD_IA"|"ONEZONE_IA"|"INTELLIGENT_TIERING"|"DEEP_ARCHIVE"|"OUTPOSTS"|"GLACIER_IR"|"SNOW"|"EXPRESS_ONEZONE"|"FSX_OPENZFS"|"FSX_ONTAP",
#'       Owner = list(
#'         DisplayName = "string",
#'         ID = "string"
#'       ),
#'       RestoreStatus = list(
#'         IsRestoreInProgress = TRUE|FALSE,
#'         RestoreExpiryDate = as.POSIXct(
#'           "2015-01-01"
#'         )
#'       )
#'     )
#'   ),
#'   Name = "string",
#'   Prefix = "string",
#'   Delimiter = "string",
#'   MaxKeys = 123,
#'   CommonPrefixes = list(
#'     list(
#'       Prefix = "string"
#'     )
#'   ),
#'   EncodingType = "url",
#'   RequestCharged = "requester"
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$list_objects(
#'   Bucket = "string",
#'   Delimiter = "string",
#'   EncodingType = "url",
#'   Marker = "string",
#'   MaxKeys = 123,
#'   Prefix = "string",
#'   RequestPayer = "requester",
#'   ExpectedBucketOwner = "string",
#'   OptionalObjectAttributes = list(
#'     "RestoreStatus"
#'   )
#' )
#' ```
#'
#' @examples
#' \dontrun{
#' # The following example list two objects in a bucket.
#' svc$list_objects(
#'   Bucket = "examplebucket",
#'   MaxKeys = "2"
#' )
#' }
#'
#' @keywords internal
#'
#' @rdname s3_list_objects
#'
#' @aliases s3_list_objects
s3_list_objects <- function(Bucket, Delimiter = NULL, EncodingType = NULL, Marker = NULL, MaxKeys = NULL, Prefix = NULL, RequestPayer = NULL, ExpectedBucketOwner = NULL, OptionalObjectAttributes = NULL) {
  op <- new_operation(
    name = "ListObjects",
    http_method = "GET",
    http_path = "/{Bucket}",
    host_prefix = "",
    paginator = list(more_results = "IsTruncated", limit_key = "MaxKeys", output_token = c("NextMarker", "Contents[-1].Key"), input_token = c("Marker", "Marker"), result_key = list( "Contents", "CommonPrefixes")),
    stream_api = FALSE
  )
  input <- .s3$list_objects_input(Bucket = Bucket, Delimiter = Delimiter, EncodingType = EncodingType, Marker = Marker, MaxKeys = MaxKeys, Prefix = Prefix, RequestPayer = RequestPayer, ExpectedBucketOwner = ExpectedBucketOwner, OptionalObjectAttributes = OptionalObjectAttributes)
  output <- .s3$list_objects_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$list_objects <- s3_list_objects

#' Returns some or all (up to 1,000) of the objects in a bucket with each
#' request
#'
#' @description
#' Returns some or all (up to 1,000) of the objects in a bucket with each
#' request. You can use the request parameters as selection criteria to
#' return a subset of the objects in a bucket. A `200 OK` response can
#' contain valid or invalid XML. Make sure to design your application to
#' parse the contents of the response and handle it appropriately. For more
#' information about listing objects, see [Listing object keys
#' programmatically](https://docs.aws.amazon.com/AmazonS3/latest/userguide/ListingKeysUsingAPIs.html)
#' in the *Amazon S3 User Guide*. To get a list of your buckets, see
#' [`list_buckets`][s3_list_buckets].
#' 
#' -   **General purpose bucket** - For general purpose buckets,
#'     [`list_objects_v2`][s3_list_objects_v2] doesn't return prefixes that
#'     are related only to in-progress multipart uploads.
#' 
#' -   **Directory buckets** - For directory buckets,
#'     [`list_objects_v2`][s3_list_objects_v2] response includes the
#'     prefixes that are related only to in-progress multipart uploads.
#' 
#' -   **Directory buckets** - For directory buckets, you must make
#'     requests for this API operation to the Zonal endpoint. These
#'     endpoints support virtual-hosted-style requests in the format
#'     `https://amzn-s3-demo-bucket.s3express-zone-id.region-code.amazonaws.com/key-name `.
#'     Path-style requests are not supported. For more information about
#'     endpoints in Availability Zones, see [Regional and Zonal endpoints
#'     for directory buckets in Availability
#'     Zones](https://docs.aws.amazon.com/AmazonS3/latest/userguide/endpoint-directory-buckets-AZ.html)
#'     in the *Amazon S3 User Guide*. For more information about endpoints
#'     in Local Zones, see [Concepts for directory buckets in Local
#'     Zones](https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-lzs-for-directory-buckets.html)
#'     in the *Amazon S3 User Guide*.
#' 
#' ### Permissions
#' 
#' -   **General purpose bucket permissions** - To use this operation, you
#'     must have READ access to the bucket. You must have permission to
#'     perform the `s3:ListBucket` action. The bucket owner has this
#'     permission by default and can grant this permission to others. For
#'     more information about permissions, see [Permissions Related to
#'     Bucket Subresource
#'     Operations](https://docs.aws.amazon.com/AmazonS3/latest/userguide/security_iam_service-with-iam.html#security_iam_service-with-iam-id-based-policies-actions)
#'     and [Managing Access Permissions to Your Amazon S3
#'     Resources](https://docs.aws.amazon.com/AmazonS3/latest/userguide/security-iam.html)
#'     in the *Amazon S3 User Guide*.
#' 
#' -   **Directory bucket permissions** - To grant access to this API
#'     operation on a directory bucket, we recommend that you use the
#'     [`create_session`](https://docs.aws.amazon.com/AmazonS3/latest/API/API_CreateSession.html)
#'     API operation for session-based authorization. Specifically, you
#'     grant the `s3express:CreateSession` permission to the directory
#'     bucket in a bucket policy or an IAM identity-based policy. Then, you
#'     make the [`create_session`][s3_create_session] API call on the
#'     bucket to obtain a session token. With the session token in your
#'     request header, you can make API requests to this operation. After
#'     the session token expires, you make another
#'     [`create_session`][s3_create_session] API call to generate a new
#'     session token for use. Amazon Web Services CLI or SDKs create
#'     session and refresh the session token automatically to avoid service
#'     interruptions when a session expires. For more information about
#'     authorization, see
#'     [`create_session`](https://docs.aws.amazon.com/AmazonS3/latest/API/API_CreateSession.html)
#'     .
#' 
#' ### Sorting order of returned objects
#' 
#' -   **General purpose bucket** - For general purpose buckets,
#'     [`list_objects_v2`][s3_list_objects_v2] returns objects in
#'     lexicographical order based on their key names.
#' 
#' -   **Directory bucket** - For directory buckets,
#'     [`list_objects_v2`][s3_list_objects_v2] does not return objects in
#'     lexicographical order.
#' 
#' ### HTTP Host header syntax
#' 
#' **Directory buckets** - The HTTP Host header syntax is
#' ` Bucket-name.s3express-zone-id.region-code.amazonaws.com`.
#' 
#' This section describes the latest revision of this action. We recommend
#' that you use this revised API operation for application development. For
#' backward compatibility, Amazon S3 continues to support the prior version
#' of this API operation, [`list_objects`][s3_list_objects].
#' 
#' The following operations are related to
#' [`list_objects_v2`][s3_list_objects_v2]:
#' 
#' -   [`get_object`][s3_get_object]
#' 
#' -   [`put_object`][s3_put_object]
#' 
#' -   [`create_bucket`][s3_create_bucket]
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#'
#' @usage
#' s3_list_objects_v2(Bucket, Delimiter, EncodingType, MaxKeys, Prefix,
#'   ContinuationToken, FetchOwner, StartAfter, RequestPayer,
#'   ExpectedBucketOwner, OptionalObjectAttributes)
#'
#' @param Bucket &#91;required&#93; **Directory buckets** - When you use this operation with a directory
#' bucket, you must use virtual-hosted-style requests in the format
#' ` Bucket-name.s3express-zone-id.region-code.amazonaws.com`. Path-style
#' requests are not supported. Directory bucket names must be unique in the
#' chosen Zone (Availability Zone or Local Zone). Bucket names must follow
#' the format ` bucket-base-name--zone-id--x-s3` (for example,
#' ` amzn-s3-demo-bucket--usw2-az1--x-s3`). For information about bucket
#' naming restrictions, see [Directory bucket naming
#' rules](https://docs.aws.amazon.com/AmazonS3/latest/userguide/directory-bucket-naming-rules.html)
#' in the *Amazon S3 User Guide*.
#' 
#' **Access points** - When you use this action with an access point for
#' general purpose buckets, you must provide the alias of the access point
#' in place of the bucket name or specify the access point ARN. When you
#' use this action with an access point for directory buckets, you must
#' provide the access point name in place of the bucket name. When using
#' the access point ARN, you must direct requests to the access point
#' hostname. The access point hostname takes the form
#' *AccessPointName*-*AccountId*.s3-accesspoint.*Region*.amazonaws.com.
#' When using this action with an access point through the Amazon Web
#' Services SDKs, you provide the access point ARN in place of the bucket
#' name. For more information about access point ARNs, see [Using access
#' points](https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-access-points.html)
#' in the *Amazon S3 User Guide*.
#' 
#' Object Lambda access points are not supported by directory buckets.
#' 
#' **S3 on Outposts** - When you use this action with S3 on Outposts, you
#' must direct requests to the S3 on Outposts hostname. The S3 on Outposts
#' hostname takes the form
#' ` AccessPointName-AccountId.outpostID.s3-outposts.Region.amazonaws.com`.
#' When you use this action with S3 on Outposts, the destination bucket
#' must be the Outposts access point ARN or the access point alias. For
#' more information about S3 on Outposts, see [What is S3 on
#' Outposts?](https://docs.aws.amazon.com/AmazonS3/latest/s3-outposts/S3onOutposts.html)
#' in the *Amazon S3 User Guide*.
#' @param Delimiter A delimiter is a character that you use to group keys.
#' 
#' `CommonPrefixes` is filtered out from results if it is not
#' lexicographically greater than the `StartAfter` value.
#' 
#' -   **Directory buckets** - For directory buckets, `/` is the only
#'     supported delimiter.
#' 
#' -   **Directory buckets** - When you query
#'     [`list_objects_v2`][s3_list_objects_v2] with a delimiter during
#'     in-progress multipart uploads, the `CommonPrefixes` response
#'     parameter contains the prefixes that are associated with the
#'     in-progress multipart uploads. For more information about multipart
#'     uploads, see [Multipart Upload
#'     Overview](https://docs.aws.amazon.com/AmazonS3/latest/userguide/mpuoverview.html)
#'     in the *Amazon S3 User Guide*.
#' @param EncodingType Encoding type used by Amazon S3 to encode the [object
#' keys](https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-keys.html)
#' in the response. Responses are encoded only in UTF-8. An object key can
#' contain any Unicode character. However, the XML 1.0 parser can't parse
#' certain characters, such as characters with an ASCII value from 0 to 10.
#' For characters that aren't supported in XML 1.0, you can add this
#' parameter to request that Amazon S3 encode the keys in the response. For
#' more information about characters to avoid in object key names, see
#' [Object key naming
#' guidelines](https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-keys.html#object-key-guidelines).
#' 
#' When using the URL encoding type, non-ASCII characters that are used in
#' an object's key name will be percent-encoded according to UTF-8 code
#' values. For example, the object `test_file(3).png` will appear as
#' `test_file%283%29.png`.
#' @param MaxKeys Sets the maximum number of keys returned in the response. By default,
#' the action returns up to 1,000 key names. The response might contain
#' fewer keys but will never contain more.
#' @param Prefix Limits the response to keys that begin with the specified prefix.
#' 
#' **Directory buckets** - For directory buckets, only prefixes that end in
#' a delimiter (`/`) are supported.
#' @param ContinuationToken `ContinuationToken` indicates to Amazon S3 that the list is being
#' continued on this bucket with a token. `ContinuationToken` is obfuscated
#' and is not a real key. You can use this `ContinuationToken` for
#' pagination of the list results.
#' @param FetchOwner The owner field is not present in
#' [`list_objects_v2`][s3_list_objects_v2] by default. If you want to
#' return the owner field with each key in the result, then set the
#' `FetchOwner` field to `true`.
#' 
#' **Directory buckets** - For directory buckets, the bucket owner is
#' returned as the object owner for all objects.
#' @param StartAfter StartAfter is where you want Amazon S3 to start listing from. Amazon S3
#' starts listing after this specified key. StartAfter can be any key in
#' the bucket.
#' 
#' This functionality is not supported for directory buckets.
#' @param RequestPayer Confirms that the requester knows that she or he will be charged for the
#' list objects request in V2 style. Bucket owners need not specify this
#' parameter in their requests.
#' 
#' This functionality is not supported for directory buckets.
#' @param ExpectedBucketOwner The account ID of the expected bucket owner. If the account ID that you
#' provide does not match the actual owner of the bucket, the request fails
#' with the HTTP status code `403 Forbidden` (access denied).
#' @param OptionalObjectAttributes Specifies the optional fields that you want returned in the response.
#' Fields that you do not specify are not returned.
#' 
#' This functionality is not supported for directory buckets.
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   IsTruncated = TRUE|FALSE,
#'   Contents = list(
#'     list(
#'       Key = "string",
#'       LastModified = as.POSIXct(
#'         "2015-01-01"
#'       ),
#'       ETag = "string",
#'       ChecksumAlgorithm = list(
#'         "CRC32"|"CRC32C"|"SHA1"|"SHA256"|"CRC64NVME"
#'       ),
#'       ChecksumType = "COMPOSITE"|"FULL_OBJECT",
#'       Size = 123,
#'       StorageClass = "STANDARD"|"REDUCED_REDUNDANCY"|"GLACIER"|"STANDARD_IA"|"ONEZONE_IA"|"INTELLIGENT_TIERING"|"DEEP_ARCHIVE"|"OUTPOSTS"|"GLACIER_IR"|"SNOW"|"EXPRESS_ONEZONE"|"FSX_OPENZFS"|"FSX_ONTAP",
#'       Owner = list(
#'         DisplayName = "string",
#'         ID = "string"
#'       ),
#'       RestoreStatus = list(
#'         IsRestoreInProgress = TRUE|FALSE,
#'         RestoreExpiryDate = as.POSIXct(
#'           "2015-01-01"
#'         )
#'       )
#'     )
#'   ),
#'   Name = "string",
#'   Prefix = "string",
#'   Delimiter = "string",
#'   MaxKeys = 123,
#'   CommonPrefixes = list(
#'     list(
#'       Prefix = "string"
#'     )
#'   ),
#'   EncodingType = "url",
#'   KeyCount = 123,
#'   ContinuationToken = "string",
#'   NextContinuationToken = "string",
#'   StartAfter = "string",
#'   RequestCharged = "requester"
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$list_objects_v2(
#'   Bucket = "string",
#'   Delimiter = "string",
#'   EncodingType = "url",
#'   MaxKeys = 123,
#'   Prefix = "string",
#'   ContinuationToken = "string",
#'   FetchOwner = TRUE|FALSE,
#'   StartAfter = "string",
#'   RequestPayer = "requester",
#'   ExpectedBucketOwner = "string",
#'   OptionalObjectAttributes = list(
#'     "RestoreStatus"
#'   )
#' )
#' ```
#'
#' @examples
#' \dontrun{
#' # The following example retrieves object list. The request specifies max
#' # keys to limit response to include only 2 object keys.
#' svc$list_objects_v2(
#'   Bucket = "examplebucket",
#'   MaxKeys = "2"
#' )
#' }
#'
#' @keywords internal
#'
#' @rdname s3_list_objects_v2
#'
#' @aliases s3_list_objects_v2
s3_list_objects_v2 <- function(Bucket, Delimiter = NULL, EncodingType = NULL, MaxKeys = NULL, Prefix = NULL, ContinuationToken = NULL, FetchOwner = NULL, StartAfter = NULL, RequestPayer = NULL, ExpectedBucketOwner = NULL, OptionalObjectAttributes = NULL) {
  op <- new_operation(
    name = "ListObjectsV2",
    http_method = "GET",
    http_path = "/{Bucket}?list-type=2",
    host_prefix = "",
    paginator = list(more_results = "IsTruncated", limit_key = "MaxKeys", output_token = "NextContinuationToken", input_token = "ContinuationToken", result_key = list("Contents", "CommonPrefixes")),
    stream_api = FALSE
  )
  input <- .s3$list_objects_v2_input(Bucket = Bucket, Delimiter = Delimiter, EncodingType = EncodingType, MaxKeys = MaxKeys, Prefix = Prefix, ContinuationToken = ContinuationToken, FetchOwner = FetchOwner, StartAfter = StartAfter, RequestPayer = RequestPayer, ExpectedBucketOwner = ExpectedBucketOwner, OptionalObjectAttributes = OptionalObjectAttributes)
  output <- .s3$list_objects_v2_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$list_objects_v2 <- s3_list_objects_v2

#' Lists the parts that have been uploaded for a specific multipart upload
#'
#' @description
#' Lists the parts that have been uploaded for a specific multipart upload.
#' 
#' To use this operation, you must provide the `upload ID` in the request.
#' You obtain this uploadID by sending the initiate multipart upload
#' request through [`create_multipart_upload`][s3_create_multipart_upload].
#' 
#' The [`list_parts`][s3_list_parts] request returns a maximum of 1,000
#' uploaded parts. The limit of 1,000 parts is also the default value. You
#' can restrict the number of parts in a response by specifying the
#' `max-parts` request parameter. If your multipart upload consists of more
#' than 1,000 parts, the response returns an `IsTruncated` field with the
#' value of `true`, and a `NextPartNumberMarker` element. To list remaining
#' uploaded parts, in subsequent [`list_parts`][s3_list_parts] requests,
#' include the `part-number-marker` query string parameter and set its
#' value to the `NextPartNumberMarker` field value from the previous
#' response.
#' 
#' For more information on multipart uploads, see [Uploading Objects Using
#' Multipart
#' Upload](https://docs.aws.amazon.com/AmazonS3/latest/userguide/mpuoverview.html)
#' in the *Amazon S3 User Guide*.
#' 
#' **Directory buckets** - For directory buckets, you must make requests
#' for this API operation to the Zonal endpoint. These endpoints support
#' virtual-hosted-style requests in the format
#' `https://amzn-s3-demo-bucket.s3express-zone-id.region-code.amazonaws.com/key-name `.
#' Path-style requests are not supported. For more information about
#' endpoints in Availability Zones, see [Regional and Zonal endpoints for
#' directory buckets in Availability
#' Zones](https://docs.aws.amazon.com/AmazonS3/latest/userguide/endpoint-directory-buckets-AZ.html)
#' in the *Amazon S3 User Guide*. For more information about endpoints in
#' Local Zones, see [Concepts for directory buckets in Local
#' Zones](https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-lzs-for-directory-buckets.html)
#' in the *Amazon S3 User Guide*.
#' 
#' ### Permissions
#' 
#' -   **General purpose bucket permissions** - For information about
#'     permissions required to use the multipart upload API, see [Multipart
#'     Upload and
#'     Permissions](https://docs.aws.amazon.com/AmazonS3/latest/userguide/mpuoverview.html)
#'     in the *Amazon S3 User Guide*.
#' 
#'     If the upload was created using server-side encryption with Key
#'     Management Service (KMS) keys (SSE-KMS) or dual-layer server-side
#'     encryption with Amazon Web Services KMS keys (DSSE-KMS), you must
#'     have permission to the `kms:Decrypt` action for the
#'     [`list_parts`][s3_list_parts] request to succeed.
#' 
#' -   **Directory bucket permissions** - To grant access to this API
#'     operation on a directory bucket, we recommend that you use the
#'     [`create_session`](https://docs.aws.amazon.com/AmazonS3/latest/API/API_CreateSession.html)
#'     API operation for session-based authorization. Specifically, you
#'     grant the `s3express:CreateSession` permission to the directory
#'     bucket in a bucket policy or an IAM identity-based policy. Then, you
#'     make the [`create_session`][s3_create_session] API call on the
#'     bucket to obtain a session token. With the session token in your
#'     request header, you can make API requests to this operation. After
#'     the session token expires, you make another
#'     [`create_session`][s3_create_session] API call to generate a new
#'     session token for use. Amazon Web Services CLI or SDKs create
#'     session and refresh the session token automatically to avoid service
#'     interruptions when a session expires. For more information about
#'     authorization, see
#'     [`create_session`](https://docs.aws.amazon.com/AmazonS3/latest/API/API_CreateSession.html)
#'     .
#' 
#' ### HTTP Host header syntax
#' 
#' **Directory buckets** - The HTTP Host header syntax is
#' ` Bucket-name.s3express-zone-id.region-code.amazonaws.com`.
#' 
#' The following operations are related to [`list_parts`][s3_list_parts]:
#' 
#' -   [`create_multipart_upload`][s3_create_multipart_upload]
#' 
#' -   [`upload_part`][s3_upload_part]
#' 
#' -   [`complete_multipart_upload`][s3_complete_multipart_upload]
#' 
#' -   [`abort_multipart_upload`][s3_abort_multipart_upload]
#' 
#' -   [`get_object_attributes`][s3_get_object_attributes]
#' 
#' -   [`list_multipart_uploads`][s3_list_multipart_uploads]
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#'
#' @usage
#' s3_list_parts(Bucket, Key, MaxParts, PartNumberMarker, UploadId,
#'   RequestPayer, ExpectedBucketOwner, SSECustomerAlgorithm, SSECustomerKey,
#'   SSECustomerKeyMD5)
#'
#' @param Bucket &#91;required&#93; The name of the bucket to which the parts are being uploaded.
#' 
#' **Directory buckets** - When you use this operation with a directory
#' bucket, you must use virtual-hosted-style requests in the format
#' ` Bucket-name.s3express-zone-id.region-code.amazonaws.com`. Path-style
#' requests are not supported. Directory bucket names must be unique in the
#' chosen Zone (Availability Zone or Local Zone). Bucket names must follow
#' the format ` bucket-base-name--zone-id--x-s3` (for example,
#' ` amzn-s3-demo-bucket--usw2-az1--x-s3`). For information about bucket
#' naming restrictions, see [Directory bucket naming
#' rules](https://docs.aws.amazon.com/AmazonS3/latest/userguide/directory-bucket-naming-rules.html)
#' in the *Amazon S3 User Guide*.
#' 
#' **Access points** - When you use this action with an access point for
#' general purpose buckets, you must provide the alias of the access point
#' in place of the bucket name or specify the access point ARN. When you
#' use this action with an access point for directory buckets, you must
#' provide the access point name in place of the bucket name. When using
#' the access point ARN, you must direct requests to the access point
#' hostname. The access point hostname takes the form
#' *AccessPointName*-*AccountId*.s3-accesspoint.*Region*.amazonaws.com.
#' When using this action with an access point through the Amazon Web
#' Services SDKs, you provide the access point ARN in place of the bucket
#' name. For more information about access point ARNs, see [Using access
#' points](https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-access-points.html)
#' in the *Amazon S3 User Guide*.
#' 
#' Object Lambda access points are not supported by directory buckets.
#' 
#' **S3 on Outposts** - When you use this action with S3 on Outposts, you
#' must direct requests to the S3 on Outposts hostname. The S3 on Outposts
#' hostname takes the form
#' ` AccessPointName-AccountId.outpostID.s3-outposts.Region.amazonaws.com`.
#' When you use this action with S3 on Outposts, the destination bucket
#' must be the Outposts access point ARN or the access point alias. For
#' more information about S3 on Outposts, see [What is S3 on
#' Outposts?](https://docs.aws.amazon.com/AmazonS3/latest/s3-outposts/S3onOutposts.html)
#' in the *Amazon S3 User Guide*.
#' @param Key &#91;required&#93; Object key for which the multipart upload was initiated.
#' @param MaxParts Sets the maximum number of parts to return.
#' @param PartNumberMarker Specifies the part after which listing should begin. Only parts with
#' higher part numbers will be listed.
#' @param UploadId &#91;required&#93; Upload ID identifying the multipart upload whose parts are being listed.
#' @param RequestPayer 
#' @param ExpectedBucketOwner The account ID of the expected bucket owner. If the account ID that you
#' provide does not match the actual owner of the bucket, the request fails
#' with the HTTP status code `403 Forbidden` (access denied).
#' @param SSECustomerAlgorithm The server-side encryption (SSE) algorithm used to encrypt the object.
#' This parameter is needed only when the object was created using a
#' checksum algorithm. For more information, see [Protecting data using
#' SSE-C
#' keys](https://docs.aws.amazon.com/AmazonS3/latest/userguide/ServerSideEncryptionCustomerKeys.html)
#' in the *Amazon S3 User Guide*.
#' 
#' This functionality is not supported for directory buckets.
#' @param SSECustomerKey The server-side encryption (SSE) customer managed key. This parameter is
#' needed only when the object was created using a checksum algorithm. For
#' more information, see [Protecting data using SSE-C
#' keys](https://docs.aws.amazon.com/AmazonS3/latest/userguide/ServerSideEncryptionCustomerKeys.html)
#' in the *Amazon S3 User Guide*.
#' 
#' This functionality is not supported for directory buckets.
#' @param SSECustomerKeyMD5 The MD5 server-side encryption (SSE) customer managed key. This
#' parameter is needed only when the object was created using a checksum
#' algorithm. For more information, see [Protecting data using SSE-C
#' keys](https://docs.aws.amazon.com/AmazonS3/latest/userguide/ServerSideEncryptionCustomerKeys.html)
#' in the *Amazon S3 User Guide*.
#' 
#' This functionality is not supported for directory buckets.
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   AbortDate = as.POSIXct(
#'     "2015-01-01"
#'   ),
#'   AbortRuleId = "string",
#'   Bucket = "string",
#'   Key = "string",
#'   UploadId = "string",
#'   PartNumberMarker = 123,
#'   NextPartNumberMarker = 123,
#'   MaxParts = 123,
#'   IsTruncated = TRUE|FALSE,
#'   Parts = list(
#'     list(
#'       PartNumber = 123,
#'       LastModified = as.POSIXct(
#'         "2015-01-01"
#'       ),
#'       ETag = "string",
#'       Size = 123,
#'       ChecksumCRC32 = "string",
#'       ChecksumCRC32C = "string",
#'       ChecksumCRC64NVME = "string",
#'       ChecksumSHA1 = "string",
#'       ChecksumSHA256 = "string"
#'     )
#'   ),
#'   Initiator = list(
#'     ID = "string",
#'     DisplayName = "string"
#'   ),
#'   Owner = list(
#'     DisplayName = "string",
#'     ID = "string"
#'   ),
#'   StorageClass = "STANDARD"|"REDUCED_REDUNDANCY"|"STANDARD_IA"|"ONEZONE_IA"|"INTELLIGENT_TIERING"|"GLACIER"|"DEEP_ARCHIVE"|"OUTPOSTS"|"GLACIER_IR"|"SNOW"|"EXPRESS_ONEZONE"|"FSX_OPENZFS"|"FSX_ONTAP",
#'   RequestCharged = "requester",
#'   ChecksumAlgorithm = "CRC32"|"CRC32C"|"SHA1"|"SHA256"|"CRC64NVME",
#'   ChecksumType = "COMPOSITE"|"FULL_OBJECT"
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$list_parts(
#'   Bucket = "string",
#'   Key = "string",
#'   MaxParts = 123,
#'   PartNumberMarker = 123,
#'   UploadId = "string",
#'   RequestPayer = "requester",
#'   ExpectedBucketOwner = "string",
#'   SSECustomerAlgorithm = "string",
#'   SSECustomerKey = "string",
#'   SSECustomerKeyMD5 = "string"
#' )
#' ```
#'
#' @examples
#' \dontrun{
#' # The following example lists parts uploaded for a specific multipart
#' # upload.
#' svc$list_parts(
#'   Bucket = "examplebucket",
#'   Key = "bigobject",
#'   UploadId = "example7YPBOJuoFiQ9cz4P3Pe6FIZwO4f7wN93uHsNBEw97pl5eNwzExg0LA..."
#' )
#' }
#'
#' @keywords internal
#'
#' @rdname s3_list_parts
#'
#' @aliases s3_list_parts
s3_list_parts <- function(Bucket, Key, MaxParts = NULL, PartNumberMarker = NULL, UploadId, RequestPayer = NULL, ExpectedBucketOwner = NULL, SSECustomerAlgorithm = NULL, SSECustomerKey = NULL, SSECustomerKeyMD5 = NULL) {
  op <- new_operation(
    name = "ListPartsRequest",
    http_method = "GET",
    http_path = "/{Bucket}/{Key+}",
    host_prefix = "",
    paginator = list(more_results = "IsTruncated", limit_key = "MaxParts", output_token = "NextPartNumberMarker", input_token = "PartNumberMarker", result_key = "Parts"),
    stream_api = FALSE
  )
  input <- .s3$list_parts_input(Bucket = Bucket, Key = Key, MaxParts = MaxParts, PartNumberMarker = PartNumberMarker, UploadId = UploadId, RequestPayer = RequestPayer, ExpectedBucketOwner = ExpectedBucketOwner, SSECustomerAlgorithm = SSECustomerAlgorithm, SSECustomerKey = SSECustomerKey, SSECustomerKeyMD5 = SSECustomerKeyMD5)
  output <- .s3$list_parts_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$list_parts <- s3_list_parts

#' Sets the attribute-based access control (ABAC) property of the general
#' purpose bucket
#'
#' @description
#' Sets the attribute-based access control (ABAC) property of the general
#' purpose bucket. You must have `s3:PutBucketABAC` permission to perform
#' this action. When you enable ABAC, you can use tags for access control
#' on your buckets. Additionally, when ABAC is enabled, you must use the
#' [TagResource](https://docs.aws.amazon.com/AmazonS3/latest/API/API_control_TagResource.html)
#' and
#' [UntagResource](https://docs.aws.amazon.com/AmazonS3/latest/API/API_control_UntagResource.html)
#' actions to manage tags on your buckets. You can nolonger use the
#' [`put_bucket_tagging`][s3_put_bucket_tagging] and
#' [`delete_bucket_tagging`][s3_delete_bucket_tagging] actions to tag your
#' bucket. For more information, see [Enabling ABAC in general purpose
#' buckets](https://docs.aws.amazon.com/AmazonS3/latest/userguide/buckets-tagging-enable-abac.html).
#'
#' @usage
#' s3_put_bucket_abac(Bucket, ContentMD5, ChecksumAlgorithm,
#'   ExpectedBucketOwner, AbacStatus)
#'
#' @param Bucket &#91;required&#93; The name of the general purpose bucket.
#' @param ContentMD5 The MD5 hash of the [`put_bucket_abac`][s3_put_bucket_abac] request
#' body.
#' 
#' For requests made using the Amazon Web Services Command Line Interface
#' (CLI) or Amazon Web Services SDKs, this field is calculated
#' automatically.
#' @param ChecksumAlgorithm Indicates the algorithm that you want Amazon S3 to use to create the
#' checksum. For more information, see [Checking object
#' integrity](https://docs.aws.amazon.com/AmazonS3/latest/userguide/checking-object-integrity.html)
#' in the *Amazon S3 User Guide*.
#' @param ExpectedBucketOwner The Amazon Web Services account ID of the general purpose bucket's
#' owner.
#' @param AbacStatus &#91;required&#93; The ABAC status of the general purpose bucket. When ABAC is enabled for
#' the general purpose bucket, you can use tags to manage access to the
#' general purpose buckets as well as for cost tracking purposes. When ABAC
#' is disabled for the general purpose buckets, you can only use tags for
#' cost tracking purposes. For more information, see [Using tags with S3
#' general purpose
#' buckets](https://docs.aws.amazon.com/AmazonS3/latest/userguide/buckets-tagging.html).
#'
#' @return
#' An empty list.
#'
#' @section Request syntax:
#' ```
#' svc$put_bucket_abac(
#'   Bucket = "string",
#'   ContentMD5 = "string",
#'   ChecksumAlgorithm = "CRC32"|"CRC32C"|"SHA1"|"SHA256"|"CRC64NVME",
#'   ExpectedBucketOwner = "string",
#'   AbacStatus = list(
#'     Status = "Enabled"|"Disabled"
#'   )
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname s3_put_bucket_abac
#'
#' @aliases s3_put_bucket_abac
s3_put_bucket_abac <- function(Bucket, ContentMD5 = NULL, ChecksumAlgorithm = NULL, ExpectedBucketOwner = NULL, AbacStatus) {
  op <- new_operation(
    name = "PutBucketAbac",
    http_method = "PUT",
    http_path = "/{Bucket}?abac",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .s3$put_bucket_abac_input(Bucket = Bucket, ContentMD5 = ContentMD5, ChecksumAlgorithm = ChecksumAlgorithm, ExpectedBucketOwner = ExpectedBucketOwner, AbacStatus = AbacStatus)
  output <- .s3$put_bucket_abac_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$put_bucket_abac <- s3_put_bucket_abac

#' This operation is not supported for directory buckets
#'
#' @description
#' This operation is not supported for directory buckets.
#' 
#' Sets the accelerate configuration of an existing bucket. Amazon S3
#' Transfer Acceleration is a bucket-level feature that enables you to
#' perform faster data transfers to Amazon S3.
#' 
#' To use this operation, you must have permission to perform the
#' `s3:PutAccelerateConfiguration` action. The bucket owner has this
#' permission by default. The bucket owner can grant this permission to
#' others. For more information about permissions, see [Permissions Related
#' to Bucket Subresource
#' Operations](https://docs.aws.amazon.com/AmazonS3/latest/userguide/security_iam_service-with-iam.html#security_iam_service-with-iam-id-based-policies-actions)
#' and [Managing Access Permissions to Your Amazon S3
#' Resources](https://docs.aws.amazon.com/AmazonS3/latest/userguide/security-iam.html).
#' 
#' The Transfer Acceleration state of a bucket can be set to one of the
#' following two values:
#' 
#' -   Enabled – Enables accelerated data transfers to the bucket.
#' 
#' -   Suspended – Disables accelerated data transfers to the bucket.
#' 
#' The
#' [`get_bucket_accelerate_configuration`][s3_get_bucket_accelerate_configuration]
#' action returns the transfer acceleration state of a bucket.
#' 
#' After setting the Transfer Acceleration state of a bucket to Enabled, it
#' might take up to thirty minutes before the data transfer rates to the
#' bucket increase.
#' 
#' The name of the bucket used for Transfer Acceleration must be
#' DNS-compliant and must not contain periods (".").
#' 
#' For more information about transfer acceleration, see [Transfer
#' Acceleration](https://docs.aws.amazon.com/AmazonS3/latest/userguide/transfer-acceleration.html).
#' 
#' The following operations are related to
#' [`put_bucket_accelerate_configuration`][s3_put_bucket_accelerate_configuration]:
#' 
#' -   [`get_bucket_accelerate_configuration`][s3_get_bucket_accelerate_configuration]
#' 
#' -   [`create_bucket`][s3_create_bucket]
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#'
#' @usage
#' s3_put_bucket_accelerate_configuration(Bucket, AccelerateConfiguration,
#'   ExpectedBucketOwner, ChecksumAlgorithm)
#'
#' @param Bucket &#91;required&#93; The name of the bucket for which the accelerate configuration is set.
#' @param AccelerateConfiguration &#91;required&#93; Container for setting the transfer acceleration state.
#' @param ExpectedBucketOwner The account ID of the expected bucket owner. If the account ID that you
#' provide does not match the actual owner of the bucket, the request fails
#' with the HTTP status code `403 Forbidden` (access denied).
#' @param ChecksumAlgorithm Indicates the algorithm used to create the checksum for the request when
#' you use the SDK. This header will not provide any additional
#' functionality if you don't use the SDK. When you send this header, there
#' must be a corresponding `x-amz-checksum` or `x-amz-trailer` header sent.
#' Otherwise, Amazon S3 fails the request with the HTTP status code
#' `400 Bad Request`. For more information, see [Checking object
#' integrity](https://docs.aws.amazon.com/AmazonS3/latest/userguide/checking-object-integrity.html)
#' in the *Amazon S3 User Guide*.
#' 
#' If you provide an individual checksum, Amazon S3 ignores any provided
#' `ChecksumAlgorithm` parameter.
#'
#' @return
#' An empty list.
#'
#' @section Request syntax:
#' ```
#' svc$put_bucket_accelerate_configuration(
#'   Bucket = "string",
#'   AccelerateConfiguration = list(
#'     Status = "Enabled"|"Suspended"
#'   ),
#'   ExpectedBucketOwner = "string",
#'   ChecksumAlgorithm = "CRC32"|"CRC32C"|"SHA1"|"SHA256"|"CRC64NVME"
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname s3_put_bucket_accelerate_configuration
#'
#' @aliases s3_put_bucket_accelerate_configuration
s3_put_bucket_accelerate_configuration <- function(Bucket, AccelerateConfiguration, ExpectedBucketOwner = NULL, ChecksumAlgorithm = NULL) {
  op <- new_operation(
    name = "PutBucketAccelerateConfiguration",
    http_method = "PUT",
    http_path = "/{Bucket}?accelerate",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .s3$put_bucket_accelerate_configuration_input(Bucket = Bucket, AccelerateConfiguration = AccelerateConfiguration, ExpectedBucketOwner = ExpectedBucketOwner, ChecksumAlgorithm = ChecksumAlgorithm)
  output <- .s3$put_bucket_accelerate_configuration_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$put_bucket_accelerate_configuration <- s3_put_bucket_accelerate_configuration

#' End of support notice: As of October 1, 2025, Amazon S3 has discontinued
#' support for Email Grantee Access Control Lists (ACLs)
#'
#' @description
#' End of support notice: As of October 1, 2025, Amazon S3 has discontinued
#' support for Email Grantee Access Control Lists (ACLs). If you attempt to
#' use an Email Grantee ACL in a request after October 1, 2025, the request
#' will receive an `HTTP 405` (Method Not Allowed) error.
#' 
#' This change affects the following Amazon Web Services Regions: US East
#' (N. Virginia), US West (N. California), US West (Oregon), Asia Pacific
#' (Singapore), Asia Pacific (Sydney), Asia Pacific (Tokyo), Europe
#' (Ireland), and South America (São Paulo).
#' 
#' This operation is not supported for directory buckets.
#' 
#' Sets the permissions on an existing bucket using access control lists
#' (ACL). For more information, see [Using
#' ACLs](https://docs.aws.amazon.com/AmazonS3/latest/userguide/acl-overview.html).
#' To set the ACL of a bucket, you must have the `WRITE_ACP` permission.
#' 
#' You can use one of the following two ways to set a bucket's permissions:
#' 
#' -   Specify the ACL in the request body
#' 
#' -   Specify permissions using request headers
#' 
#' You cannot specify access permission using both the body and the request
#' headers.
#' 
#' Depending on your application needs, you may choose to set the ACL on a
#' bucket using either the request body or the headers. For example, if you
#' have an existing application that updates a bucket ACL using the request
#' body, then you can continue to use that approach.
#' 
#' If your bucket uses the bucket owner enforced setting for S3 Object
#' Ownership, ACLs are disabled and no longer affect permissions. You must
#' use policies to grant access to your bucket and the objects in it.
#' Requests to set ACLs or update ACLs fail and return the
#' `AccessControlListNotSupported` error code. Requests to read ACLs are
#' still supported. For more information, see [Controlling object
#' ownership](https://docs.aws.amazon.com/AmazonS3/latest/userguide/about-object-ownership.html)
#' in the *Amazon S3 User Guide*.
#' 
#' ### Permissions
#' 
#' You can set access permissions by using one of the following methods:
#' 
#' -   Specify a canned ACL with the `x-amz-acl` request header. Amazon S3
#'     supports a set of predefined ACLs, known as *canned ACLs*. Each
#'     canned ACL has a predefined set of grantees and permissions. Specify
#'     the canned ACL name as the value of `x-amz-acl`. If you use this
#'     header, you cannot use other access control-specific headers in your
#'     request. For more information, see [Canned
#'     ACL](https://docs.aws.amazon.com/AmazonS3/latest/userguide/acl-overview.html#CannedACL).
#' 
#' -   Specify access permissions explicitly with the `x-amz-grant-read`,
#'     `x-amz-grant-read-acp`, `x-amz-grant-write-acp`, and
#'     `x-amz-grant-full-control` headers. When using these headers, you
#'     specify explicit access permissions and grantees (Amazon Web
#'     Services accounts or Amazon S3 groups) who will receive the
#'     permission. If you use these ACL-specific headers, you cannot use
#'     the `x-amz-acl` header to set a canned ACL. These parameters map to
#'     the set of permissions that Amazon S3 supports in an ACL. For more
#'     information, see [Access Control List (ACL)
#'     Overview](https://docs.aws.amazon.com/AmazonS3/latest/userguide/acl-overview.html).
#' 
#'     You specify each grantee as a type=value pair, where the type is one
#'     of the following:
#' 
#'     -   `id` – if the value specified is the canonical user ID of an
#'         Amazon Web Services account
#' 
#'     -   `uri` – if you are granting permissions to a predefined group
#' 
#'     -   `emailAddress` – if the value specified is the email address of
#'         an Amazon Web Services account
#' 
#'         Using email addresses to specify a grantee is only supported in
#'         the following Amazon Web Services Regions:
#' 
#'         -   US East (N. Virginia)
#' 
#'         -   US West (N. California)
#' 
#'         -   US West (Oregon)
#' 
#'         -   Asia Pacific (Singapore)
#' 
#'         -   Asia Pacific (Sydney)
#' 
#'         -   Asia Pacific (Tokyo)
#' 
#'         -   Europe (Ireland)
#' 
#'         -   South America (São Paulo)
#' 
#'         For a list of all the Amazon S3 supported Regions and endpoints,
#'         see [Regions and
#'         Endpoints](https://docs.aws.amazon.com/general/latest/gr/rande.html#s3_region)
#'         in the Amazon Web Services General Reference.
#' 
#'     For example, the following `x-amz-grant-write` header grants create,
#'     overwrite, and delete objects permission to LogDelivery group
#'     predefined by Amazon S3 and two Amazon Web Services accounts
#'     identified by their email addresses.
#' 
#'     `x-amz-grant-write: uri="http://acs.amazonaws.com/groups/s3/LogDelivery", id="111122223333", id="555566667777" `
#' 
#' You can use either a canned ACL or specify access permissions
#' explicitly. You cannot do both.
#' 
#' ### Grantee Values
#' 
#' You can specify the person (grantee) to whom you're assigning access
#' rights (using request elements) in the following ways. For examples of
#' how to specify these grantee values in JSON format, see the Amazon Web
#' Services CLI example in [Enabling Amazon S3 server access
#' logging](https://docs.aws.amazon.com/AmazonS3/latest/userguide/enable-server-access-logging.html)
#' in the *Amazon S3 User Guide*.
#' 
#' -   By the person's ID:
#' 
#'     `<Grantee xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:type="CanonicalUser"><ID><>ID<></ID><DisplayName><>GranteesEmail<></DisplayName> </Grantee>`
#' 
#'     DisplayName is optional and ignored in the request
#' 
#' -   By URI:
#' 
#'     `<Grantee xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:type="Group"><URI><>http://acs.amazonaws.com/groups/global/AuthenticatedUsers<></URI></Grantee>`
#' 
#' -   By Email address:
#' 
#'     `<Grantee xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:type="AmazonCustomerByEmail"><EmailAddress><>Grantees@@email.com<></EmailAddress>&</Grantee>`
#' 
#'     The grantee is resolved to the CanonicalUser and, in a response to a
#'     GET Object acl request, appears as the CanonicalUser.
#' 
#'     Using email addresses to specify a grantee is only supported in the
#'     following Amazon Web Services Regions:
#' 
#'     -   US East (N. Virginia)
#' 
#'     -   US West (N. California)
#' 
#'     -   US West (Oregon)
#' 
#'     -   Asia Pacific (Singapore)
#' 
#'     -   Asia Pacific (Sydney)
#' 
#'     -   Asia Pacific (Tokyo)
#' 
#'     -   Europe (Ireland)
#' 
#'     -   South America (São Paulo)
#' 
#'     For a list of all the Amazon S3 supported Regions and endpoints, see
#'     [Regions and
#'     Endpoints](https://docs.aws.amazon.com/general/latest/gr/rande.html#s3_region)
#'     in the Amazon Web Services General Reference.
#' 
#' The following operations are related to
#' [`put_bucket_acl`][s3_put_bucket_acl]:
#' 
#' -   [`create_bucket`][s3_create_bucket]
#' 
#' -   [`delete_bucket`][s3_delete_bucket]
#' 
#' -   [`get_object_acl`][s3_get_object_acl]
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#'
#' @usage
#' s3_put_bucket_acl(ACL, AccessControlPolicy, Bucket, ContentMD5,
#'   ChecksumAlgorithm, GrantFullControl, GrantRead, GrantReadACP,
#'   GrantWrite, GrantWriteACP, ExpectedBucketOwner)
#'
#' @param ACL The canned ACL to apply to the bucket.
#' @param AccessControlPolicy Contains the elements that set the ACL permissions for an object per
#' grantee.
#' @param Bucket &#91;required&#93; The bucket to which to apply the ACL.
#' @param ContentMD5 The Base64 encoded 128-bit `MD5` digest of the data. This header must be
#' used as a message integrity check to verify that the request body was
#' not corrupted in transit. For more information, go to [RFC
#' 1864.](https://www.ietf.org/rfc/rfc1864.txt)
#' 
#' For requests made using the Amazon Web Services Command Line Interface
#' (CLI) or Amazon Web Services SDKs, this field is calculated
#' automatically.
#' @param ChecksumAlgorithm Indicates the algorithm used to create the checksum for the request when
#' you use the SDK. This header will not provide any additional
#' functionality if you don't use the SDK. When you send this header, there
#' must be a corresponding `x-amz-checksum` or `x-amz-trailer` header sent.
#' Otherwise, Amazon S3 fails the request with the HTTP status code
#' `400 Bad Request`. For more information, see [Checking object
#' integrity](https://docs.aws.amazon.com/AmazonS3/latest/userguide/checking-object-integrity.html)
#' in the *Amazon S3 User Guide*.
#' 
#' If you provide an individual checksum, Amazon S3 ignores any provided
#' `ChecksumAlgorithm` parameter.
#' @param GrantFullControl Allows grantee the read, write, read ACP, and write ACP permissions on
#' the bucket.
#' @param GrantRead Allows grantee to list the objects in the bucket.
#' @param GrantReadACP Allows grantee to read the bucket ACL.
#' @param GrantWrite Allows grantee to create new objects in the bucket.
#' 
#' For the bucket and object owners of existing objects, also allows
#' deletions and overwrites of those objects.
#' @param GrantWriteACP Allows grantee to write the ACL for the applicable bucket.
#' @param ExpectedBucketOwner The account ID of the expected bucket owner. If the account ID that you
#' provide does not match the actual owner of the bucket, the request fails
#' with the HTTP status code `403 Forbidden` (access denied).
#'
#' @return
#' An empty list.
#'
#' @section Request syntax:
#' ```
#' svc$put_bucket_acl(
#'   ACL = "private"|"public-read"|"public-read-write"|"authenticated-read",
#'   AccessControlPolicy = list(
#'     Grants = list(
#'       list(
#'         Grantee = list(
#'           DisplayName = "string",
#'           EmailAddress = "string",
#'           ID = "string",
#'           Type = "CanonicalUser"|"AmazonCustomerByEmail"|"Group",
#'           URI = "string"
#'         ),
#'         Permission = "FULL_CONTROL"|"WRITE"|"WRITE_ACP"|"READ"|"READ_ACP"
#'       )
#'     ),
#'     Owner = list(
#'       DisplayName = "string",
#'       ID = "string"
#'     )
#'   ),
#'   Bucket = "string",
#'   ContentMD5 = "string",
#'   ChecksumAlgorithm = "CRC32"|"CRC32C"|"SHA1"|"SHA256"|"CRC64NVME",
#'   GrantFullControl = "string",
#'   GrantRead = "string",
#'   GrantReadACP = "string",
#'   GrantWrite = "string",
#'   GrantWriteACP = "string",
#'   ExpectedBucketOwner = "string"
#' )
#' ```
#'
#' @examples
#' \dontrun{
#' # The following example replaces existing ACL on a bucket. The ACL grants
#' # the bucket owner (specified using the owner ID) and write permission to
#' # the LogDelivery group. Because this is a replace operation, you must
#' # specify all the grants in your request. To incrementally add or remove
#' # ACL grants, you might use the console.
#' svc$put_bucket_acl(
#'   Bucket = "examplebucket",
#'   GrantFullControl = "id=examplee7a2f25102679df27bb0ae12b3f85be6f290b936c4393484",
#'   GrantWrite = "uri=http://acs.amazonaws.com/groups/s3/LogDelivery"
#' )
#' }
#'
#' @keywords internal
#'
#' @rdname s3_put_bucket_acl
#'
#' @aliases s3_put_bucket_acl
s3_put_bucket_acl <- function(ACL = NULL, AccessControlPolicy = NULL, Bucket, ContentMD5 = NULL, ChecksumAlgorithm = NULL, GrantFullControl = NULL, GrantRead = NULL, GrantReadACP = NULL, GrantWrite = NULL, GrantWriteACP = NULL, ExpectedBucketOwner = NULL) {
  op <- new_operation(
    name = "PutBucketAcl",
    http_method = "PUT",
    http_path = "/{Bucket}?acl",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .s3$put_bucket_acl_input(ACL = ACL, AccessControlPolicy = AccessControlPolicy, Bucket = Bucket, ContentMD5 = ContentMD5, ChecksumAlgorithm = ChecksumAlgorithm, GrantFullControl = GrantFullControl, GrantRead = GrantRead, GrantReadACP = GrantReadACP, GrantWrite = GrantWrite, GrantWriteACP = GrantWriteACP, ExpectedBucketOwner = ExpectedBucketOwner)
  output <- .s3$put_bucket_acl_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$put_bucket_acl <- s3_put_bucket_acl

#' This operation is not supported for directory buckets
#'
#' @description
#' This operation is not supported for directory buckets.
#' 
#' Sets an analytics configuration for the bucket (specified by the
#' analytics configuration ID). You can have up to 1,000 analytics
#' configurations per bucket.
#' 
#' You can choose to have storage class analysis export analysis reports
#' sent to a comma-separated values (CSV) flat file. See the `DataExport`
#' request element. Reports are updated daily and are based on the object
#' filters that you configure. When selecting data export, you specify a
#' destination bucket and an optional destination prefix where the file is
#' written. You can export the data to a destination bucket in a different
#' account. However, the destination bucket must be in the same Region as
#' the bucket that you are making the PUT analytics configuration to. For
#' more information, see [Amazon S3 Analytics – Storage Class
#' Analysis](https://docs.aws.amazon.com/AmazonS3/latest/userguide/analytics-storage-class.html).
#' 
#' You must create a bucket policy on the destination bucket where the
#' exported file is written to grant permissions to Amazon S3 to write
#' objects to the bucket. For an example policy, see [Granting Permissions
#' for Amazon S3 Inventory and Storage Class
#' Analysis](https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-bucket-policies.html#example-bucket-policies-use-case-9).
#' 
#' To use this operation, you must have permissions to perform the
#' `s3:PutAnalyticsConfiguration` action. The bucket owner has this
#' permission by default. The bucket owner can grant this permission to
#' others. For more information about permissions, see [Permissions Related
#' to Bucket Subresource
#' Operations](https://docs.aws.amazon.com/AmazonS3/latest/userguide/security_iam_service-with-iam.html#security_iam_service-with-iam-id-based-policies-actions)
#' and [Managing Access Permissions to Your Amazon S3
#' Resources](https://docs.aws.amazon.com/AmazonS3/latest/userguide/security-iam.html).
#' 
#' [`put_bucket_analytics_configuration`][s3_put_bucket_analytics_configuration]
#' has the following special errors:
#' 
#' -   -   *HTTP Error: HTTP 400 Bad Request*
#' 
#'     -   *Code: InvalidArgument*
#' 
#'     -   *Cause: Invalid argument.*
#' 
#' -   -   *HTTP Error: HTTP 400 Bad Request*
#' 
#'     -   *Code: TooManyConfigurations*
#' 
#'     -   *Cause: You are attempting to create a new configuration but
#'         have already reached the 1,000-configuration limit.*
#' 
#' -   -   *HTTP Error: HTTP 403 Forbidden*
#' 
#'     -   *Code: AccessDenied*
#' 
#'     -   *Cause: You are not the owner of the specified bucket, or you do
#'         not have the s3:PutAnalyticsConfiguration bucket permission to
#'         set the configuration on the bucket.*
#' 
#' The following operations are related to
#' [`put_bucket_analytics_configuration`][s3_put_bucket_analytics_configuration]:
#' 
#' -   [`get_bucket_analytics_configuration`][s3_get_bucket_analytics_configuration]
#' 
#' -   [`delete_bucket_analytics_configuration`][s3_delete_bucket_analytics_configuration]
#' 
#' -   [`list_bucket_analytics_configurations`][s3_list_bucket_analytics_configurations]
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#'
#' @usage
#' s3_put_bucket_analytics_configuration(Bucket, Id,
#'   AnalyticsConfiguration, ExpectedBucketOwner)
#'
#' @param Bucket &#91;required&#93; The name of the bucket to which an analytics configuration is stored.
#' @param Id &#91;required&#93; The ID that identifies the analytics configuration.
#' @param AnalyticsConfiguration &#91;required&#93; The configuration and any analyses for the analytics filter.
#' @param ExpectedBucketOwner The account ID of the expected bucket owner. If the account ID that you
#' provide does not match the actual owner of the bucket, the request fails
#' with the HTTP status code `403 Forbidden` (access denied).
#'
#' @return
#' An empty list.
#'
#' @section Request syntax:
#' ```
#' svc$put_bucket_analytics_configuration(
#'   Bucket = "string",
#'   Id = "string",
#'   AnalyticsConfiguration = list(
#'     Id = "string",
#'     Filter = list(
#'       Prefix = "string",
#'       Tag = list(
#'         Key = "string",
#'         Value = "string"
#'       ),
#'       And = list(
#'         Prefix = "string",
#'         Tags = list(
#'           list(
#'             Key = "string",
#'             Value = "string"
#'           )
#'         )
#'       )
#'     ),
#'     StorageClassAnalysis = list(
#'       DataExport = list(
#'         OutputSchemaVersion = "V_1",
#'         Destination = list(
#'           S3BucketDestination = list(
#'             Format = "CSV",
#'             BucketAccountId = "string",
#'             Bucket = "string",
#'             Prefix = "string"
#'           )
#'         )
#'       )
#'     )
#'   ),
#'   ExpectedBucketOwner = "string"
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname s3_put_bucket_analytics_configuration
#'
#' @aliases s3_put_bucket_analytics_configuration
s3_put_bucket_analytics_configuration <- function(Bucket, Id, AnalyticsConfiguration, ExpectedBucketOwner = NULL) {
  op <- new_operation(
    name = "PutBucketAnalyticsConfiguration",
    http_method = "PUT",
    http_path = "/{Bucket}?analytics",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .s3$put_bucket_analytics_configuration_input(Bucket = Bucket, Id = Id, AnalyticsConfiguration = AnalyticsConfiguration, ExpectedBucketOwner = ExpectedBucketOwner)
  output <- .s3$put_bucket_analytics_configuration_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$put_bucket_analytics_configuration <- s3_put_bucket_analytics_configuration

#' This operation is not supported for directory buckets
#'
#' @description
#' This operation is not supported for directory buckets.
#' 
#' Sets the `cors` configuration for your bucket. If the configuration
#' exists, Amazon S3 replaces it.
#' 
#' To use this operation, you must be allowed to perform the
#' `s3:PutBucketCORS` action. By default, the bucket owner has this
#' permission and can grant it to others.
#' 
#' You set this configuration on a bucket so that the bucket can service
#' cross-origin requests. For example, you might want to enable a request
#' whose origin is `http://www.example.com` to access your Amazon S3 bucket
#' at `my.example.bucket.com` by using the browser's `XMLHttpRequest`
#' capability.
#' 
#' To enable cross-origin resource sharing (CORS) on a bucket, you add the
#' `cors` subresource to the bucket. The `cors` subresource is an XML
#' document in which you configure rules that identify origins and the HTTP
#' methods that can be executed on your bucket. The document is limited to
#' 64 KB in size.
#' 
#' When Amazon S3 receives a cross-origin request (or a pre-flight OPTIONS
#' request) against a bucket, it evaluates the `cors` configuration on the
#' bucket and uses the first `CORSRule` rule that matches the incoming
#' browser request to enable a cross-origin request. For a rule to match,
#' the following conditions must be met:
#' 
#' -   The request's `Origin` header must match `AllowedOrigin` elements.
#' 
#' -   The request method (for example, GET, PUT, HEAD, and so on) or the
#'     `Access-Control-Request-Method` header in case of a pre-flight
#'     `OPTIONS` request must be one of the `AllowedMethod` elements.
#' 
#' -   Every header specified in the `Access-Control-Request-Headers`
#'     request header of a pre-flight request must match an `AllowedHeader`
#'     element.
#' 
#' For more information about CORS, go to [Enabling Cross-Origin Resource
#' Sharing](https://docs.aws.amazon.com/AmazonS3/latest/userguide/cors.html)
#' in the *Amazon S3 User Guide*.
#' 
#' The following operations are related to
#' [`put_bucket_cors`][s3_put_bucket_cors]:
#' 
#' -   [`get_bucket_cors`][s3_get_bucket_cors]
#' 
#' -   [`delete_bucket_cors`][s3_delete_bucket_cors]
#' 
#' -   [RESTOPTIONSobject](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTOPTIONSobject.html)
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#'
#' @usage
#' s3_put_bucket_cors(Bucket, CORSConfiguration, ContentMD5,
#'   ChecksumAlgorithm, ExpectedBucketOwner)
#'
#' @param Bucket &#91;required&#93; Specifies the bucket impacted by the `cors`configuration.
#' @param CORSConfiguration &#91;required&#93; Describes the cross-origin access configuration for objects in an Amazon
#' S3 bucket. For more information, see [Enabling Cross-Origin Resource
#' Sharing](https://docs.aws.amazon.com/AmazonS3/latest/userguide/cors.html)
#' in the *Amazon S3 User Guide*.
#' @param ContentMD5 The Base64 encoded 128-bit `MD5` digest of the data. This header must be
#' used as a message integrity check to verify that the request body was
#' not corrupted in transit. For more information, go to [RFC
#' 1864.](https://www.ietf.org/rfc/rfc1864.txt)
#' 
#' For requests made using the Amazon Web Services Command Line Interface
#' (CLI) or Amazon Web Services SDKs, this field is calculated
#' automatically.
#' @param ChecksumAlgorithm Indicates the algorithm used to create the checksum for the request when
#' you use the SDK. This header will not provide any additional
#' functionality if you don't use the SDK. When you send this header, there
#' must be a corresponding `x-amz-checksum` or `x-amz-trailer` header sent.
#' Otherwise, Amazon S3 fails the request with the HTTP status code
#' `400 Bad Request`. For more information, see [Checking object
#' integrity](https://docs.aws.amazon.com/AmazonS3/latest/userguide/checking-object-integrity.html)
#' in the *Amazon S3 User Guide*.
#' 
#' If you provide an individual checksum, Amazon S3 ignores any provided
#' `ChecksumAlgorithm` parameter.
#' @param ExpectedBucketOwner The account ID of the expected bucket owner. If the account ID that you
#' provide does not match the actual owner of the bucket, the request fails
#' with the HTTP status code `403 Forbidden` (access denied).
#'
#' @return
#' An empty list.
#'
#' @section Request syntax:
#' ```
#' svc$put_bucket_cors(
#'   Bucket = "string",
#'   CORSConfiguration = list(
#'     CORSRules = list(
#'       list(
#'         ID = "string",
#'         AllowedHeaders = list(
#'           "string"
#'         ),
#'         AllowedMethods = list(
#'           "string"
#'         ),
#'         AllowedOrigins = list(
#'           "string"
#'         ),
#'         ExposeHeaders = list(
#'           "string"
#'         ),
#'         MaxAgeSeconds = 123
#'       )
#'     )
#'   ),
#'   ContentMD5 = "string",
#'   ChecksumAlgorithm = "CRC32"|"CRC32C"|"SHA1"|"SHA256"|"CRC64NVME",
#'   ExpectedBucketOwner = "string"
#' )
#' ```
#'
#' @examples
#' \dontrun{
#' # The following example enables PUT, POST, and DELETE requests from
#' # www.example.com, and enables GET requests from any domain.
#' svc$put_bucket_cors(
#'   Bucket = "",
#'   CORSConfiguration = list(
#'     CORSRules = list(
#'       list(
#'         AllowedHeaders = list(
#'           "*"
#'         ),
#'         AllowedMethods = list(
#'           "PUT",
#'           "POST",
#'           "DELETE"
#'         ),
#'         AllowedOrigins = list(
#'           "http://www.example.com"
#'         ),
#'         ExposeHeaders = list(
#'           "x-amz-server-side-encryption"
#'         ),
#'         MaxAgeSeconds = 3000L
#'       ),
#'       list(
#'         AllowedHeaders = list(
#'           "Authorization"
#'         ),
#'         AllowedMethods = list(
#'           "GET"
#'         ),
#'         AllowedOrigins = list(
#'           "*"
#'         ),
#'         MaxAgeSeconds = 3000L
#'       )
#'     )
#'   ),
#'   ContentMD5 = ""
#' )
#' }
#'
#' @keywords internal
#'
#' @rdname s3_put_bucket_cors
#'
#' @aliases s3_put_bucket_cors
s3_put_bucket_cors <- function(Bucket, CORSConfiguration, ContentMD5 = NULL, ChecksumAlgorithm = NULL, ExpectedBucketOwner = NULL) {
  op <- new_operation(
    name = "PutBucketCors",
    http_method = "PUT",
    http_path = "/{Bucket}?cors",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .s3$put_bucket_cors_input(Bucket = Bucket, CORSConfiguration = CORSConfiguration, ContentMD5 = ContentMD5, ChecksumAlgorithm = ChecksumAlgorithm, ExpectedBucketOwner = ExpectedBucketOwner)
  output <- .s3$put_bucket_cors_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$put_bucket_cors <- s3_put_bucket_cors

#' This operation configures default encryption and Amazon S3 Bucket Keys
#' for an existing bucket
#'
#' @description
#' This operation configures default encryption and Amazon S3 Bucket Keys
#' for an existing bucket. You can also [block encryption
#' types](https://docs.aws.amazon.com/AmazonS3/latest/API/API_BlockedEncryptionTypes.html)
#' using this operation.
#' 
#' **Directory buckets** - For directory buckets, you must make requests
#' for this API operation to the Regional endpoint. These endpoints support
#' path-style requests in the format
#' `https://s3express-control.region-code.amazonaws.com/bucket-name `.
#' Virtual-hosted-style requests aren't supported. For more information
#' about endpoints in Availability Zones, see [Regional and Zonal endpoints
#' for directory buckets in Availability
#' Zones](https://docs.aws.amazon.com/AmazonS3/latest/userguide/endpoint-directory-buckets-AZ.html)
#' in the *Amazon S3 User Guide*. For more information about endpoints in
#' Local Zones, see [Concepts for directory buckets in Local
#' Zones](https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-lzs-for-directory-buckets.html)
#' in the *Amazon S3 User Guide*.
#' 
#' By default, all buckets have a default encryption configuration that
#' uses server-side encryption with Amazon S3 managed keys (SSE-S3).
#' 
#' -   **General purpose buckets**
#' 
#'     -   You can optionally configure default encryption for a bucket by
#'         using server-side encryption with Key Management Service (KMS)
#'         keys (SSE-KMS) or dual-layer server-side encryption with Amazon
#'         Web Services KMS keys (DSSE-KMS). If you specify default
#'         encryption by using SSE-KMS, you can also configure [Amazon S3
#'         Bucket
#'         Keys](https://docs.aws.amazon.com/AmazonS3/latest/userguide/bucket-key.html).
#'         For information about the bucket default encryption feature, see
#'         [Amazon S3 Bucket Default
#'         Encryption](https://docs.aws.amazon.com/AmazonS3/latest/userguide/bucket-encryption.html)
#'         in the *Amazon S3 User Guide*.
#' 
#'     -   If you use PutBucketEncryption to set your [default bucket
#'         encryption](https://docs.aws.amazon.com/AmazonS3/latest/userguide/bucket-encryption.html)
#'         to SSE-KMS, you should verify that your KMS key ID is correct.
#'         Amazon S3 doesn't validate the KMS key ID provided in
#'         PutBucketEncryption requests.
#' 
#' -   **Directory buckets** - You can optionally configure default
#'     encryption for a bucket by using server-side encryption with Key
#'     Management Service (KMS) keys (SSE-KMS).
#' 
#'     -   We recommend that the bucket's default encryption uses the
#'         desired encryption configuration and you don't override the
#'         bucket default encryption in your
#'         [`create_session`][s3_create_session] requests or `PUT` object
#'         requests. Then, new objects are automatically encrypted with the
#'         desired encryption settings. For more information about the
#'         encryption overriding behaviors in directory buckets, see
#'         [Specifying server-side encryption with KMS for new object
#'         uploads](https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-express-specifying-kms-encryption.html).
#' 
#'     -   Your SSE-KMS configuration can only support 1 [customer managed
#'         key](https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#customer-cmk)
#'         per directory bucket's lifetime. The [Amazon Web Services
#'         managed
#'         key](https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#aws-managed-cmk)
#'         (`aws/s3`) isn't supported.
#' 
#'     -   S3 Bucket Keys are always enabled for `GET` and `PUT` operations
#'         in a directory bucket and can’t be disabled. S3 Bucket Keys
#'         aren't supported, when you copy SSE-KMS encrypted objects from
#'         general purpose buckets to directory buckets, from directory
#'         buckets to general purpose buckets, or between directory
#'         buckets, through [`copy_object`][s3_copy_object],
#'         [`upload_part_copy`][s3_upload_part_copy], [the Copy operation
#'         in Batch
#'         Operations](https://docs.aws.amazon.com/AmazonS3/latest/userguide/directory-buckets-objects-Batch-Ops.html),
#'         or [the import
#'         jobs](https://docs.aws.amazon.com/AmazonS3/latest/userguide/create-import-job.html).
#'         In this case, Amazon S3 makes a call to KMS every time a copy
#'         request is made for a KMS-encrypted object.
#' 
#'     -   When you specify an [KMS customer managed
#'         key](https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#customer-cmk)
#'         for encryption in your directory bucket, only use the key ID or
#'         key ARN. The key alias format of the KMS key isn't supported.
#' 
#'     -   For directory buckets, if you use PutBucketEncryption to set
#'         your [default bucket
#'         encryption](https://docs.aws.amazon.com/AmazonS3/latest/userguide/bucket-encryption.html)
#'         to SSE-KMS, Amazon S3 validates the KMS key ID provided in
#'         PutBucketEncryption requests.
#' 
#' If you're specifying a customer managed KMS key, we recommend using a
#' fully qualified KMS key ARN. If you use a KMS key alias instead, then
#' KMS resolves the key within the requester’s account. This behavior can
#' result in data that's encrypted with a KMS key that belongs to the
#' requester, and not the bucket owner.
#' 
#' Also, this action requires Amazon Web Services Signature Version 4. For
#' more information, see [Authenticating Requests (Amazon Web Services
#' Signature Version
#' 4)](https://docs.aws.amazon.com/AmazonS3/latest/API/sig-v4-authenticating-requests.html).
#' 
#' ### Permissions
#' 
#' -   **General purpose bucket permissions** - The
#'     `s3:PutEncryptionConfiguration` permission is required in a policy.
#'     The bucket owner has this permission by default. The bucket owner
#'     can grant this permission to others. For more information about
#'     permissions, see [Permissions Related to Bucket
#'     Operations](https://docs.aws.amazon.com/AmazonS3/latest/userguide/security_iam_service-with-iam.html#security_iam_service-with-iam-id-based-policies-actions)
#'     and [Managing Access Permissions to Your Amazon S3
#'     Resources](https://docs.aws.amazon.com/AmazonS3/latest/userguide/security-iam.html)
#'     in the *Amazon S3 User Guide*.
#' 
#' -   **Directory bucket permissions** - To grant access to this API
#'     operation, you must have the `s3express:PutEncryptionConfiguration`
#'     permission in an IAM identity-based policy instead of a bucket
#'     policy. Cross-account access to this API operation isn't supported.
#'     This operation can only be performed by the Amazon Web Services
#'     account that owns the resource. For more information about directory
#'     bucket policies and permissions, see [Amazon Web Services Identity
#'     and Access Management (IAM) for S3 Express One
#'     Zone](https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-express-security-iam.html)
#'     in the *Amazon S3 User Guide*.
#' 
#'     To set a directory bucket default encryption with SSE-KMS, you must
#'     also have the `kms:GenerateDataKey` and the `kms:Decrypt`
#'     permissions in IAM identity-based policies and KMS key policies for
#'     the target KMS key.
#' 
#' ### HTTP Host header syntax
#' 
#' **Directory buckets** - The HTTP Host header syntax is
#' `s3express-control.region-code.amazonaws.com`.
#' 
#' The following operations are related to
#' [`put_bucket_encryption`][s3_put_bucket_encryption]:
#' 
#' -   [`get_bucket_encryption`][s3_get_bucket_encryption]
#' 
#' -   [`delete_bucket_encryption`][s3_delete_bucket_encryption]
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#'
#' @usage
#' s3_put_bucket_encryption(Bucket, ContentMD5, ChecksumAlgorithm,
#'   ServerSideEncryptionConfiguration, ExpectedBucketOwner)
#'
#' @param Bucket &#91;required&#93; Specifies default encryption for a bucket using server-side encryption
#' with different key options.
#' 
#' **Directory buckets** - When you use this operation with a directory
#' bucket, you must use path-style requests in the format
#' `https://s3express-control.region-code.amazonaws.com/bucket-name `.
#' Virtual-hosted-style requests aren't supported. Directory bucket names
#' must be unique in the chosen Zone (Availability Zone or Local Zone).
#' Bucket names must also follow the format
#' ` bucket-base-name--zone-id--x-s3` (for example,
#' ` DOC-EXAMPLE-BUCKET--usw2-az1--x-s3`). For information about bucket
#' naming restrictions, see [Directory bucket naming
#' rules](https://docs.aws.amazon.com/AmazonS3/latest/userguide/directory-bucket-naming-rules.html)
#' in the *Amazon S3 User Guide*
#' @param ContentMD5 The Base64 encoded 128-bit `MD5` digest of the server-side encryption
#' configuration.
#' 
#' For requests made using the Amazon Web Services Command Line Interface
#' (CLI) or Amazon Web Services SDKs, this field is calculated
#' automatically.
#' 
#' This functionality is not supported for directory buckets.
#' @param ChecksumAlgorithm Indicates the algorithm used to create the checksum for the request when
#' you use the SDK. This header will not provide any additional
#' functionality if you don't use the SDK. When you send this header, there
#' must be a corresponding `x-amz-checksum` or `x-amz-trailer` header sent.
#' Otherwise, Amazon S3 fails the request with the HTTP status code
#' `400 Bad Request`. For more information, see [Checking object
#' integrity](https://docs.aws.amazon.com/AmazonS3/latest/userguide/checking-object-integrity.html)
#' in the *Amazon S3 User Guide*.
#' 
#' If you provide an individual checksum, Amazon S3 ignores any provided
#' `ChecksumAlgorithm` parameter.
#' 
#' For directory buckets, when you use Amazon Web Services SDKs, `CRC32` is
#' the default checksum algorithm that's used for performance.
#' @param ServerSideEncryptionConfiguration &#91;required&#93; 
#' @param ExpectedBucketOwner The account ID of the expected bucket owner. If the account ID that you
#' provide does not match the actual owner of the bucket, the request fails
#' with the HTTP status code `403 Forbidden` (access denied).
#' 
#' For directory buckets, this header is not supported in this API
#' operation. If you specify this header, the request fails with the HTTP
#' status code `501 Not Implemented`.
#'
#' @return
#' An empty list.
#'
#' @section Request syntax:
#' ```
#' svc$put_bucket_encryption(
#'   Bucket = "string",
#'   ContentMD5 = "string",
#'   ChecksumAlgorithm = "CRC32"|"CRC32C"|"SHA1"|"SHA256"|"CRC64NVME",
#'   ServerSideEncryptionConfiguration = list(
#'     Rules = list(
#'       list(
#'         ApplyServerSideEncryptionByDefault = list(
#'           SSEAlgorithm = "AES256"|"aws:fsx"|"aws:kms"|"aws:kms:dsse",
#'           KMSMasterKeyID = "string"
#'         ),
#'         BucketKeyEnabled = TRUE|FALSE,
#'         BlockedEncryptionTypes = list(
#'           EncryptionType = list(
#'             "NONE"|"SSE-C"
#'           )
#'         )
#'       )
#'     )
#'   ),
#'   ExpectedBucketOwner = "string"
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname s3_put_bucket_encryption
#'
#' @aliases s3_put_bucket_encryption
s3_put_bucket_encryption <- function(Bucket, ContentMD5 = NULL, ChecksumAlgorithm = NULL, ServerSideEncryptionConfiguration, ExpectedBucketOwner = NULL) {
  op <- new_operation(
    name = "PutBucketEncryption",
    http_method = "PUT",
    http_path = "/{Bucket}?encryption",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .s3$put_bucket_encryption_input(Bucket = Bucket, ContentMD5 = ContentMD5, ChecksumAlgorithm = ChecksumAlgorithm, ServerSideEncryptionConfiguration = ServerSideEncryptionConfiguration, ExpectedBucketOwner = ExpectedBucketOwner)
  output <- .s3$put_bucket_encryption_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$put_bucket_encryption <- s3_put_bucket_encryption

#' This operation is not supported for directory buckets
#'
#' @description
#' This operation is not supported for directory buckets.
#' 
#' Puts a S3 Intelligent-Tiering configuration to the specified bucket. You
#' can have up to 1,000 S3 Intelligent-Tiering configurations per bucket.
#' 
#' The S3 Intelligent-Tiering storage class is designed to optimize storage
#' costs by automatically moving data to the most cost-effective storage
#' access tier, without performance impact or operational overhead. S3
#' Intelligent-Tiering delivers automatic cost savings in three low latency
#' and high throughput access tiers. To get the lowest storage cost on data
#' that can be accessed in minutes to hours, you can choose to activate
#' additional archiving capabilities.
#' 
#' The S3 Intelligent-Tiering storage class is the ideal storage class for
#' data with unknown, changing, or unpredictable access patterns,
#' independent of object size or retention period. If the size of an object
#' is less than 128 KB, it is not monitored and not eligible for
#' auto-tiering. Smaller objects can be stored, but they are always charged
#' at the Frequent Access tier rates in the S3 Intelligent-Tiering storage
#' class.
#' 
#' For more information, see [Storage class for automatically optimizing
#' frequently and infrequently accessed
#' objects](https://docs.aws.amazon.com/AmazonS3/latest/userguide/storage-class-intro.html#sc-dynamic-data-access).
#' 
#' Operations related to
#' [`put_bucket_intelligent_tiering_configuration`][s3_put_bucket_intelligent_tiering_configuration]
#' include:
#' 
#' -   [`delete_bucket_intelligent_tiering_configuration`][s3_delete_bucket_intelligent_tiering_configuration]
#' 
#' -   [`get_bucket_intelligent_tiering_configuration`][s3_get_bucket_intelligent_tiering_configuration]
#' 
#' -   [`list_bucket_intelligent_tiering_configurations`][s3_list_bucket_intelligent_tiering_configurations]
#' 
#' You only need S3 Intelligent-Tiering enabled on a bucket if you want to
#' automatically move objects stored in the S3 Intelligent-Tiering storage
#' class to the Archive Access or Deep Archive Access tier.
#' 
#' [`put_bucket_intelligent_tiering_configuration`][s3_put_bucket_intelligent_tiering_configuration]
#' has the following special errors:
#' 
#' ### HTTP 400 Bad Request Error
#' 
#' *Code:* InvalidArgument
#' 
#' *Cause:* Invalid Argument
#' 
#' ### HTTP 400 Bad Request Error
#' 
#' *Code:* TooManyConfigurations
#' 
#' *Cause:* You are attempting to create a new configuration but have
#' already reached the 1,000-configuration limit.
#' 
#' ### HTTP 403 Forbidden Error
#' 
#' *Cause:* You are not the owner of the specified bucket, or you do not
#' have the `s3:PutIntelligentTieringConfiguration` bucket permission to
#' set the configuration on the bucket.
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#'
#' @usage
#' s3_put_bucket_intelligent_tiering_configuration(Bucket, Id,
#'   ExpectedBucketOwner, IntelligentTieringConfiguration)
#'
#' @param Bucket &#91;required&#93; The name of the Amazon S3 bucket whose configuration you want to modify
#' or retrieve.
#' @param Id &#91;required&#93; The ID used to identify the S3 Intelligent-Tiering configuration.
#' @param ExpectedBucketOwner The account ID of the expected bucket owner. If the account ID that you
#' provide does not match the actual owner of the bucket, the request fails
#' with the HTTP status code `403 Forbidden` (access denied).
#' @param IntelligentTieringConfiguration &#91;required&#93; Container for S3 Intelligent-Tiering configuration.
#'
#' @return
#' An empty list.
#'
#' @section Request syntax:
#' ```
#' svc$put_bucket_intelligent_tiering_configuration(
#'   Bucket = "string",
#'   Id = "string",
#'   ExpectedBucketOwner = "string",
#'   IntelligentTieringConfiguration = list(
#'     Id = "string",
#'     Filter = list(
#'       Prefix = "string",
#'       Tag = list(
#'         Key = "string",
#'         Value = "string"
#'       ),
#'       And = list(
#'         Prefix = "string",
#'         Tags = list(
#'           list(
#'             Key = "string",
#'             Value = "string"
#'           )
#'         )
#'       )
#'     ),
#'     Status = "Enabled"|"Disabled",
#'     Tierings = list(
#'       list(
#'         Days = 123,
#'         AccessTier = "ARCHIVE_ACCESS"|"DEEP_ARCHIVE_ACCESS"
#'       )
#'     )
#'   )
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname s3_put_bucket_intelligent_tiering_configuration
#'
#' @aliases s3_put_bucket_intelligent_tiering_configuration
s3_put_bucket_intelligent_tiering_configuration <- function(Bucket, Id, ExpectedBucketOwner = NULL, IntelligentTieringConfiguration) {
  op <- new_operation(
    name = "PutBucketIntelligentTieringConfiguration",
    http_method = "PUT",
    http_path = "/{Bucket}?intelligent-tiering",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .s3$put_bucket_intelligent_tiering_configuration_input(Bucket = Bucket, Id = Id, ExpectedBucketOwner = ExpectedBucketOwner, IntelligentTieringConfiguration = IntelligentTieringConfiguration)
  output <- .s3$put_bucket_intelligent_tiering_configuration_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$put_bucket_intelligent_tiering_configuration <- s3_put_bucket_intelligent_tiering_configuration

#' This operation is not supported for directory buckets
#'
#' @description
#' This operation is not supported for directory buckets.
#' 
#' This implementation of the `PUT` action adds an S3 Inventory
#' configuration (identified by the inventory ID) to the bucket. You can
#' have up to 1,000 inventory configurations per bucket.
#' 
#' Amazon S3 inventory generates inventories of the objects in the bucket
#' on a daily or weekly basis, and the results are published to a flat
#' file. The bucket that is inventoried is called the *source* bucket, and
#' the bucket where the inventory flat file is stored is called the
#' *destination* bucket. The *destination* bucket must be in the same
#' Amazon Web Services Region as the *source* bucket.
#' 
#' When you configure an inventory for a *source* bucket, you specify the
#' *destination* bucket where you want the inventory to be stored, and
#' whether to generate the inventory daily or weekly. You can also
#' configure what object metadata to include and whether to inventory all
#' object versions or only current versions. For more information, see
#' [Amazon S3
#' Inventory](https://docs.aws.amazon.com/AmazonS3/latest/userguide/storage-inventory.html)
#' in the Amazon S3 User Guide.
#' 
#' You must create a bucket policy on the *destination* bucket to grant
#' permissions to Amazon S3 to write objects to the bucket in the defined
#' location. For an example policy, see [Granting Permissions for Amazon S3
#' Inventory and Storage Class
#' Analysis](https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-bucket-policies.html#example-bucket-policies-use-case-9).
#' 
#' ### Permissions
#' 
#' To use this operation, you must have permission to perform the
#' `s3:PutInventoryConfiguration` action. The bucket owner has this
#' permission by default and can grant this permission to others.
#' 
#' The `s3:PutInventoryConfiguration` permission allows a user to create an
#' [S3
#' Inventory](https://docs.aws.amazon.com/AmazonS3/latest/userguide/storage-inventory.html)
#' report that includes all object metadata fields available and to specify
#' the destination bucket to store the inventory. A user with read access
#' to objects in the destination bucket can also access all object metadata
#' fields that are available in the inventory report.
#' 
#' To restrict access to an inventory report, see [Restricting access to an
#' Amazon S3 Inventory
#' report](https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-bucket-policies.html#example-bucket-policies-use-case-10)
#' in the *Amazon S3 User Guide*. For more information about the metadata
#' fields available in S3 Inventory, see [Amazon S3 Inventory
#' lists](https://docs.aws.amazon.com/AmazonS3/latest/userguide/storage-inventory.html#storage-inventory-contents)
#' in the *Amazon S3 User Guide*. For more information about permissions,
#' see [Permissions related to bucket subresource
#' operations](https://docs.aws.amazon.com/AmazonS3/latest/userguide/security_iam_service-with-iam.html#security_iam_service-with-iam-id-based-policies-actions)
#' and [Identity and access management in Amazon
#' S3](https://docs.aws.amazon.com/AmazonS3/latest/userguide/security-iam.html)
#' in the *Amazon S3 User Guide*.
#' 
#' [`put_bucket_inventory_configuration`][s3_put_bucket_inventory_configuration]
#' has the following special errors:
#' 
#' ### HTTP 400 Bad Request Error
#' 
#' *Code:* InvalidArgument
#' 
#' *Cause:* Invalid Argument
#' 
#' ### HTTP 400 Bad Request Error
#' 
#' *Code:* TooManyConfigurations
#' 
#' *Cause:* You are attempting to create a new configuration but have
#' already reached the 1,000-configuration limit.
#' 
#' ### HTTP 403 Forbidden Error
#' 
#' *Cause:* You are not the owner of the specified bucket, or you do not
#' have the `s3:PutInventoryConfiguration` bucket permission to set the
#' configuration on the bucket.
#' 
#' The following operations are related to
#' [`put_bucket_inventory_configuration`][s3_put_bucket_inventory_configuration]:
#' 
#' -   [`get_bucket_inventory_configuration`][s3_get_bucket_inventory_configuration]
#' 
#' -   [`delete_bucket_inventory_configuration`][s3_delete_bucket_inventory_configuration]
#' 
#' -   [`list_bucket_inventory_configurations`][s3_list_bucket_inventory_configurations]
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#'
#' @usage
#' s3_put_bucket_inventory_configuration(Bucket, Id,
#'   InventoryConfiguration, ExpectedBucketOwner)
#'
#' @param Bucket &#91;required&#93; The name of the bucket where the inventory configuration will be stored.
#' @param Id &#91;required&#93; The ID used to identify the inventory configuration.
#' @param InventoryConfiguration &#91;required&#93; Specifies the inventory configuration.
#' @param ExpectedBucketOwner The account ID of the expected bucket owner. If the account ID that you
#' provide does not match the actual owner of the bucket, the request fails
#' with the HTTP status code `403 Forbidden` (access denied).
#'
#' @return
#' An empty list.
#'
#' @section Request syntax:
#' ```
#' svc$put_bucket_inventory_configuration(
#'   Bucket = "string",
#'   Id = "string",
#'   InventoryConfiguration = list(
#'     Destination = list(
#'       S3BucketDestination = list(
#'         AccountId = "string",
#'         Bucket = "string",
#'         Format = "CSV"|"ORC"|"Parquet",
#'         Prefix = "string",
#'         Encryption = list(
#'           SSES3 = list(),
#'           SSEKMS = list(
#'             KeyId = "string"
#'           )
#'         )
#'       )
#'     ),
#'     IsEnabled = TRUE|FALSE,
#'     Filter = list(
#'       Prefix = "string"
#'     ),
#'     Id = "string",
#'     IncludedObjectVersions = "All"|"Current",
#'     OptionalFields = list(
#'       "Size"|"LastModifiedDate"|"StorageClass"|"ETag"|"IsMultipartUploaded"|"ReplicationStatus"|"EncryptionStatus"|"ObjectLockRetainUntilDate"|"ObjectLockMode"|"ObjectLockLegalHoldStatus"|"IntelligentTieringAccessTier"|"BucketKeyStatus"|"ChecksumAlgorithm"|"ObjectAccessControlList"|"ObjectOwner"|"LifecycleExpirationDate"
#'     ),
#'     Schedule = list(
#'       Frequency = "Daily"|"Weekly"
#'     )
#'   ),
#'   ExpectedBucketOwner = "string"
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname s3_put_bucket_inventory_configuration
#'
#' @aliases s3_put_bucket_inventory_configuration
s3_put_bucket_inventory_configuration <- function(Bucket, Id, InventoryConfiguration, ExpectedBucketOwner = NULL) {
  op <- new_operation(
    name = "PutBucketInventoryConfiguration",
    http_method = "PUT",
    http_path = "/{Bucket}?inventory",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .s3$put_bucket_inventory_configuration_input(Bucket = Bucket, Id = Id, InventoryConfiguration = InventoryConfiguration, ExpectedBucketOwner = ExpectedBucketOwner)
  output <- .s3$put_bucket_inventory_configuration_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$put_bucket_inventory_configuration <- s3_put_bucket_inventory_configuration

#' This operation is not supported for directory buckets
#'
#' @description
#' This operation is not supported for directory buckets.
#' 
#' For an updated version of this API, see
#' [`put_bucket_lifecycle_configuration`][s3_put_bucket_lifecycle_configuration].
#' This version has been deprecated. Existing lifecycle configurations will
#' work. For new lifecycle configurations, use the updated API.
#' 
#' This operation is not supported for directory buckets.
#' 
#' Creates a new lifecycle configuration for the bucket or replaces an
#' existing lifecycle configuration. For information about lifecycle
#' configuration, see [Object Lifecycle
#' Management](https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lifecycle-mgmt.html)
#' in the *Amazon S3 User Guide*.
#' 
#' By default, all Amazon S3 resources, including buckets, objects, and
#' related subresources (for example, lifecycle configuration and website
#' configuration) are private. Only the resource owner, the Amazon Web
#' Services account that created the resource, can access it. The resource
#' owner can optionally grant access permissions to others by writing an
#' access policy. For this operation, users must get the
#' `s3:PutLifecycleConfiguration` permission.
#' 
#' You can also explicitly deny permissions. Explicit denial also
#' supersedes any other permissions. If you want to prevent users or
#' accounts from removing or deleting objects from your bucket, you must
#' deny them permissions for the following actions:
#' 
#' -   `s3:DeleteObject`
#' 
#' -   `s3:DeleteObjectVersion`
#' 
#' -   `s3:PutLifecycleConfiguration`
#' 
#' For more information about permissions, see [Managing Access Permissions
#' to your Amazon S3
#' Resources](https://docs.aws.amazon.com/AmazonS3/latest/userguide/security-iam.html)
#' in the *Amazon S3 User Guide*.
#' 
#' For more examples of transitioning objects to storage classes such as
#' STANDARD_IA or ONEZONE_IA, see [Examples of Lifecycle
#' Configuration](https://docs.aws.amazon.com/AmazonS3/latest/userguide/intro-lifecycle-rules.html#lifecycle-configuration-examples).
#' 
#' The following operations are related to
#' [`put_bucket_lifecycle`][s3_put_bucket_lifecycle]:
#' 
#' -   [`get_bucket_lifecycle`][s3_get_bucket_lifecycle](Deprecated)
#' 
#' -   [`get_bucket_lifecycle_configuration`][s3_get_bucket_lifecycle_configuration]
#' 
#' -   [`restore_object`][s3_restore_object]
#' 
#' -   By default, a resource owner—in this case, a bucket owner, which is
#'     the Amazon Web Services account that created the bucket—can perform
#'     any of the operations. A resource owner can also grant others
#'     permission to perform the operation. For more information, see the
#'     following topics in the Amazon S3 User Guide:
#' 
#'     -   [Specifying Permissions in a
#'         Policy](https://docs.aws.amazon.com/AmazonS3/latest/userguide/security_iam_service-with-iam.html#security_iam_service-with-iam-id-based-policies-actions)
#' 
#'     -   [Managing Access Permissions to your Amazon S3
#'         Resources](https://docs.aws.amazon.com/AmazonS3/latest/userguide/security-iam.html)
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#'
#' @usage
#' s3_put_bucket_lifecycle(Bucket, ContentMD5, ChecksumAlgorithm,
#'   LifecycleConfiguration, ExpectedBucketOwner)
#'
#' @param Bucket &#91;required&#93; 
#' @param ContentMD5 For requests made using the Amazon Web Services Command Line Interface
#' (CLI) or Amazon Web Services SDKs, this field is calculated
#' automatically.
#' @param ChecksumAlgorithm Indicates the algorithm used to create the checksum for the request when
#' you use the SDK. This header will not provide any additional
#' functionality if you don't use the SDK. When you send this header, there
#' must be a corresponding `x-amz-checksum` or `x-amz-trailer` header sent.
#' Otherwise, Amazon S3 fails the request with the HTTP status code
#' `400 Bad Request`. For more information, see [Checking object
#' integrity](https://docs.aws.amazon.com/AmazonS3/latest/userguide/checking-object-integrity.html)
#' in the *Amazon S3 User Guide*.
#' 
#' If you provide an individual checksum, Amazon S3 ignores any provided
#' `ChecksumAlgorithm` parameter.
#' @param LifecycleConfiguration 
#' @param ExpectedBucketOwner The account ID of the expected bucket owner. If the account ID that you
#' provide does not match the actual owner of the bucket, the request fails
#' with the HTTP status code `403 Forbidden` (access denied).
#'
#' @return
#' An empty list.
#'
#' @section Request syntax:
#' ```
#' svc$put_bucket_lifecycle(
#'   Bucket = "string",
#'   ContentMD5 = "string",
#'   ChecksumAlgorithm = "CRC32"|"CRC32C"|"SHA1"|"SHA256"|"CRC64NVME",
#'   LifecycleConfiguration = list(
#'     Rules = list(
#'       list(
#'         Expiration = list(
#'           Date = as.POSIXct(
#'             "2015-01-01"
#'           ),
#'           Days = 123,
#'           ExpiredObjectDeleteMarker = TRUE|FALSE
#'         ),
#'         ID = "string",
#'         Prefix = "string",
#'         Status = "Enabled"|"Disabled",
#'         Transition = list(
#'           Date = as.POSIXct(
#'             "2015-01-01"
#'           ),
#'           Days = 123,
#'           StorageClass = "GLACIER"|"STANDARD_IA"|"ONEZONE_IA"|"INTELLIGENT_TIERING"|"DEEP_ARCHIVE"|"GLACIER_IR"
#'         ),
#'         NoncurrentVersionTransition = list(
#'           NoncurrentDays = 123,
#'           StorageClass = "GLACIER"|"STANDARD_IA"|"ONEZONE_IA"|"INTELLIGENT_TIERING"|"DEEP_ARCHIVE"|"GLACIER_IR",
#'           NewerNoncurrentVersions = 123
#'         ),
#'         NoncurrentVersionExpiration = list(
#'           NoncurrentDays = 123,
#'           NewerNoncurrentVersions = 123
#'         ),
#'         AbortIncompleteMultipartUpload = list(
#'           DaysAfterInitiation = 123
#'         )
#'       )
#'     )
#'   ),
#'   ExpectedBucketOwner = "string"
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname s3_put_bucket_lifecycle
#'
#' @aliases s3_put_bucket_lifecycle
s3_put_bucket_lifecycle <- function(Bucket, ContentMD5 = NULL, ChecksumAlgorithm = NULL, LifecycleConfiguration = NULL, ExpectedBucketOwner = NULL) {
  op <- new_operation(
    name = "PutBucketLifecycle",
    http_method = "PUT",
    http_path = "/{Bucket}?lifecycle",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .s3$put_bucket_lifecycle_input(Bucket = Bucket, ContentMD5 = ContentMD5, ChecksumAlgorithm = ChecksumAlgorithm, LifecycleConfiguration = LifecycleConfiguration, ExpectedBucketOwner = ExpectedBucketOwner)
  output <- .s3$put_bucket_lifecycle_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$put_bucket_lifecycle <- s3_put_bucket_lifecycle

#' Creates a new lifecycle configuration for the bucket or replaces an
#' existing lifecycle configuration
#'
#' @description
#' Creates a new lifecycle configuration for the bucket or replaces an
#' existing lifecycle configuration. Keep in mind that this will overwrite
#' an existing lifecycle configuration, so if you want to retain any
#' configuration details, they must be included in the new lifecycle
#' configuration. For information about lifecycle configuration, see
#' [Managing your storage
#' lifecycle](https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lifecycle-mgmt.html).
#' 
#' Bucket lifecycle configuration now supports specifying a lifecycle rule
#' using an object key name prefix, one or more object tags, object size,
#' or any combination of these. Accordingly, this section describes the
#' latest API. The previous version of the API supported filtering based
#' only on an object key name prefix, which is supported for backward
#' compatibility. For the related API description, see
#' [`put_bucket_lifecycle`][s3_put_bucket_lifecycle].
#' 
#' ### Rules
#' 
#' ### Permissions
#' 
#' ### HTTP Host header syntax
#' 
#' You specify the lifecycle configuration in your request body. The
#' lifecycle configuration is specified as XML consisting of one or more
#' rules. An Amazon S3 Lifecycle configuration can have up to 1,000 rules.
#' This limit is not adjustable.
#' 
#' Bucket lifecycle configuration supports specifying a lifecycle rule
#' using an object key name prefix, one or more object tags, object size,
#' or any combination of these. Accordingly, this section describes the
#' latest API. The previous version of the API supported filtering based
#' only on an object key name prefix, which is supported for backward
#' compatibility for general purpose buckets. For the related API
#' description, see [`put_bucket_lifecycle`][s3_put_bucket_lifecycle].
#' 
#' Lifecyle configurations for directory buckets only support expiring
#' objects and cancelling multipart uploads. Expiring of versioned
#' objects,transitions and tag filters are not supported.
#' 
#' A lifecycle rule consists of the following:
#' 
#' -   A filter identifying a subset of objects to which the rule applies.
#'     The filter can be based on a key name prefix, object tags, object
#'     size, or any combination of these.
#' 
#' -   A status indicating whether the rule is in effect.
#' 
#' -   One or more lifecycle transition and expiration actions that you
#'     want Amazon S3 to perform on the objects identified by the filter.
#'     If the state of your bucket is versioning-enabled or
#'     versioning-suspended, you can have many versions of the same object
#'     (one current version and zero or more noncurrent versions). Amazon
#'     S3 provides predefined actions that you can specify for current and
#'     noncurrent object versions.
#' 
#' For more information, see [Object Lifecycle
#' Management](https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lifecycle-mgmt.html)
#' and [Lifecycle Configuration
#' Elements](https://docs.aws.amazon.com/AmazonS3/latest/userguide/intro-lifecycle-rules.html).
#' 
#' -   **General purpose bucket permissions** - By default, all Amazon S3
#'     resources are private, including buckets, objects, and related
#'     subresources (for example, lifecycle configuration and website
#'     configuration). Only the resource owner (that is, the Amazon Web
#'     Services account that created it) can access the resource. The
#'     resource owner can optionally grant access permissions to others by
#'     writing an access policy. For this operation, a user must have the
#'     `s3:PutLifecycleConfiguration` permission.
#' 
#'     You can also explicitly deny permissions. An explicit deny also
#'     supersedes any other permissions. If you want to block users or
#'     accounts from removing or deleting objects from your bucket, you
#'     must deny them permissions for the following actions:
#' 
#'     -   `s3:DeleteObject`
#' 
#'     -   `s3:DeleteObjectVersion`
#' 
#'     -   `s3:PutLifecycleConfiguration`
#' 
#'         For more information about permissions, see [Managing Access
#'         Permissions to Your Amazon S3
#'         Resources](https://docs.aws.amazon.com/AmazonS3/latest/userguide/security-iam.html).
#' 
#' 
#' -   **Directory bucket permissions** - You must have the
#'     `s3express:PutLifecycleConfiguration` permission in an IAM
#'     identity-based policy to use this operation. Cross-account access to
#'     this API operation isn't supported. The resource owner can
#'     optionally grant access permissions to others by creating a role or
#'     user for them as long as they are within the same account as the
#'     owner and resource.
#' 
#'     For more information about directory bucket policies and
#'     permissions, see [Authorizing Regional endpoint APIs with
#'     IAM](https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-express-security-iam.html)
#'     in the *Amazon S3 User Guide*.
#' 
#'     **Directory buckets** - For directory buckets, you must make
#'     requests for this API operation to the Regional endpoint. These
#'     endpoints support path-style requests in the format
#'     `https://s3express-control.region-code.amazonaws.com/bucket-name `.
#'     Virtual-hosted-style requests aren't supported. For more information
#'     about endpoints in Availability Zones, see [Regional and Zonal
#'     endpoints for directory buckets in Availability
#'     Zones](https://docs.aws.amazon.com/AmazonS3/latest/userguide/endpoint-directory-buckets-AZ.html)
#'     in the *Amazon S3 User Guide*. For more information about endpoints
#'     in Local Zones, see [Concepts for directory buckets in Local
#'     Zones](https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-lzs-for-directory-buckets.html)
#'     in the *Amazon S3 User Guide*.
#' 
#' **Directory buckets** - The HTTP Host header syntax is
#' `s3express-control.region.amazonaws.com`.
#' 
#' The following operations are related to
#' [`put_bucket_lifecycle_configuration`][s3_put_bucket_lifecycle_configuration]:
#' 
#' -   [`get_bucket_lifecycle_configuration`][s3_get_bucket_lifecycle_configuration]
#' 
#' -   [`delete_bucket_lifecycle`][s3_delete_bucket_lifecycle]
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#'
#' @usage
#' s3_put_bucket_lifecycle_configuration(Bucket, ChecksumAlgorithm,
#'   LifecycleConfiguration, ExpectedBucketOwner,
#'   TransitionDefaultMinimumObjectSize)
#'
#' @param Bucket &#91;required&#93; The name of the bucket for which to set the configuration.
#' @param ChecksumAlgorithm Indicates the algorithm used to create the checksum for the request when
#' you use the SDK. This header will not provide any additional
#' functionality if you don't use the SDK. When you send this header, there
#' must be a corresponding `x-amz-checksum` or `x-amz-trailer` header sent.
#' Otherwise, Amazon S3 fails the request with the HTTP status code
#' `400 Bad Request`. For more information, see [Checking object
#' integrity](https://docs.aws.amazon.com/AmazonS3/latest/userguide/checking-object-integrity.html)
#' in the *Amazon S3 User Guide*.
#' 
#' If you provide an individual checksum, Amazon S3 ignores any provided
#' `ChecksumAlgorithm` parameter.
#' @param LifecycleConfiguration Container for lifecycle rules. You can add as many as 1,000 rules.
#' @param ExpectedBucketOwner The account ID of the expected bucket owner. If the account ID that you
#' provide does not match the actual owner of the bucket, the request fails
#' with the HTTP status code `403 Forbidden` (access denied).
#' 
#' This parameter applies to general purpose buckets only. It is not
#' supported for directory bucket lifecycle configurations.
#' @param TransitionDefaultMinimumObjectSize Indicates which default minimum object size behavior is applied to the
#' lifecycle configuration.
#' 
#' This parameter applies to general purpose buckets only. It is not
#' supported for directory bucket lifecycle configurations.
#' 
#' -   `all_storage_classes_128K` - Objects smaller than 128 KB will not
#'     transition to any storage class by default.
#' 
#' -   `varies_by_storage_class` - Objects smaller than 128 KB will
#'     transition to Glacier Flexible Retrieval or Glacier Deep Archive
#'     storage classes. By default, all other storage classes will prevent
#'     transitions smaller than 128 KB.
#' 
#' To customize the minimum object size for any transition you can add a
#' filter that specifies a custom `ObjectSizeGreaterThan` or
#' `ObjectSizeLessThan` in the body of your transition rule. Custom filters
#' always take precedence over the default transition behavior.
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   TransitionDefaultMinimumObjectSize = "varies_by_storage_class"|"all_storage_classes_128K"
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$put_bucket_lifecycle_configuration(
#'   Bucket = "string",
#'   ChecksumAlgorithm = "CRC32"|"CRC32C"|"SHA1"|"SHA256"|"CRC64NVME",
#'   LifecycleConfiguration = list(
#'     Rules = list(
#'       list(
#'         Expiration = list(
#'           Date = as.POSIXct(
#'             "2015-01-01"
#'           ),
#'           Days = 123,
#'           ExpiredObjectDeleteMarker = TRUE|FALSE
#'         ),
#'         ID = "string",
#'         Prefix = "string",
#'         Filter = list(
#'           Prefix = "string",
#'           Tag = list(
#'             Key = "string",
#'             Value = "string"
#'           ),
#'           ObjectSizeGreaterThan = 123,
#'           ObjectSizeLessThan = 123,
#'           And = list(
#'             Prefix = "string",
#'             Tags = list(
#'               list(
#'                 Key = "string",
#'                 Value = "string"
#'               )
#'             ),
#'             ObjectSizeGreaterThan = 123,
#'             ObjectSizeLessThan = 123
#'           )
#'         ),
#'         Status = "Enabled"|"Disabled",
#'         Transitions = list(
#'           list(
#'             Date = as.POSIXct(
#'               "2015-01-01"
#'             ),
#'             Days = 123,
#'             StorageClass = "GLACIER"|"STANDARD_IA"|"ONEZONE_IA"|"INTELLIGENT_TIERING"|"DEEP_ARCHIVE"|"GLACIER_IR"
#'           )
#'         ),
#'         NoncurrentVersionTransitions = list(
#'           list(
#'             NoncurrentDays = 123,
#'             StorageClass = "GLACIER"|"STANDARD_IA"|"ONEZONE_IA"|"INTELLIGENT_TIERING"|"DEEP_ARCHIVE"|"GLACIER_IR",
#'             NewerNoncurrentVersions = 123
#'           )
#'         ),
#'         NoncurrentVersionExpiration = list(
#'           NoncurrentDays = 123,
#'           NewerNoncurrentVersions = 123
#'         ),
#'         AbortIncompleteMultipartUpload = list(
#'           DaysAfterInitiation = 123
#'         )
#'       )
#'     )
#'   ),
#'   ExpectedBucketOwner = "string",
#'   TransitionDefaultMinimumObjectSize = "varies_by_storage_class"|"all_storage_classes_128K"
#' )
#' ```
#'
#' @examples
#' \dontrun{
#' # The following example replaces existing lifecycle configuration, if any,
#' # on the specified bucket.
#' svc$put_bucket_lifecycle_configuration(
#'   Bucket = "examplebucket",
#'   LifecycleConfiguration = list(
#'     Rules = list(
#'       list(
#'         Expiration = list(
#'           Days = 3650L
#'         ),
#'         Filter = list(
#'           Prefix = "documents/"
#'         ),
#'         ID = "TestOnly",
#'         Status = "Enabled",
#'         Transitions = list(
#'           list(
#'             Days = 365L,
#'             StorageClass = "GLACIER"
#'           )
#'         )
#'       )
#'     )
#'   )
#' )
#' }
#'
#' @keywords internal
#'
#' @rdname s3_put_bucket_lifecycle_configuration
#'
#' @aliases s3_put_bucket_lifecycle_configuration
s3_put_bucket_lifecycle_configuration <- function(Bucket, ChecksumAlgorithm = NULL, LifecycleConfiguration = NULL, ExpectedBucketOwner = NULL, TransitionDefaultMinimumObjectSize = NULL) {
  op <- new_operation(
    name = "PutBucketLifecycleConfiguration",
    http_method = "PUT",
    http_path = "/{Bucket}?lifecycle",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .s3$put_bucket_lifecycle_configuration_input(Bucket = Bucket, ChecksumAlgorithm = ChecksumAlgorithm, LifecycleConfiguration = LifecycleConfiguration, ExpectedBucketOwner = ExpectedBucketOwner, TransitionDefaultMinimumObjectSize = TransitionDefaultMinimumObjectSize)
  output <- .s3$put_bucket_lifecycle_configuration_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$put_bucket_lifecycle_configuration <- s3_put_bucket_lifecycle_configuration

#' End of support notice: As of October 1, 2025, Amazon S3 has discontinued
#' support for Email Grantee Access Control Lists (ACLs)
#'
#' @description
#' End of support notice: As of October 1, 2025, Amazon S3 has discontinued
#' support for Email Grantee Access Control Lists (ACLs). If you attempt to
#' use an Email Grantee ACL in a request after October 1, 2025, the request
#' will receive an `HTTP 405` (Method Not Allowed) error.
#' 
#' This change affects the following Amazon Web Services Regions: US East
#' (N. Virginia), US West (N. California), US West (Oregon), Asia Pacific
#' (Singapore), Asia Pacific (Sydney), Asia Pacific (Tokyo), Europe
#' (Ireland), and South America (São Paulo).
#' 
#' This operation is not supported for directory buckets.
#' 
#' Set the logging parameters for a bucket and to specify permissions for
#' who can view and modify the logging parameters. All logs are saved to
#' buckets in the same Amazon Web Services Region as the source bucket. To
#' set the logging status of a bucket, you must be the bucket owner.
#' 
#' The bucket owner is automatically granted FULL_CONTROL to all logs. You
#' use the `Grantee` request element to grant access to other people. The
#' `Permissions` request element specifies the kind of access the grantee
#' has to the logs.
#' 
#' If the target bucket for log delivery uses the bucket owner enforced
#' setting for S3 Object Ownership, you can't use the `Grantee` request
#' element to grant access to others. Permissions can only be granted using
#' policies. For more information, see [Permissions for server access log
#' delivery](https://docs.aws.amazon.com/AmazonS3/latest/userguide/enable-server-access-logging.html#grant-log-delivery-permissions-general)
#' in the *Amazon S3 User Guide*.
#' 
#' ### Grantee Values
#' 
#' You can specify the person (grantee) to whom you're assigning access
#' rights (by using request elements) in the following ways. For examples
#' of how to specify these grantee values in JSON format, see the Amazon
#' Web Services CLI example in [Enabling Amazon S3 server access
#' logging](https://docs.aws.amazon.com/AmazonS3/latest/userguide/enable-server-access-logging.html)
#' in the *Amazon S3 User Guide*.
#' 
#' -   By the person's ID:
#' 
#'     `<Grantee xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:type="CanonicalUser"><ID><>ID<></ID><DisplayName><>GranteesEmail<></DisplayName> </Grantee>`
#' 
#'     `DisplayName` is optional and ignored in the request.
#' 
#' -   By Email address:
#' 
#'     ` <Grantee xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:type="AmazonCustomerByEmail"><EmailAddress><>Grantees@@email.com<></EmailAddress></Grantee>`
#' 
#'     The grantee is resolved to the `CanonicalUser` and, in a response to
#'     a `GETObjectAcl` request, appears as the CanonicalUser.
#' 
#' -   By URI:
#' 
#'     `<Grantee xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:type="Group"><URI><>http://acs.amazonaws.com/groups/global/AuthenticatedUsers<></URI></Grantee>`
#' 
#' To enable logging, you use `LoggingEnabled` and its children request
#' elements. To disable logging, you use an empty `BucketLoggingStatus`
#' request element:
#' 
#' `<BucketLoggingStatus xmlns="http://doc.s3.amazonaws.com/2006-03-01" />`
#' 
#' For more information about server access logging, see [Server Access
#' Logging](https://docs.aws.amazon.com/AmazonS3/latest/userguide/ServerLogs.html)
#' in the *Amazon S3 User Guide*.
#' 
#' For more information about creating a bucket, see
#' [`create_bucket`][s3_create_bucket]. For more information about
#' returning the logging status of a bucket, see
#' [`get_bucket_logging`][s3_get_bucket_logging].
#' 
#' The following operations are related to
#' [`put_bucket_logging`][s3_put_bucket_logging]:
#' 
#' -   [`put_object`][s3_put_object]
#' 
#' -   [`delete_bucket`][s3_delete_bucket]
#' 
#' -   [`create_bucket`][s3_create_bucket]
#' 
#' -   [`get_bucket_logging`][s3_get_bucket_logging]
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#'
#' @usage
#' s3_put_bucket_logging(Bucket, BucketLoggingStatus, ContentMD5,
#'   ChecksumAlgorithm, ExpectedBucketOwner)
#'
#' @param Bucket &#91;required&#93; The name of the bucket for which to set the logging parameters.
#' @param BucketLoggingStatus &#91;required&#93; Container for logging status information.
#' @param ContentMD5 The MD5 hash of the [`put_bucket_logging`][s3_put_bucket_logging]
#' request body.
#' 
#' For requests made using the Amazon Web Services Command Line Interface
#' (CLI) or Amazon Web Services SDKs, this field is calculated
#' automatically.
#' @param ChecksumAlgorithm Indicates the algorithm used to create the checksum for the request when
#' you use the SDK. This header will not provide any additional
#' functionality if you don't use the SDK. When you send this header, there
#' must be a corresponding `x-amz-checksum` or `x-amz-trailer` header sent.
#' Otherwise, Amazon S3 fails the request with the HTTP status code
#' `400 Bad Request`. For more information, see [Checking object
#' integrity](https://docs.aws.amazon.com/AmazonS3/latest/userguide/checking-object-integrity.html)
#' in the *Amazon S3 User Guide*.
#' 
#' If you provide an individual checksum, Amazon S3 ignores any provided
#' `ChecksumAlgorithm` parameter.
#' @param ExpectedBucketOwner The account ID of the expected bucket owner. If the account ID that you
#' provide does not match the actual owner of the bucket, the request fails
#' with the HTTP status code `403 Forbidden` (access denied).
#'
#' @return
#' An empty list.
#'
#' @section Request syntax:
#' ```
#' svc$put_bucket_logging(
#'   Bucket = "string",
#'   BucketLoggingStatus = list(
#'     LoggingEnabled = list(
#'       TargetBucket = "string",
#'       TargetGrants = list(
#'         list(
#'           Grantee = list(
#'             DisplayName = "string",
#'             EmailAddress = "string",
#'             ID = "string",
#'             Type = "CanonicalUser"|"AmazonCustomerByEmail"|"Group",
#'             URI = "string"
#'           ),
#'           Permission = "FULL_CONTROL"|"READ"|"WRITE"
#'         )
#'       ),
#'       TargetPrefix = "string",
#'       TargetObjectKeyFormat = list(
#'         SimplePrefix = list(),
#'         PartitionedPrefix = list(
#'           PartitionDateSource = "EventTime"|"DeliveryTime"
#'         )
#'       )
#'     )
#'   ),
#'   ContentMD5 = "string",
#'   ChecksumAlgorithm = "CRC32"|"CRC32C"|"SHA1"|"SHA256"|"CRC64NVME",
#'   ExpectedBucketOwner = "string"
#' )
#' ```
#'
#' @examples
#' \dontrun{
#' # The following example sets logging policy on a bucket. For the Log
#' # Delivery group to deliver logs to the destination bucket, it needs
#' # permission for the READ_ACP action which the policy grants.
#' svc$put_bucket_logging(
#'   Bucket = "sourcebucket",
#'   BucketLoggingStatus = list(
#'     LoggingEnabled = list(
#'       TargetBucket = "targetbucket",
#'       TargetGrants = list(
#'         list(
#'           Grantee = list(
#'             Type = "Group",
#'             URI = "http://acs.amazonaws.com/groups/global/AllUsers"
#'           ),
#'           Permission = "READ"
#'         )
#'       ),
#'       TargetPrefix = "MyBucketLogs/"
#'     )
#'   )
#' )
#' }
#'
#' @keywords internal
#'
#' @rdname s3_put_bucket_logging
#'
#' @aliases s3_put_bucket_logging
s3_put_bucket_logging <- function(Bucket, BucketLoggingStatus, ContentMD5 = NULL, ChecksumAlgorithm = NULL, ExpectedBucketOwner = NULL) {
  op <- new_operation(
    name = "PutBucketLogging",
    http_method = "PUT",
    http_path = "/{Bucket}?logging",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .s3$put_bucket_logging_input(Bucket = Bucket, BucketLoggingStatus = BucketLoggingStatus, ContentMD5 = ContentMD5, ChecksumAlgorithm = ChecksumAlgorithm, ExpectedBucketOwner = ExpectedBucketOwner)
  output <- .s3$put_bucket_logging_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$put_bucket_logging <- s3_put_bucket_logging

#' This operation is not supported for directory buckets
#'
#' @description
#' This operation is not supported for directory buckets.
#' 
#' Sets a metrics configuration (specified by the metrics configuration ID)
#' for the bucket. You can have up to 1,000 metrics configurations per
#' bucket. If you're updating an existing metrics configuration, note that
#' this is a full replacement of the existing metrics configuration. If you
#' don't include the elements you want to keep, they are erased.
#' 
#' To use this operation, you must have permissions to perform the
#' `s3:PutMetricsConfiguration` action. The bucket owner has this
#' permission by default. The bucket owner can grant this permission to
#' others. For more information about permissions, see [Permissions Related
#' to Bucket Subresource
#' Operations](https://docs.aws.amazon.com/AmazonS3/latest/userguide/security_iam_service-with-iam.html#security_iam_service-with-iam-id-based-policies-actions)
#' and [Managing Access Permissions to Your Amazon S3
#' Resources](https://docs.aws.amazon.com/AmazonS3/latest/userguide/security-iam.html).
#' 
#' For information about CloudWatch request metrics for Amazon S3, see
#' [Monitoring Metrics with Amazon
#' CloudWatch](https://docs.aws.amazon.com/AmazonS3/latest/userguide/cloudwatch-monitoring.html).
#' 
#' The following operations are related to
#' [`put_bucket_metrics_configuration`][s3_put_bucket_metrics_configuration]:
#' 
#' -   [`delete_bucket_metrics_configuration`][s3_delete_bucket_metrics_configuration]
#' 
#' -   [`get_bucket_metrics_configuration`][s3_get_bucket_metrics_configuration]
#' 
#' -   [`list_bucket_metrics_configurations`][s3_list_bucket_metrics_configurations]
#' 
#' [`put_bucket_metrics_configuration`][s3_put_bucket_metrics_configuration]
#' has the following special error:
#' 
#' -   Error code: `TooManyConfigurations`
#' 
#'     -   Description: You are attempting to create a new configuration
#'         but have already reached the 1,000-configuration limit.
#' 
#'     -   HTTP Status Code: HTTP 400 Bad Request
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#'
#' @usage
#' s3_put_bucket_metrics_configuration(Bucket, Id, MetricsConfiguration,
#'   ExpectedBucketOwner)
#'
#' @param Bucket &#91;required&#93; The name of the bucket for which the metrics configuration is set.
#' @param Id &#91;required&#93; The ID used to identify the metrics configuration. The ID has a 64
#' character limit and can only contain letters, numbers, periods, dashes,
#' and underscores.
#' @param MetricsConfiguration &#91;required&#93; Specifies the metrics configuration.
#' @param ExpectedBucketOwner The account ID of the expected bucket owner. If the account ID that you
#' provide does not match the actual owner of the bucket, the request fails
#' with the HTTP status code `403 Forbidden` (access denied).
#'
#' @return
#' An empty list.
#'
#' @section Request syntax:
#' ```
#' svc$put_bucket_metrics_configuration(
#'   Bucket = "string",
#'   Id = "string",
#'   MetricsConfiguration = list(
#'     Id = "string",
#'     Filter = list(
#'       Prefix = "string",
#'       Tag = list(
#'         Key = "string",
#'         Value = "string"
#'       ),
#'       AccessPointArn = "string",
#'       And = list(
#'         Prefix = "string",
#'         Tags = list(
#'           list(
#'             Key = "string",
#'             Value = "string"
#'           )
#'         ),
#'         AccessPointArn = "string"
#'       )
#'     )
#'   ),
#'   ExpectedBucketOwner = "string"
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname s3_put_bucket_metrics_configuration
#'
#' @aliases s3_put_bucket_metrics_configuration
s3_put_bucket_metrics_configuration <- function(Bucket, Id, MetricsConfiguration, ExpectedBucketOwner = NULL) {
  op <- new_operation(
    name = "PutBucketMetricsConfiguration",
    http_method = "PUT",
    http_path = "/{Bucket}?metrics",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .s3$put_bucket_metrics_configuration_input(Bucket = Bucket, Id = Id, MetricsConfiguration = MetricsConfiguration, ExpectedBucketOwner = ExpectedBucketOwner)
  output <- .s3$put_bucket_metrics_configuration_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$put_bucket_metrics_configuration <- s3_put_bucket_metrics_configuration

#' This operation is not supported for directory buckets
#'
#' @description
#' This operation is not supported for directory buckets.
#' 
#' No longer used, see the
#' [`put_bucket_notification_configuration`][s3_put_bucket_notification_configuration]
#' operation.
#'
#' @usage
#' s3_put_bucket_notification(Bucket, ContentMD5, ChecksumAlgorithm,
#'   NotificationConfiguration, ExpectedBucketOwner)
#'
#' @param Bucket &#91;required&#93; The name of the bucket.
#' @param ContentMD5 The MD5 hash of the
#' [`put_public_access_block`][s3_put_public_access_block] request body.
#' 
#' For requests made using the Amazon Web Services Command Line Interface
#' (CLI) or Amazon Web Services SDKs, this field is calculated
#' automatically.
#' @param ChecksumAlgorithm Indicates the algorithm used to create the checksum for the request when
#' you use the SDK. This header will not provide any additional
#' functionality if you don't use the SDK. When you send this header, there
#' must be a corresponding `x-amz-checksum` or `x-amz-trailer` header sent.
#' Otherwise, Amazon S3 fails the request with the HTTP status code
#' `400 Bad Request`. For more information, see [Checking object
#' integrity](https://docs.aws.amazon.com/AmazonS3/latest/userguide/checking-object-integrity.html)
#' in the *Amazon S3 User Guide*.
#' 
#' If you provide an individual checksum, Amazon S3 ignores any provided
#' `ChecksumAlgorithm` parameter.
#' @param NotificationConfiguration &#91;required&#93; The container for the configuration.
#' @param ExpectedBucketOwner The account ID of the expected bucket owner. If the account ID that you
#' provide does not match the actual owner of the bucket, the request fails
#' with the HTTP status code `403 Forbidden` (access denied).
#'
#' @return
#' An empty list.
#'
#' @section Request syntax:
#' ```
#' svc$put_bucket_notification(
#'   Bucket = "string",
#'   ContentMD5 = "string",
#'   ChecksumAlgorithm = "CRC32"|"CRC32C"|"SHA1"|"SHA256"|"CRC64NVME",
#'   NotificationConfiguration = list(
#'     TopicConfiguration = list(
#'       Id = "string",
#'       Events = list(
#'         "s3:ReducedRedundancyLostObject"|"s3:ObjectCreated:*"|"s3:ObjectCreated:Put"|"s3:ObjectCreated:Post"|"s3:ObjectCreated:Copy"|"s3:ObjectCreated:CompleteMultipartUpload"|"s3:ObjectRemoved:*"|"s3:ObjectRemoved:Delete"|"s3:ObjectRemoved:DeleteMarkerCreated"|"s3:ObjectRestore:*"|"s3:ObjectRestore:Post"|"s3:ObjectRestore:Completed"|"s3:Replication:*"|"s3:Replication:OperationFailedReplication"|"s3:Replication:OperationNotTracked"|"s3:Replication:OperationMissedThreshold"|"s3:Replication:OperationReplicatedAfterThreshold"|"s3:ObjectRestore:Delete"|"s3:LifecycleTransition"|"s3:IntelligentTiering"|"s3:ObjectAcl:Put"|"s3:LifecycleExpiration:*"|"s3:LifecycleExpiration:Delete"|"s3:LifecycleExpiration:DeleteMarkerCreated"|"s3:ObjectTagging:*"|"s3:ObjectTagging:Put"|"s3:ObjectTagging:Delete"
#'       ),
#'       Event = "s3:ReducedRedundancyLostObject"|"s3:ObjectCreated:*"|"s3:ObjectCreated:Put"|"s3:ObjectCreated:Post"|"s3:ObjectCreated:Copy"|"s3:ObjectCreated:CompleteMultipartUpload"|"s3:ObjectRemoved:*"|"s3:ObjectRemoved:Delete"|"s3:ObjectRemoved:DeleteMarkerCreated"|"s3:ObjectRestore:*"|"s3:ObjectRestore:Post"|"s3:ObjectRestore:Completed"|"s3:Replication:*"|"s3:Replication:OperationFailedReplication"|"s3:Replication:OperationNotTracked"|"s3:Replication:OperationMissedThreshold"|"s3:Replication:OperationReplicatedAfterThreshold"|"s3:ObjectRestore:Delete"|"s3:LifecycleTransition"|"s3:IntelligentTiering"|"s3:ObjectAcl:Put"|"s3:LifecycleExpiration:*"|"s3:LifecycleExpiration:Delete"|"s3:LifecycleExpiration:DeleteMarkerCreated"|"s3:ObjectTagging:*"|"s3:ObjectTagging:Put"|"s3:ObjectTagging:Delete",
#'       Topic = "string"
#'     ),
#'     QueueConfiguration = list(
#'       Id = "string",
#'       Event = "s3:ReducedRedundancyLostObject"|"s3:ObjectCreated:*"|"s3:ObjectCreated:Put"|"s3:ObjectCreated:Post"|"s3:ObjectCreated:Copy"|"s3:ObjectCreated:CompleteMultipartUpload"|"s3:ObjectRemoved:*"|"s3:ObjectRemoved:Delete"|"s3:ObjectRemoved:DeleteMarkerCreated"|"s3:ObjectRestore:*"|"s3:ObjectRestore:Post"|"s3:ObjectRestore:Completed"|"s3:Replication:*"|"s3:Replication:OperationFailedReplication"|"s3:Replication:OperationNotTracked"|"s3:Replication:OperationMissedThreshold"|"s3:Replication:OperationReplicatedAfterThreshold"|"s3:ObjectRestore:Delete"|"s3:LifecycleTransition"|"s3:IntelligentTiering"|"s3:ObjectAcl:Put"|"s3:LifecycleExpiration:*"|"s3:LifecycleExpiration:Delete"|"s3:LifecycleExpiration:DeleteMarkerCreated"|"s3:ObjectTagging:*"|"s3:ObjectTagging:Put"|"s3:ObjectTagging:Delete",
#'       Events = list(
#'         "s3:ReducedRedundancyLostObject"|"s3:ObjectCreated:*"|"s3:ObjectCreated:Put"|"s3:ObjectCreated:Post"|"s3:ObjectCreated:Copy"|"s3:ObjectCreated:CompleteMultipartUpload"|"s3:ObjectRemoved:*"|"s3:ObjectRemoved:Delete"|"s3:ObjectRemoved:DeleteMarkerCreated"|"s3:ObjectRestore:*"|"s3:ObjectRestore:Post"|"s3:ObjectRestore:Completed"|"s3:Replication:*"|"s3:Replication:OperationFailedReplication"|"s3:Replication:OperationNotTracked"|"s3:Replication:OperationMissedThreshold"|"s3:Replication:OperationReplicatedAfterThreshold"|"s3:ObjectRestore:Delete"|"s3:LifecycleTransition"|"s3:IntelligentTiering"|"s3:ObjectAcl:Put"|"s3:LifecycleExpiration:*"|"s3:LifecycleExpiration:Delete"|"s3:LifecycleExpiration:DeleteMarkerCreated"|"s3:ObjectTagging:*"|"s3:ObjectTagging:Put"|"s3:ObjectTagging:Delete"
#'       ),
#'       Queue = "string"
#'     ),
#'     CloudFunctionConfiguration = list(
#'       Id = "string",
#'       Event = "s3:ReducedRedundancyLostObject"|"s3:ObjectCreated:*"|"s3:ObjectCreated:Put"|"s3:ObjectCreated:Post"|"s3:ObjectCreated:Copy"|"s3:ObjectCreated:CompleteMultipartUpload"|"s3:ObjectRemoved:*"|"s3:ObjectRemoved:Delete"|"s3:ObjectRemoved:DeleteMarkerCreated"|"s3:ObjectRestore:*"|"s3:ObjectRestore:Post"|"s3:ObjectRestore:Completed"|"s3:Replication:*"|"s3:Replication:OperationFailedReplication"|"s3:Replication:OperationNotTracked"|"s3:Replication:OperationMissedThreshold"|"s3:Replication:OperationReplicatedAfterThreshold"|"s3:ObjectRestore:Delete"|"s3:LifecycleTransition"|"s3:IntelligentTiering"|"s3:ObjectAcl:Put"|"s3:LifecycleExpiration:*"|"s3:LifecycleExpiration:Delete"|"s3:LifecycleExpiration:DeleteMarkerCreated"|"s3:ObjectTagging:*"|"s3:ObjectTagging:Put"|"s3:ObjectTagging:Delete",
#'       Events = list(
#'         "s3:ReducedRedundancyLostObject"|"s3:ObjectCreated:*"|"s3:ObjectCreated:Put"|"s3:ObjectCreated:Post"|"s3:ObjectCreated:Copy"|"s3:ObjectCreated:CompleteMultipartUpload"|"s3:ObjectRemoved:*"|"s3:ObjectRemoved:Delete"|"s3:ObjectRemoved:DeleteMarkerCreated"|"s3:ObjectRestore:*"|"s3:ObjectRestore:Post"|"s3:ObjectRestore:Completed"|"s3:Replication:*"|"s3:Replication:OperationFailedReplication"|"s3:Replication:OperationNotTracked"|"s3:Replication:OperationMissedThreshold"|"s3:Replication:OperationReplicatedAfterThreshold"|"s3:ObjectRestore:Delete"|"s3:LifecycleTransition"|"s3:IntelligentTiering"|"s3:ObjectAcl:Put"|"s3:LifecycleExpiration:*"|"s3:LifecycleExpiration:Delete"|"s3:LifecycleExpiration:DeleteMarkerCreated"|"s3:ObjectTagging:*"|"s3:ObjectTagging:Put"|"s3:ObjectTagging:Delete"
#'       ),
#'       CloudFunction = "string",
#'       InvocationRole = "string"
#'     )
#'   ),
#'   ExpectedBucketOwner = "string"
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname s3_put_bucket_notification
#'
#' @aliases s3_put_bucket_notification
s3_put_bucket_notification <- function(Bucket, ContentMD5 = NULL, ChecksumAlgorithm = NULL, NotificationConfiguration, ExpectedBucketOwner = NULL) {
  op <- new_operation(
    name = "PutBucketNotification",
    http_method = "PUT",
    http_path = "/{Bucket}?notification",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .s3$put_bucket_notification_input(Bucket = Bucket, ContentMD5 = ContentMD5, ChecksumAlgorithm = ChecksumAlgorithm, NotificationConfiguration = NotificationConfiguration, ExpectedBucketOwner = ExpectedBucketOwner)
  output <- .s3$put_bucket_notification_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$put_bucket_notification <- s3_put_bucket_notification

#' This operation is not supported for directory buckets
#'
#' @description
#' This operation is not supported for directory buckets.
#' 
#' Enables notifications of specified events for a bucket. For more
#' information about event notifications, see [Configuring Event
#' Notifications](https://docs.aws.amazon.com/AmazonS3/latest/userguide/EventNotifications.html).
#' 
#' Using this API, you can replace an existing notification configuration.
#' The configuration is an XML file that defines the event types that you
#' want Amazon S3 to publish and the destination where you want Amazon S3
#' to publish an event notification when it detects an event of the
#' specified type.
#' 
#' By default, your bucket has no event notifications configured. That is,
#' the notification configuration will be an empty
#' `NotificationConfiguration`.
#' 
#' `<NotificationConfiguration>`
#' 
#' `</NotificationConfiguration>`
#' 
#' This action replaces the existing notification configuration with the
#' configuration you include in the request body.
#' 
#' After Amazon S3 receives this request, it first verifies that any Amazon
#' Simple Notification Service (Amazon SNS) or Amazon Simple Queue Service
#' (Amazon SQS) destination exists, and that the bucket owner has
#' permission to publish to it by sending a test notification. In the case
#' of Lambda destinations, Amazon S3 verifies that the Lambda function
#' permissions grant Amazon S3 permission to invoke the function from the
#' Amazon S3 bucket. For more information, see [Configuring Notifications
#' for Amazon S3
#' Events](https://docs.aws.amazon.com/AmazonS3/latest/userguide/EventNotifications.html).
#' 
#' You can disable notifications by adding the empty
#' NotificationConfiguration element.
#' 
#' For more information about the number of event notification
#' configurations that you can create per bucket, see [Amazon S3 service
#' quotas](https://docs.aws.amazon.com/general/latest/gr/s3.html#limits_s3)
#' in *Amazon Web Services General Reference*.
#' 
#' By default, only the bucket owner can configure notifications on a
#' bucket. However, bucket owners can use a bucket policy to grant
#' permission to other users to set this configuration with the required
#' `s3:PutBucketNotification` permission.
#' 
#' The PUT notification is an atomic operation. For example, suppose your
#' notification configuration includes SNS topic, SQS queue, and Lambda
#' function configurations. When you send a PUT request with this
#' configuration, Amazon S3 sends test messages to your SNS topic. If the
#' message fails, the entire PUT action will fail, and Amazon S3 will not
#' add the configuration to your bucket.
#' 
#' If the configuration in the request body includes only one
#' `TopicConfiguration` specifying only the
#' `s3:ReducedRedundancyLostObject` event type, the response will also
#' include the `x-amz-sns-test-message-id` header containing the message ID
#' of the test notification sent to the topic.
#' 
#' The following action is related to
#' [`put_bucket_notification_configuration`][s3_put_bucket_notification_configuration]:
#' 
#' -   [`get_bucket_notification_configuration`][s3_get_bucket_notification_configuration]
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#'
#' @usage
#' s3_put_bucket_notification_configuration(Bucket,
#'   NotificationConfiguration, ExpectedBucketOwner,
#'   SkipDestinationValidation)
#'
#' @param Bucket &#91;required&#93; The name of the bucket.
#' @param NotificationConfiguration &#91;required&#93; 
#' @param ExpectedBucketOwner The account ID of the expected bucket owner. If the account ID that you
#' provide does not match the actual owner of the bucket, the request fails
#' with the HTTP status code `403 Forbidden` (access denied).
#' @param SkipDestinationValidation Skips validation of Amazon SQS, Amazon SNS, and Lambda destinations.
#' True or false value.
#'
#' @return
#' An empty list.
#'
#' @section Request syntax:
#' ```
#' svc$put_bucket_notification_configuration(
#'   Bucket = "string",
#'   NotificationConfiguration = list(
#'     TopicConfigurations = list(
#'       list(
#'         Id = "string",
#'         TopicArn = "string",
#'         Events = list(
#'           "s3:ReducedRedundancyLostObject"|"s3:ObjectCreated:*"|"s3:ObjectCreated:Put"|"s3:ObjectCreated:Post"|"s3:ObjectCreated:Copy"|"s3:ObjectCreated:CompleteMultipartUpload"|"s3:ObjectRemoved:*"|"s3:ObjectRemoved:Delete"|"s3:ObjectRemoved:DeleteMarkerCreated"|"s3:ObjectRestore:*"|"s3:ObjectRestore:Post"|"s3:ObjectRestore:Completed"|"s3:Replication:*"|"s3:Replication:OperationFailedReplication"|"s3:Replication:OperationNotTracked"|"s3:Replication:OperationMissedThreshold"|"s3:Replication:OperationReplicatedAfterThreshold"|"s3:ObjectRestore:Delete"|"s3:LifecycleTransition"|"s3:IntelligentTiering"|"s3:ObjectAcl:Put"|"s3:LifecycleExpiration:*"|"s3:LifecycleExpiration:Delete"|"s3:LifecycleExpiration:DeleteMarkerCreated"|"s3:ObjectTagging:*"|"s3:ObjectTagging:Put"|"s3:ObjectTagging:Delete"
#'         ),
#'         Filter = list(
#'           Key = list(
#'             FilterRules = list(
#'               list(
#'                 Name = "prefix"|"suffix",
#'                 Value = "string"
#'               )
#'             )
#'           )
#'         )
#'       )
#'     ),
#'     QueueConfigurations = list(
#'       list(
#'         Id = "string",
#'         QueueArn = "string",
#'         Events = list(
#'           "s3:ReducedRedundancyLostObject"|"s3:ObjectCreated:*"|"s3:ObjectCreated:Put"|"s3:ObjectCreated:Post"|"s3:ObjectCreated:Copy"|"s3:ObjectCreated:CompleteMultipartUpload"|"s3:ObjectRemoved:*"|"s3:ObjectRemoved:Delete"|"s3:ObjectRemoved:DeleteMarkerCreated"|"s3:ObjectRestore:*"|"s3:ObjectRestore:Post"|"s3:ObjectRestore:Completed"|"s3:Replication:*"|"s3:Replication:OperationFailedReplication"|"s3:Replication:OperationNotTracked"|"s3:Replication:OperationMissedThreshold"|"s3:Replication:OperationReplicatedAfterThreshold"|"s3:ObjectRestore:Delete"|"s3:LifecycleTransition"|"s3:IntelligentTiering"|"s3:ObjectAcl:Put"|"s3:LifecycleExpiration:*"|"s3:LifecycleExpiration:Delete"|"s3:LifecycleExpiration:DeleteMarkerCreated"|"s3:ObjectTagging:*"|"s3:ObjectTagging:Put"|"s3:ObjectTagging:Delete"
#'         ),
#'         Filter = list(
#'           Key = list(
#'             FilterRules = list(
#'               list(
#'                 Name = "prefix"|"suffix",
#'                 Value = "string"
#'               )
#'             )
#'           )
#'         )
#'       )
#'     ),
#'     LambdaFunctionConfigurations = list(
#'       list(
#'         Id = "string",
#'         LambdaFunctionArn = "string",
#'         Events = list(
#'           "s3:ReducedRedundancyLostObject"|"s3:ObjectCreated:*"|"s3:ObjectCreated:Put"|"s3:ObjectCreated:Post"|"s3:ObjectCreated:Copy"|"s3:ObjectCreated:CompleteMultipartUpload"|"s3:ObjectRemoved:*"|"s3:ObjectRemoved:Delete"|"s3:ObjectRemoved:DeleteMarkerCreated"|"s3:ObjectRestore:*"|"s3:ObjectRestore:Post"|"s3:ObjectRestore:Completed"|"s3:Replication:*"|"s3:Replication:OperationFailedReplication"|"s3:Replication:OperationNotTracked"|"s3:Replication:OperationMissedThreshold"|"s3:Replication:OperationReplicatedAfterThreshold"|"s3:ObjectRestore:Delete"|"s3:LifecycleTransition"|"s3:IntelligentTiering"|"s3:ObjectAcl:Put"|"s3:LifecycleExpiration:*"|"s3:LifecycleExpiration:Delete"|"s3:LifecycleExpiration:DeleteMarkerCreated"|"s3:ObjectTagging:*"|"s3:ObjectTagging:Put"|"s3:ObjectTagging:Delete"
#'         ),
#'         Filter = list(
#'           Key = list(
#'             FilterRules = list(
#'               list(
#'                 Name = "prefix"|"suffix",
#'                 Value = "string"
#'               )
#'             )
#'           )
#'         )
#'       )
#'     ),
#'     EventBridgeConfiguration = list()
#'   ),
#'   ExpectedBucketOwner = "string",
#'   SkipDestinationValidation = TRUE|FALSE
#' )
#' ```
#'
#' @examples
#' \dontrun{
#' # The following example sets notification configuration on a bucket to
#' # publish the object created events to an SNS topic.
#' svc$put_bucket_notification_configuration(
#'   Bucket = "examplebucket",
#'   NotificationConfiguration = list(
#'     TopicConfigurations = list(
#'       list(
#'         Events = list(
#'           "s3:ObjectCreated:*"
#'         ),
#'         TopicArn = "arn:aws:sns:us-west-2:123456789012:s3-notification-topic"
#'       )
#'     )
#'   )
#' )
#' }
#'
#' @keywords internal
#'
#' @rdname s3_put_bucket_notification_configuration
#'
#' @aliases s3_put_bucket_notification_configuration
s3_put_bucket_notification_configuration <- function(Bucket, NotificationConfiguration, ExpectedBucketOwner = NULL, SkipDestinationValidation = NULL) {
  op <- new_operation(
    name = "PutBucketNotificationConfiguration",
    http_method = "PUT",
    http_path = "/{Bucket}?notification",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .s3$put_bucket_notification_configuration_input(Bucket = Bucket, NotificationConfiguration = NotificationConfiguration, ExpectedBucketOwner = ExpectedBucketOwner, SkipDestinationValidation = SkipDestinationValidation)
  output <- .s3$put_bucket_notification_configuration_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$put_bucket_notification_configuration <- s3_put_bucket_notification_configuration

#' This operation is not supported for directory buckets
#'
#' @description
#' This operation is not supported for directory buckets.
#' 
#' Creates or modifies `OwnershipControls` for an Amazon S3 bucket. To use
#' this operation, you must have the `s3:PutBucketOwnershipControls`
#' permission. For more information about Amazon S3 permissions, see
#' [Specifying permissions in a
#' policy](https://docs.aws.amazon.com/AmazonS3/latest/userguide/security_iam_service-with-iam.html#security_iam_service-with-iam-id-based-policies-actions).
#' 
#' For information about Amazon S3 Object Ownership, see [Using object
#' ownership](https://docs.aws.amazon.com/AmazonS3/latest/userguide//about-object-ownership.html).
#' 
#' The following operations are related to
#' [`put_bucket_ownership_controls`][s3_put_bucket_ownership_controls]:
#' 
#' -   [`get_bucket_ownership_controls`][s3_get_bucket_ownership_controls]
#' 
#' -   [`delete_bucket_ownership_controls`][s3_delete_bucket_ownership_controls]
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#'
#' @usage
#' s3_put_bucket_ownership_controls(Bucket, ContentMD5,
#'   ExpectedBucketOwner, OwnershipControls, ChecksumAlgorithm)
#'
#' @param Bucket &#91;required&#93; The name of the Amazon S3 bucket whose `OwnershipControls` you want to
#' set.
#' @param ContentMD5 The MD5 hash of the `OwnershipControls` request body.
#' 
#' For requests made using the Amazon Web Services Command Line Interface
#' (CLI) or Amazon Web Services SDKs, this field is calculated
#' automatically.
#' @param ExpectedBucketOwner The account ID of the expected bucket owner. If the account ID that you
#' provide does not match the actual owner of the bucket, the request fails
#' with the HTTP status code `403 Forbidden` (access denied).
#' @param OwnershipControls &#91;required&#93; The `OwnershipControls` (BucketOwnerEnforced, BucketOwnerPreferred, or
#' ObjectWriter) that you want to apply to this Amazon S3 bucket.
#' @param ChecksumAlgorithm Indicates the algorithm used to create the checksum for the object when
#' you use the SDK. This header will not provide any additional
#' functionality if you don't use the SDK. When you send this header, there
#' must be a corresponding `x-amz-checksum-algorithm ` header sent.
#' Otherwise, Amazon S3 fails the request with the HTTP status code
#' `400 Bad Request`. For more information, see [Checking object
#' integrity](https://docs.aws.amazon.com/AmazonS3/latest/userguide/checking-object-integrity.html)
#' in the *Amazon S3 User Guide*.
#' 
#' If you provide an individual checksum, Amazon S3 ignores any provided
#' `ChecksumAlgorithm` parameter.
#'
#' @return
#' An empty list.
#'
#' @section Request syntax:
#' ```
#' svc$put_bucket_ownership_controls(
#'   Bucket = "string",
#'   ContentMD5 = "string",
#'   ExpectedBucketOwner = "string",
#'   OwnershipControls = list(
#'     Rules = list(
#'       list(
#'         ObjectOwnership = "BucketOwnerPreferred"|"ObjectWriter"|"BucketOwnerEnforced"
#'       )
#'     )
#'   ),
#'   ChecksumAlgorithm = "CRC32"|"CRC32C"|"SHA1"|"SHA256"|"CRC64NVME"
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname s3_put_bucket_ownership_controls
#'
#' @aliases s3_put_bucket_ownership_controls
s3_put_bucket_ownership_controls <- function(Bucket, ContentMD5 = NULL, ExpectedBucketOwner = NULL, OwnershipControls, ChecksumAlgorithm = NULL) {
  op <- new_operation(
    name = "PutBucketOwnershipControls",
    http_method = "PUT",
    http_path = "/{Bucket}?ownershipControls",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .s3$put_bucket_ownership_controls_input(Bucket = Bucket, ContentMD5 = ContentMD5, ExpectedBucketOwner = ExpectedBucketOwner, OwnershipControls = OwnershipControls, ChecksumAlgorithm = ChecksumAlgorithm)
  output <- .s3$put_bucket_ownership_controls_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$put_bucket_ownership_controls <- s3_put_bucket_ownership_controls

#' Applies an Amazon S3 bucket policy to an Amazon S3 bucket
#'
#' @description
#' Applies an Amazon S3 bucket policy to an Amazon S3 bucket.
#' 
#' **Directory buckets** - For directory buckets, you must make requests
#' for this API operation to the Regional endpoint. These endpoints support
#' path-style requests in the format
#' `https://s3express-control.region-code.amazonaws.com/bucket-name `.
#' Virtual-hosted-style requests aren't supported. For more information
#' about endpoints in Availability Zones, see [Regional and Zonal endpoints
#' for directory buckets in Availability
#' Zones](https://docs.aws.amazon.com/AmazonS3/latest/userguide/endpoint-directory-buckets-AZ.html)
#' in the *Amazon S3 User Guide*. For more information about endpoints in
#' Local Zones, see [Concepts for directory buckets in Local
#' Zones](https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-lzs-for-directory-buckets.html)
#' in the *Amazon S3 User Guide*.
#' 
#' ### Permissions
#' 
#' If you are using an identity other than the root user of the Amazon Web
#' Services account that owns the bucket, the calling identity must both
#' have the [`put_bucket_policy`][s3_put_bucket_policy] permissions on the
#' specified bucket and belong to the bucket owner's account in order to
#' use this operation.
#' 
#' If you don't have [`put_bucket_policy`][s3_put_bucket_policy]
#' permissions, Amazon S3 returns a `403 Access Denied` error. If you have
#' the correct permissions, but you're not using an identity that belongs
#' to the bucket owner's account, Amazon S3 returns a
#' `405 Method Not Allowed` error.
#' 
#' To ensure that bucket owners don't inadvertently lock themselves out of
#' their own buckets, the root principal in a bucket owner's Amazon Web
#' Services account can perform the
#' [`get_bucket_policy`][s3_get_bucket_policy],
#' [`put_bucket_policy`][s3_put_bucket_policy], and
#' [`delete_bucket_policy`][s3_delete_bucket_policy] API actions, even if
#' their bucket policy explicitly denies the root principal's access.
#' Bucket owner root principals can only be blocked from performing these
#' API actions by VPC endpoint policies and Amazon Web Services
#' Organizations policies.
#' 
#' -   **General purpose bucket permissions** - The `s3:PutBucketPolicy`
#'     permission is required in a policy. For more information about
#'     general purpose buckets bucket policies, see [Using Bucket Policies
#'     and User
#'     Policies](https://docs.aws.amazon.com/AmazonS3/latest/userguide/security_iam_service-with-iam.html)
#'     in the *Amazon S3 User Guide*.
#' 
#' -   **Directory bucket permissions** - To grant access to this API
#'     operation, you must have the `s3express:PutBucketPolicy` permission
#'     in an IAM identity-based policy instead of a bucket policy.
#'     Cross-account access to this API operation isn't supported. This
#'     operation can only be performed by the Amazon Web Services account
#'     that owns the resource. For more information about directory bucket
#'     policies and permissions, see [Amazon Web Services Identity and
#'     Access Management (IAM) for S3 Express One
#'     Zone](https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-express-security-iam.html)
#'     in the *Amazon S3 User Guide*.
#' 
#' ### Example bucket policies
#' 
#' **General purpose buckets example bucket policies** - See [Bucket policy
#' examples](https://docs.aws.amazon.com/AmazonS3/latest/userguide/example-bucket-policies.html)
#' in the *Amazon S3 User Guide*.
#' 
#' **Directory bucket example bucket policies** - See [Example bucket
#' policies for S3 Express One
#' Zone](https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-express-security-iam-example-bucket-policies.html)
#' in the *Amazon S3 User Guide*.
#' 
#' ### HTTP Host header syntax
#' 
#' **Directory buckets** - The HTTP Host header syntax is
#' `s3express-control.region-code.amazonaws.com`.
#' 
#' The following operations are related to
#' [`put_bucket_policy`][s3_put_bucket_policy]:
#' 
#' -   [`create_bucket`][s3_create_bucket]
#' 
#' -   [`delete_bucket`][s3_delete_bucket]
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#'
#' @usage
#' s3_put_bucket_policy(Bucket, ContentMD5, ChecksumAlgorithm,
#'   ConfirmRemoveSelfBucketAccess, Policy, ExpectedBucketOwner)
#'
#' @param Bucket &#91;required&#93; The name of the bucket.
#' 
#' **Directory buckets** - When you use this operation with a directory
#' bucket, you must use path-style requests in the format
#' `https://s3express-control.region-code.amazonaws.com/bucket-name `.
#' Virtual-hosted-style requests aren't supported. Directory bucket names
#' must be unique in the chosen Zone (Availability Zone or Local Zone).
#' Bucket names must also follow the format
#' ` bucket-base-name--zone-id--x-s3` (for example,
#' ` DOC-EXAMPLE-BUCKET--usw2-az1--x-s3`). For information about bucket
#' naming restrictions, see [Directory bucket naming
#' rules](https://docs.aws.amazon.com/AmazonS3/latest/userguide/directory-bucket-naming-rules.html)
#' in the *Amazon S3 User Guide*
#' @param ContentMD5 The MD5 hash of the request body.
#' 
#' For requests made using the Amazon Web Services Command Line Interface
#' (CLI) or Amazon Web Services SDKs, this field is calculated
#' automatically.
#' 
#' This functionality is not supported for directory buckets.
#' @param ChecksumAlgorithm Indicates the algorithm used to create the checksum for the request when
#' you use the SDK. This header will not provide any additional
#' functionality if you don't use the SDK. When you send this header, there
#' must be a corresponding `x-amz-checksum-algorithm ` or `x-amz-trailer`
#' header sent. Otherwise, Amazon S3 fails the request with the HTTP status
#' code `400 Bad Request`.
#' 
#' For the `x-amz-checksum-algorithm ` header, replace ` algorithm ` with
#' the supported algorithm from the following list:
#' 
#' -   `CRC32`
#' 
#' -   `CRC32C`
#' 
#' -   `CRC64NVME`
#' 
#' -   `SHA1`
#' 
#' -   `SHA256`
#' 
#' For more information, see [Checking object
#' integrity](https://docs.aws.amazon.com/AmazonS3/latest/userguide/checking-object-integrity.html)
#' in the *Amazon S3 User Guide*.
#' 
#' If the individual checksum value you provide through
#' `x-amz-checksum-algorithm ` doesn't match the checksum algorithm you set
#' through `x-amz-sdk-checksum-algorithm`, Amazon S3 fails the request with
#' a `BadDigest` error.
#' 
#' For directory buckets, when you use Amazon Web Services SDKs, `CRC32` is
#' the default checksum algorithm that's used for performance.
#' @param ConfirmRemoveSelfBucketAccess Set this parameter to true to confirm that you want to remove your
#' permissions to change this bucket policy in the future.
#' 
#' This functionality is not supported for directory buckets.
#' @param Policy &#91;required&#93; The bucket policy as a JSON document.
#' 
#' For directory buckets, the only IAM action supported in the bucket
#' policy is `s3express:CreateSession`.
#' @param ExpectedBucketOwner The account ID of the expected bucket owner. If the account ID that you
#' provide does not match the actual owner of the bucket, the request fails
#' with the HTTP status code `403 Forbidden` (access denied).
#' 
#' For directory buckets, this header is not supported in this API
#' operation. If you specify this header, the request fails with the HTTP
#' status code `501 Not Implemented`.
#'
#' @return
#' An empty list.
#'
#' @section Request syntax:
#' ```
#' svc$put_bucket_policy(
#'   Bucket = "string",
#'   ContentMD5 = "string",
#'   ChecksumAlgorithm = "CRC32"|"CRC32C"|"SHA1"|"SHA256"|"CRC64NVME",
#'   ConfirmRemoveSelfBucketAccess = TRUE|FALSE,
#'   Policy = "string",
#'   ExpectedBucketOwner = "string"
#' )
#' ```
#'
#' @examples
#' \dontrun{
#' # The following example sets a permission policy on a bucket.
#' svc$put_bucket_policy(
#'   Bucket = "examplebucket",
#'   Policy = "\{\"Version\": \"2012-10-17\", \"Statement\": [\{ \"Sid\": \"id-1\",\"Effect..."
#' )
#' }
#'
#' @keywords internal
#'
#' @rdname s3_put_bucket_policy
#'
#' @aliases s3_put_bucket_policy
s3_put_bucket_policy <- function(Bucket, ContentMD5 = NULL, ChecksumAlgorithm = NULL, ConfirmRemoveSelfBucketAccess = NULL, Policy, ExpectedBucketOwner = NULL) {
  op <- new_operation(
    name = "PutBucketPolicy",
    http_method = "PUT",
    http_path = "/{Bucket}?policy",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .s3$put_bucket_policy_input(Bucket = Bucket, ContentMD5 = ContentMD5, ChecksumAlgorithm = ChecksumAlgorithm, ConfirmRemoveSelfBucketAccess = ConfirmRemoveSelfBucketAccess, Policy = Policy, ExpectedBucketOwner = ExpectedBucketOwner)
  output <- .s3$put_bucket_policy_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$put_bucket_policy <- s3_put_bucket_policy

#' This operation is not supported for directory buckets
#'
#' @description
#' This operation is not supported for directory buckets.
#' 
#' Creates a replication configuration or replaces an existing one. For
#' more information, see
#' [Replication](https://docs.aws.amazon.com/AmazonS3/latest/userguide/replication.html)
#' in the *Amazon S3 User Guide*.
#' 
#' Specify the replication configuration in the request body. In the
#' replication configuration, you provide the name of the destination
#' bucket or buckets where you want Amazon S3 to replicate objects, the IAM
#' role that Amazon S3 can assume to replicate objects on your behalf, and
#' other relevant information. You can invoke this request for a specific
#' Amazon Web Services Region by using the
#' [`aws:RequestedRegion`](https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_condition-keys.html#condition-keys-requestedregion)
#' condition key.
#' 
#' A replication configuration must include at least one rule, and can
#' contain a maximum of 1,000. Each rule identifies a subset of objects to
#' replicate by filtering the objects in the source bucket. To choose
#' additional subsets of objects to replicate, add a rule for each subset.
#' 
#' To specify a subset of the objects in the source bucket to apply a
#' replication rule to, add the Filter element as a child of the Rule
#' element. You can filter objects based on an object key prefix, one or
#' more object tags, or both. When you add the Filter element in the
#' configuration, you must also add the following elements:
#' `DeleteMarkerReplication`, `Status`, and `Priority`.
#' 
#' If you are using an earlier version of the replication configuration,
#' Amazon S3 handles replication of delete markers differently. For more
#' information, see [Backward
#' Compatibility](https://docs.aws.amazon.com/AmazonS3/latest/userguide/replication-add-config.html#replication-backward-compat-considerations).
#' 
#' For information about enabling versioning on a bucket, see [Using
#' Versioning](https://docs.aws.amazon.com/AmazonS3/latest/userguide/Versioning.html).
#' 
#' ### Handling Replication of Encrypted Objects
#' 
#' By default, Amazon S3 doesn't replicate objects that are stored at rest
#' using server-side encryption with KMS keys. To replicate Amazon Web
#' Services KMS-encrypted objects, add the following:
#' `SourceSelectionCriteria`, `SseKmsEncryptedObjects`, `Status`,
#' `EncryptionConfiguration`, and `ReplicaKmsKeyID`. For information about
#' replication configuration, see [Replicating Objects Created with SSE
#' Using KMS
#' keys](https://docs.aws.amazon.com/AmazonS3/latest/userguide/replication-config-for-kms-objects.html).
#' 
#' For information on [`put_bucket_replication`][s3_put_bucket_replication]
#' errors, see [List of replication-related error
#' codes](https://docs.aws.amazon.com/AmazonS3/latest/API/ErrorResponses.html#ReplicationErrorCodeList)
#' 
#' ### Permissions
#' 
#' To create a [`put_bucket_replication`][s3_put_bucket_replication]
#' request, you must have `s3:PutReplicationConfiguration` permissions for
#' the bucket.
#' 
#' By default, a resource owner, in this case the Amazon Web Services
#' account that created the bucket, can perform this operation. The
#' resource owner can also grant others permissions to perform the
#' operation. For more information about permissions, see [Specifying
#' Permissions in a
#' Policy](https://docs.aws.amazon.com/AmazonS3/latest/userguide/security_iam_service-with-iam.html#security_iam_service-with-iam-id-based-policies-actions)
#' and [Managing Access Permissions to Your Amazon S3
#' Resources](https://docs.aws.amazon.com/AmazonS3/latest/userguide/security-iam.html).
#' 
#' To perform this operation, the user or role performing the action must
#' have the
#' [iam:PassRole](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_passrole.html)
#' permission.
#' 
#' The following operations are related to
#' [`put_bucket_replication`][s3_put_bucket_replication]:
#' 
#' -   [`get_bucket_replication`][s3_get_bucket_replication]
#' 
#' -   [`delete_bucket_replication`][s3_delete_bucket_replication]
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#'
#' @usage
#' s3_put_bucket_replication(Bucket, ContentMD5, ChecksumAlgorithm,
#'   ReplicationConfiguration, Token, ExpectedBucketOwner)
#'
#' @param Bucket &#91;required&#93; The name of the bucket
#' @param ContentMD5 The Base64 encoded 128-bit `MD5` digest of the data. You must use this
#' header as a message integrity check to verify that the request body was
#' not corrupted in transit. For more information, see [RFC
#' 1864](https://www.ietf.org/rfc/rfc1864.txt).
#' 
#' For requests made using the Amazon Web Services Command Line Interface
#' (CLI) or Amazon Web Services SDKs, this field is calculated
#' automatically.
#' @param ChecksumAlgorithm Indicates the algorithm used to create the checksum for the request when
#' you use the SDK. This header will not provide any additional
#' functionality if you don't use the SDK. When you send this header, there
#' must be a corresponding `x-amz-checksum` or `x-amz-trailer` header sent.
#' Otherwise, Amazon S3 fails the request with the HTTP status code
#' `400 Bad Request`. For more information, see [Checking object
#' integrity](https://docs.aws.amazon.com/AmazonS3/latest/userguide/checking-object-integrity.html)
#' in the *Amazon S3 User Guide*.
#' 
#' If you provide an individual checksum, Amazon S3 ignores any provided
#' `ChecksumAlgorithm` parameter.
#' @param ReplicationConfiguration &#91;required&#93; 
#' @param Token A token to allow Object Lock to be enabled for an existing bucket.
#' @param ExpectedBucketOwner The account ID of the expected bucket owner. If the account ID that you
#' provide does not match the actual owner of the bucket, the request fails
#' with the HTTP status code `403 Forbidden` (access denied).
#'
#' @return
#' An empty list.
#'
#' @section Request syntax:
#' ```
#' svc$put_bucket_replication(
#'   Bucket = "string",
#'   ContentMD5 = "string",
#'   ChecksumAlgorithm = "CRC32"|"CRC32C"|"SHA1"|"SHA256"|"CRC64NVME",
#'   ReplicationConfiguration = list(
#'     Role = "string",
#'     Rules = list(
#'       list(
#'         ID = "string",
#'         Priority = 123,
#'         Prefix = "string",
#'         Filter = list(
#'           Prefix = "string",
#'           Tag = list(
#'             Key = "string",
#'             Value = "string"
#'           ),
#'           And = list(
#'             Prefix = "string",
#'             Tags = list(
#'               list(
#'                 Key = "string",
#'                 Value = "string"
#'               )
#'             )
#'           )
#'         ),
#'         Status = "Enabled"|"Disabled",
#'         SourceSelectionCriteria = list(
#'           SseKmsEncryptedObjects = list(
#'             Status = "Enabled"|"Disabled"
#'           ),
#'           ReplicaModifications = list(
#'             Status = "Enabled"|"Disabled"
#'           )
#'         ),
#'         ExistingObjectReplication = list(
#'           Status = "Enabled"|"Disabled"
#'         ),
#'         Destination = list(
#'           Bucket = "string",
#'           Account = "string",
#'           StorageClass = "STANDARD"|"REDUCED_REDUNDANCY"|"STANDARD_IA"|"ONEZONE_IA"|"INTELLIGENT_TIERING"|"GLACIER"|"DEEP_ARCHIVE"|"OUTPOSTS"|"GLACIER_IR"|"SNOW"|"EXPRESS_ONEZONE"|"FSX_OPENZFS"|"FSX_ONTAP",
#'           AccessControlTranslation = list(
#'             Owner = "Destination"
#'           ),
#'           EncryptionConfiguration = list(
#'             ReplicaKmsKeyID = "string"
#'           ),
#'           ReplicationTime = list(
#'             Status = "Enabled"|"Disabled",
#'             Time = list(
#'               Minutes = 123
#'             )
#'           ),
#'           Metrics = list(
#'             Status = "Enabled"|"Disabled",
#'             EventThreshold = list(
#'               Minutes = 123
#'             )
#'           )
#'         ),
#'         DeleteMarkerReplication = list(
#'           Status = "Enabled"|"Disabled"
#'         )
#'       )
#'     )
#'   ),
#'   Token = "string",
#'   ExpectedBucketOwner = "string"
#' )
#' ```
#'
#' @examples
#' \dontrun{
#' # The following example sets replication configuration on a bucket.
#' svc$put_bucket_replication(
#'   Bucket = "examplebucket",
#'   ReplicationConfiguration = list(
#'     Role = "arn:aws:iam::123456789012:role/examplerole",
#'     Rules = list(
#'       list(
#'         Destination = list(
#'           Bucket = "arn:aws:s3:::destinationbucket",
#'           StorageClass = "STANDARD"
#'         ),
#'         Prefix = "",
#'         Status = "Enabled"
#'       )
#'     )
#'   )
#' )
#' }
#'
#' @keywords internal
#'
#' @rdname s3_put_bucket_replication
#'
#' @aliases s3_put_bucket_replication
s3_put_bucket_replication <- function(Bucket, ContentMD5 = NULL, ChecksumAlgorithm = NULL, ReplicationConfiguration, Token = NULL, ExpectedBucketOwner = NULL) {
  op <- new_operation(
    name = "PutBucketReplication",
    http_method = "PUT",
    http_path = "/{Bucket}?replication",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .s3$put_bucket_replication_input(Bucket = Bucket, ContentMD5 = ContentMD5, ChecksumAlgorithm = ChecksumAlgorithm, ReplicationConfiguration = ReplicationConfiguration, Token = Token, ExpectedBucketOwner = ExpectedBucketOwner)
  output <- .s3$put_bucket_replication_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$put_bucket_replication <- s3_put_bucket_replication

#' This operation is not supported for directory buckets
#'
#' @description
#' This operation is not supported for directory buckets.
#' 
#' Sets the request payment configuration for a bucket. By default, the
#' bucket owner pays for downloads from the bucket. This configuration
#' parameter enables the bucket owner (only) to specify that the person
#' requesting the download will be charged for the download. For more
#' information, see [Requester Pays
#' Buckets](https://docs.aws.amazon.com/AmazonS3/latest/userguide/RequesterPaysBuckets.html).
#' 
#' The following operations are related to
#' [`put_bucket_request_payment`][s3_put_bucket_request_payment]:
#' 
#' -   [`create_bucket`][s3_create_bucket]
#' 
#' -   [`get_bucket_request_payment`][s3_get_bucket_request_payment]
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#'
#' @usage
#' s3_put_bucket_request_payment(Bucket, ContentMD5, ChecksumAlgorithm,
#'   RequestPaymentConfiguration, ExpectedBucketOwner)
#'
#' @param Bucket &#91;required&#93; The bucket name.
#' @param ContentMD5 The Base64 encoded 128-bit `MD5` digest of the data. You must use this
#' header as a message integrity check to verify that the request body was
#' not corrupted in transit. For more information, see [RFC
#' 1864](https://www.ietf.org/rfc/rfc1864.txt).
#' 
#' For requests made using the Amazon Web Services Command Line Interface
#' (CLI) or Amazon Web Services SDKs, this field is calculated
#' automatically.
#' @param ChecksumAlgorithm Indicates the algorithm used to create the checksum for the request when
#' you use the SDK. This header will not provide any additional
#' functionality if you don't use the SDK. When you send this header, there
#' must be a corresponding `x-amz-checksum` or `x-amz-trailer` header sent.
#' Otherwise, Amazon S3 fails the request with the HTTP status code
#' `400 Bad Request`. For more information, see [Checking object
#' integrity](https://docs.aws.amazon.com/AmazonS3/latest/userguide/checking-object-integrity.html)
#' in the *Amazon S3 User Guide*.
#' 
#' If you provide an individual checksum, Amazon S3 ignores any provided
#' `ChecksumAlgorithm` parameter.
#' @param RequestPaymentConfiguration &#91;required&#93; Container for Payer.
#' @param ExpectedBucketOwner The account ID of the expected bucket owner. If the account ID that you
#' provide does not match the actual owner of the bucket, the request fails
#' with the HTTP status code `403 Forbidden` (access denied).
#'
#' @return
#' An empty list.
#'
#' @section Request syntax:
#' ```
#' svc$put_bucket_request_payment(
#'   Bucket = "string",
#'   ContentMD5 = "string",
#'   ChecksumAlgorithm = "CRC32"|"CRC32C"|"SHA1"|"SHA256"|"CRC64NVME",
#'   RequestPaymentConfiguration = list(
#'     Payer = "Requester"|"BucketOwner"
#'   ),
#'   ExpectedBucketOwner = "string"
#' )
#' ```
#'
#' @examples
#' \dontrun{
#' # The following example sets request payment configuration on a bucket so
#' # that person requesting the download is charged.
#' svc$put_bucket_request_payment(
#'   Bucket = "examplebucket",
#'   RequestPaymentConfiguration = list(
#'     Payer = "Requester"
#'   )
#' )
#' }
#'
#' @keywords internal
#'
#' @rdname s3_put_bucket_request_payment
#'
#' @aliases s3_put_bucket_request_payment
s3_put_bucket_request_payment <- function(Bucket, ContentMD5 = NULL, ChecksumAlgorithm = NULL, RequestPaymentConfiguration, ExpectedBucketOwner = NULL) {
  op <- new_operation(
    name = "PutBucketRequestPayment",
    http_method = "PUT",
    http_path = "/{Bucket}?requestPayment",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .s3$put_bucket_request_payment_input(Bucket = Bucket, ContentMD5 = ContentMD5, ChecksumAlgorithm = ChecksumAlgorithm, RequestPaymentConfiguration = RequestPaymentConfiguration, ExpectedBucketOwner = ExpectedBucketOwner)
  output <- .s3$put_bucket_request_payment_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$put_bucket_request_payment <- s3_put_bucket_request_payment

#' This operation is not supported for directory buckets
#'
#' @description
#' This operation is not supported for directory buckets.
#' 
#' Sets the tags for a general purpose bucket if attribute based access
#' control (ABAC) is not enabled for the bucket. When you [enable ABAC for
#' a general purpose
#' bucket](https://docs.aws.amazon.com/AmazonS3/latest/userguide/buckets-tagging-enable-abac.html),
#' you can no longer use this operation for that bucket and must use the
#' [TagResource](https://docs.aws.amazon.com/AmazonS3/latest/API/API_control_TagResource.html)
#' or
#' [UntagResource](https://docs.aws.amazon.com/AmazonS3/latest/API/API_control_UntagResource.html)
#' operations instead.
#' 
#' Use tags to organize your Amazon Web Services bill to reflect your own
#' cost structure. To do this, sign up to get your Amazon Web Services
#' account bill with tag key values included. Then, to see the cost of
#' combined resources, organize your billing information according to
#' resources with the same tag key values. For example, you can tag several
#' resources with a specific application name, and then organize your
#' billing information to see the total cost of that application across
#' several services. For more information, see [Cost Allocation and
#' Tagging](https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/cost-alloc-tags.html)
#' and [Using Cost Allocation in Amazon S3 Bucket
#' Tags](https://docs.aws.amazon.com/AmazonS3/latest/userguide/CostAllocTagging.html).
#' 
#' When this operation sets the tags for a bucket, it will overwrite any
#' current tags the bucket already has. You cannot use this operation to
#' add tags to an existing list of tags.
#' 
#' To use this operation, you must have permissions to perform the
#' `s3:PutBucketTagging` action. The bucket owner has this permission by
#' default and can grant this permission to others. For more information
#' about permissions, see [Permissions Related to Bucket Subresource
#' Operations](https://docs.aws.amazon.com/AmazonS3/latest/userguide/security_iam_service-with-iam.html#security_iam_service-with-iam-id-based-policies-actions)
#' and [Managing Access Permissions to Your Amazon S3
#' Resources](https://docs.aws.amazon.com/AmazonS3/latest/userguide/security-iam.html).
#' 
#' [`put_bucket_tagging`][s3_put_bucket_tagging] has the following special
#' errors. For more Amazon S3 errors see, [Error
#' Responses](https://docs.aws.amazon.com/AmazonS3/latest/API/ErrorResponses.html).
#' 
#' -   `InvalidTag` - The tag provided was not a valid tag. This error can
#'     occur if the tag did not pass input validation. For more
#'     information, see [Using Cost Allocation in Amazon S3 Bucket
#'     Tags](https://docs.aws.amazon.com/AmazonS3/latest/userguide/CostAllocTagging.html).
#' 
#' -   `MalformedXML` - The XML provided does not match the schema.
#' 
#' -   `OperationAborted` - A conflicting conditional action is currently
#'     in progress against this resource. Please try again.
#' 
#' -   `InternalError` - The service was unable to apply the provided tag
#'     to the bucket.
#' 
#' The following operations are related to
#' [`put_bucket_tagging`][s3_put_bucket_tagging]:
#' 
#' -   [`get_bucket_tagging`][s3_get_bucket_tagging]
#' 
#' -   [`delete_bucket_tagging`][s3_delete_bucket_tagging]
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#'
#' @usage
#' s3_put_bucket_tagging(Bucket, ContentMD5, ChecksumAlgorithm, Tagging,
#'   ExpectedBucketOwner)
#'
#' @param Bucket &#91;required&#93; The bucket name.
#' @param ContentMD5 The Base64 encoded 128-bit `MD5` digest of the data. You must use this
#' header as a message integrity check to verify that the request body was
#' not corrupted in transit. For more information, see [RFC
#' 1864](https://www.ietf.org/rfc/rfc1864.txt).
#' 
#' For requests made using the Amazon Web Services Command Line Interface
#' (CLI) or Amazon Web Services SDKs, this field is calculated
#' automatically.
#' @param ChecksumAlgorithm Indicates the algorithm used to create the checksum for the request when
#' you use the SDK. This header will not provide any additional
#' functionality if you don't use the SDK. When you send this header, there
#' must be a corresponding `x-amz-checksum` or `x-amz-trailer` header sent.
#' Otherwise, Amazon S3 fails the request with the HTTP status code
#' `400 Bad Request`. For more information, see [Checking object
#' integrity](https://docs.aws.amazon.com/AmazonS3/latest/userguide/checking-object-integrity.html)
#' in the *Amazon S3 User Guide*.
#' 
#' If you provide an individual checksum, Amazon S3 ignores any provided
#' `ChecksumAlgorithm` parameter.
#' @param Tagging &#91;required&#93; Container for the `TagSet` and `Tag` elements.
#' @param ExpectedBucketOwner The account ID of the expected bucket owner. If the account ID that you
#' provide does not match the actual owner of the bucket, the request fails
#' with the HTTP status code `403 Forbidden` (access denied).
#'
#' @return
#' An empty list.
#'
#' @section Request syntax:
#' ```
#' svc$put_bucket_tagging(
#'   Bucket = "string",
#'   ContentMD5 = "string",
#'   ChecksumAlgorithm = "CRC32"|"CRC32C"|"SHA1"|"SHA256"|"CRC64NVME",
#'   Tagging = list(
#'     TagSet = list(
#'       list(
#'         Key = "string",
#'         Value = "string"
#'       )
#'     )
#'   ),
#'   ExpectedBucketOwner = "string"
#' )
#' ```
#'
#' @examples
#' \dontrun{
#' # The following example sets tags on a bucket. Any existing tags are
#' # replaced.
#' svc$put_bucket_tagging(
#'   Bucket = "examplebucket",
#'   Tagging = list(
#'     TagSet = list(
#'       list(
#'         Key = "Key1",
#'         Value = "Value1"
#'       ),
#'       list(
#'         Key = "Key2",
#'         Value = "Value2"
#'       )
#'     )
#'   )
#' )
#' }
#'
#' @keywords internal
#'
#' @rdname s3_put_bucket_tagging
#'
#' @aliases s3_put_bucket_tagging
s3_put_bucket_tagging <- function(Bucket, ContentMD5 = NULL, ChecksumAlgorithm = NULL, Tagging, ExpectedBucketOwner = NULL) {
  op <- new_operation(
    name = "PutBucketTagging",
    http_method = "PUT",
    http_path = "/{Bucket}?tagging",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .s3$put_bucket_tagging_input(Bucket = Bucket, ContentMD5 = ContentMD5, ChecksumAlgorithm = ChecksumAlgorithm, Tagging = Tagging, ExpectedBucketOwner = ExpectedBucketOwner)
  output <- .s3$put_bucket_tagging_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$put_bucket_tagging <- s3_put_bucket_tagging

#' This operation is not supported for directory buckets
#'
#' @description
#' This operation is not supported for directory buckets.
#' 
#' When you enable versioning on a bucket for the first time, it might take
#' a short amount of time for the change to be fully propagated. While this
#' change is propagating, you might encounter intermittent
#' `HTTP 404 NoSuchKey` errors for requests to objects created or updated
#' after enabling versioning. We recommend that you wait for 15 minutes
#' after enabling versioning before issuing write operations (`PUT` or
#' `DELETE`) on objects in the bucket.
#' 
#' Sets the versioning state of an existing bucket.
#' 
#' You can set the versioning state with one of the following values:
#' 
#' **Enabled**—Enables versioning for the objects in the bucket. All
#' objects added to the bucket receive a unique version ID.
#' 
#' **Suspended**—Disables versioning for the objects in the bucket. All
#' objects added to the bucket receive the version ID null.
#' 
#' If the versioning state has never been set on a bucket, it has no
#' versioning state; a [`get_bucket_versioning`][s3_get_bucket_versioning]
#' request does not return a versioning state value.
#' 
#' In order to enable MFA Delete, you must be the bucket owner. If you are
#' the bucket owner and want to enable MFA Delete in the bucket versioning
#' configuration, you must include the `x-amz-mfa request` header and the
#' `Status` and the `MfaDelete` request elements in a request to set the
#' versioning state of the bucket.
#' 
#' If you have an object expiration lifecycle configuration in your
#' non-versioned bucket and you want to maintain the same permanent delete
#' behavior when you enable versioning, you must add a noncurrent
#' expiration policy. The noncurrent expiration lifecycle configuration
#' will manage the deletes of the noncurrent object versions in the
#' version-enabled bucket. (A version-enabled bucket maintains one current
#' and zero or more noncurrent object versions.) For more information, see
#' [Lifecycle and
#' Versioning](https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lifecycle-mgmt.html#lifecycle-and-other-bucket-config).
#' 
#' The following operations are related to
#' [`put_bucket_versioning`][s3_put_bucket_versioning]:
#' 
#' -   [`create_bucket`][s3_create_bucket]
#' 
#' -   [`delete_bucket`][s3_delete_bucket]
#' 
#' -   [`get_bucket_versioning`][s3_get_bucket_versioning]
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#'
#' @usage
#' s3_put_bucket_versioning(Bucket, ContentMD5, ChecksumAlgorithm, MFA,
#'   VersioningConfiguration, ExpectedBucketOwner)
#'
#' @param Bucket &#91;required&#93; The bucket name.
#' @param ContentMD5 \>The Base64 encoded 128-bit `MD5` digest of the data. You must use this
#' header as a message integrity check to verify that the request body was
#' not corrupted in transit. For more information, see [RFC
#' 1864](https://www.ietf.org/rfc/rfc1864.txt).
#' 
#' For requests made using the Amazon Web Services Command Line Interface
#' (CLI) or Amazon Web Services SDKs, this field is calculated
#' automatically.
#' @param ChecksumAlgorithm Indicates the algorithm used to create the checksum for the request when
#' you use the SDK. This header will not provide any additional
#' functionality if you don't use the SDK. When you send this header, there
#' must be a corresponding `x-amz-checksum` or `x-amz-trailer` header sent.
#' Otherwise, Amazon S3 fails the request with the HTTP status code
#' `400 Bad Request`. For more information, see [Checking object
#' integrity](https://docs.aws.amazon.com/AmazonS3/latest/userguide/checking-object-integrity.html)
#' in the *Amazon S3 User Guide*.
#' 
#' If you provide an individual checksum, Amazon S3 ignores any provided
#' `ChecksumAlgorithm` parameter.
#' @param MFA The concatenation of the authentication device's serial number, a space,
#' and the value that is displayed on your authentication device. The
#' serial number is the number that uniquely identifies the MFA device. For
#' physical MFA devices, this is the unique serial number that's provided
#' with the device. For virtual MFA devices, the serial number is the
#' device ARN. For more information, see [Enabling versioning on
#' buckets](https://docs.aws.amazon.com/AmazonS3/latest/userguide/manage-versioning-examples.html)
#' and [Configuring MFA
#' delete](https://docs.aws.amazon.com/AmazonS3/latest/userguide/MultiFactorAuthenticationDelete.html)
#' in the *Amazon Simple Storage Service User Guide*.
#' @param VersioningConfiguration &#91;required&#93; Container for setting the versioning state.
#' @param ExpectedBucketOwner The account ID of the expected bucket owner. If the account ID that you
#' provide does not match the actual owner of the bucket, the request fails
#' with the HTTP status code `403 Forbidden` (access denied).
#'
#' @return
#' An empty list.
#'
#' @section Request syntax:
#' ```
#' svc$put_bucket_versioning(
#'   Bucket = "string",
#'   ContentMD5 = "string",
#'   ChecksumAlgorithm = "CRC32"|"CRC32C"|"SHA1"|"SHA256"|"CRC64NVME",
#'   MFA = "string",
#'   VersioningConfiguration = list(
#'     MFADelete = "Enabled"|"Disabled",
#'     Status = "Enabled"|"Suspended"
#'   ),
#'   ExpectedBucketOwner = "string"
#' )
#' ```
#'
#' @examples
#' \dontrun{
#' # The following example sets versioning configuration on bucket. The
#' # configuration enables versioning on the bucket.
#' svc$put_bucket_versioning(
#'   Bucket = "examplebucket",
#'   VersioningConfiguration = list(
#'     MFADelete = "Disabled",
#'     Status = "Enabled"
#'   )
#' )
#' }
#'
#' @keywords internal
#'
#' @rdname s3_put_bucket_versioning
#'
#' @aliases s3_put_bucket_versioning
s3_put_bucket_versioning <- function(Bucket, ContentMD5 = NULL, ChecksumAlgorithm = NULL, MFA = NULL, VersioningConfiguration, ExpectedBucketOwner = NULL) {
  op <- new_operation(
    name = "PutBucketVersioning",
    http_method = "PUT",
    http_path = "/{Bucket}?versioning",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .s3$put_bucket_versioning_input(Bucket = Bucket, ContentMD5 = ContentMD5, ChecksumAlgorithm = ChecksumAlgorithm, MFA = MFA, VersioningConfiguration = VersioningConfiguration, ExpectedBucketOwner = ExpectedBucketOwner)
  output <- .s3$put_bucket_versioning_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$put_bucket_versioning <- s3_put_bucket_versioning

#' This operation is not supported for directory buckets
#'
#' @description
#' This operation is not supported for directory buckets.
#' 
#' Sets the configuration of the website that is specified in the `website`
#' subresource. To configure a bucket as a website, you can add this
#' subresource on the bucket with website configuration information such as
#' the file name of the index document and any redirect rules. For more
#' information, see [Hosting Websites on Amazon
#' S3](https://docs.aws.amazon.com/AmazonS3/latest/userguide/WebsiteHosting.html).
#' 
#' This PUT action requires the `S3:PutBucketWebsite` permission. By
#' default, only the bucket owner can configure the website attached to a
#' bucket; however, bucket owners can allow other users to set the website
#' configuration by writing a bucket policy that grants them the
#' `S3:PutBucketWebsite` permission.
#' 
#' To redirect all website requests sent to the bucket's website endpoint,
#' you add a website configuration with the following elements. Because all
#' requests are sent to another website, you don't need to provide index
#' document name for the bucket.
#' 
#' -   `WebsiteConfiguration`
#' 
#' -   `RedirectAllRequestsTo`
#' 
#' -   `HostName`
#' 
#' -   `Protocol`
#' 
#' If you want granular control over redirects, you can use the following
#' elements to add routing rules that describe conditions for redirecting
#' requests and information about the redirect destination. In this case,
#' the website configuration must provide an index document for the bucket,
#' because some requests might not be redirected.
#' 
#' -   `WebsiteConfiguration`
#' 
#' -   `IndexDocument`
#' 
#' -   `Suffix`
#' 
#' -   `ErrorDocument`
#' 
#' -   `Key`
#' 
#' -   `RoutingRules`
#' 
#' -   `RoutingRule`
#' 
#' -   `Condition`
#' 
#' -   `HttpErrorCodeReturnedEquals`
#' 
#' -   `KeyPrefixEquals`
#' 
#' -   `Redirect`
#' 
#' -   `Protocol`
#' 
#' -   `HostName`
#' 
#' -   `ReplaceKeyPrefixWith`
#' 
#' -   `ReplaceKeyWith`
#' 
#' -   `HttpRedirectCode`
#' 
#' Amazon S3 has a limitation of 50 routing rules per website
#' configuration. If you require more than 50 routing rules, you can use
#' object redirect. For more information, see [Configuring an Object
#' Redirect](https://docs.aws.amazon.com/AmazonS3/latest/userguide/how-to-page-redirect.html)
#' in the *Amazon S3 User Guide*.
#' 
#' The maximum request length is limited to 128 KB.
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#'
#' @usage
#' s3_put_bucket_website(Bucket, ContentMD5, ChecksumAlgorithm,
#'   WebsiteConfiguration, ExpectedBucketOwner)
#'
#' @param Bucket &#91;required&#93; The bucket name.
#' @param ContentMD5 The Base64 encoded 128-bit `MD5` digest of the data. You must use this
#' header as a message integrity check to verify that the request body was
#' not corrupted in transit. For more information, see [RFC
#' 1864](https://www.ietf.org/rfc/rfc1864.txt).
#' 
#' For requests made using the Amazon Web Services Command Line Interface
#' (CLI) or Amazon Web Services SDKs, this field is calculated
#' automatically.
#' @param ChecksumAlgorithm Indicates the algorithm used to create the checksum for the request when
#' you use the SDK. This header will not provide any additional
#' functionality if you don't use the SDK. When you send this header, there
#' must be a corresponding `x-amz-checksum` or `x-amz-trailer` header sent.
#' Otherwise, Amazon S3 fails the request with the HTTP status code
#' `400 Bad Request`. For more information, see [Checking object
#' integrity](https://docs.aws.amazon.com/AmazonS3/latest/userguide/checking-object-integrity.html)
#' in the *Amazon S3 User Guide*.
#' 
#' If you provide an individual checksum, Amazon S3 ignores any provided
#' `ChecksumAlgorithm` parameter.
#' @param WebsiteConfiguration &#91;required&#93; Container for the request.
#' @param ExpectedBucketOwner The account ID of the expected bucket owner. If the account ID that you
#' provide does not match the actual owner of the bucket, the request fails
#' with the HTTP status code `403 Forbidden` (access denied).
#'
#' @return
#' An empty list.
#'
#' @section Request syntax:
#' ```
#' svc$put_bucket_website(
#'   Bucket = "string",
#'   ContentMD5 = "string",
#'   ChecksumAlgorithm = "CRC32"|"CRC32C"|"SHA1"|"SHA256"|"CRC64NVME",
#'   WebsiteConfiguration = list(
#'     ErrorDocument = list(
#'       Key = "string"
#'     ),
#'     IndexDocument = list(
#'       Suffix = "string"
#'     ),
#'     RedirectAllRequestsTo = list(
#'       HostName = "string",
#'       Protocol = "http"|"https"
#'     ),
#'     RoutingRules = list(
#'       list(
#'         Condition = list(
#'           HttpErrorCodeReturnedEquals = "string",
#'           KeyPrefixEquals = "string"
#'         ),
#'         Redirect = list(
#'           HostName = "string",
#'           HttpRedirectCode = "string",
#'           Protocol = "http"|"https",
#'           ReplaceKeyPrefixWith = "string",
#'           ReplaceKeyWith = "string"
#'         )
#'       )
#'     )
#'   ),
#'   ExpectedBucketOwner = "string"
#' )
#' ```
#'
#' @examples
#' \dontrun{
#' # The following example adds website configuration to a bucket.
#' svc$put_bucket_website(
#'   Bucket = "examplebucket",
#'   ContentMD5 = "",
#'   WebsiteConfiguration = list(
#'     ErrorDocument = list(
#'       Key = "error.html"
#'     ),
#'     IndexDocument = list(
#'       Suffix = "index.html"
#'     )
#'   )
#' )
#' }
#'
#' @keywords internal
#'
#' @rdname s3_put_bucket_website
#'
#' @aliases s3_put_bucket_website
s3_put_bucket_website <- function(Bucket, ContentMD5 = NULL, ChecksumAlgorithm = NULL, WebsiteConfiguration, ExpectedBucketOwner = NULL) {
  op <- new_operation(
    name = "PutBucketWebsite",
    http_method = "PUT",
    http_path = "/{Bucket}?website",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .s3$put_bucket_website_input(Bucket = Bucket, ContentMD5 = ContentMD5, ChecksumAlgorithm = ChecksumAlgorithm, WebsiteConfiguration = WebsiteConfiguration, ExpectedBucketOwner = ExpectedBucketOwner)
  output <- .s3$put_bucket_website_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$put_bucket_website <- s3_put_bucket_website

#' End of support notice: As of October 1, 2025, Amazon S3 has discontinued
#' support for Email Grantee Access Control Lists (ACLs)
#'
#' @description
#' End of support notice: As of October 1, 2025, Amazon S3 has discontinued
#' support for Email Grantee Access Control Lists (ACLs). If you attempt to
#' use an Email Grantee ACL in a request after October 1, 2025, the request
#' will receive an `HTTP 405` (Method Not Allowed) error.
#' 
#' This change affects the following Amazon Web Services Regions: US East
#' (N. Virginia), US West (N. California), US West (Oregon), Asia Pacific
#' (Singapore), Asia Pacific (Sydney), Asia Pacific (Tokyo), Europe
#' (Ireland), and South America (São Paulo).
#' 
#' Adds an object to a bucket.
#' 
#' -   Amazon S3 never adds partial objects; if you receive a success
#'     response, Amazon S3 added the entire object to the bucket. You
#'     cannot use [`put_object`][s3_put_object] to only update a single
#'     piece of metadata for an existing object. You must put the entire
#'     object with updated metadata if you want to update some values.
#' 
#' -   If your bucket uses the bucket owner enforced setting for Object
#'     Ownership, ACLs are disabled and no longer affect permissions. All
#'     objects written to the bucket by any account will be owned by the
#'     bucket owner.
#' 
#' -   **Directory buckets** - For directory buckets, you must make
#'     requests for this API operation to the Zonal endpoint. These
#'     endpoints support virtual-hosted-style requests in the format
#'     `https://amzn-s3-demo-bucket.s3express-zone-id.region-code.amazonaws.com/key-name `.
#'     Path-style requests are not supported. For more information about
#'     endpoints in Availability Zones, see [Regional and Zonal endpoints
#'     for directory buckets in Availability
#'     Zones](https://docs.aws.amazon.com/AmazonS3/latest/userguide/endpoint-directory-buckets-AZ.html)
#'     in the *Amazon S3 User Guide*. For more information about endpoints
#'     in Local Zones, see [Concepts for directory buckets in Local
#'     Zones](https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-lzs-for-directory-buckets.html)
#'     in the *Amazon S3 User Guide*.
#' 
#' Amazon S3 is a distributed system. If it receives multiple write
#' requests for the same object simultaneously, it overwrites all but the
#' last object written. However, Amazon S3 provides features that can
#' modify this behavior:
#' 
#' -   **S3 Object Lock** - To prevent objects from being deleted or
#'     overwritten, you can use [Amazon S3 Object
#'     Lock](https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock.html)
#'     in the *Amazon S3 User Guide*.
#' 
#'     This functionality is not supported for directory buckets.
#' 
#' -   **If-None-Match** - Uploads the object only if the object key name
#'     does not already exist in the specified bucket. Otherwise, Amazon S3
#'     returns a `412 Precondition Failed` error. If a conflicting
#'     operation occurs during the upload, S3 returns a
#'     `409 ConditionalRequestConflict` response. On a 409 failure, retry
#'     the upload.
#' 
#'     Expects the * character (asterisk).
#' 
#'     For more information, see [Add preconditions to S3 operations with
#'     conditional
#'     requests](https://docs.aws.amazon.com/AmazonS3/latest/userguide/conditional-requests.html)
#'     in the *Amazon S3 User Guide* or [RFC
#'     7232](https://datatracker.ietf.org/doc/rfc7232/).
#' 
#'     This functionality is not supported for S3 on Outposts.
#' 
#' -   **S3 Versioning** - When you enable versioning for a bucket, if
#'     Amazon S3 receives multiple write requests for the same object
#'     simultaneously, it stores all versions of the objects. For each
#'     write request that is made to the same object, Amazon S3
#'     automatically generates a unique version ID of that object being
#'     stored in Amazon S3. You can retrieve, replace, or delete any
#'     version of the object. For more information about versioning, see
#'     [Adding Objects to Versioning-Enabled
#'     Buckets](https://docs.aws.amazon.com/AmazonS3/latest/userguide/AddingObjectstoVersioningEnabledBuckets.html)
#'     in the *Amazon S3 User Guide*. For information about returning the
#'     versioning state of a bucket, see
#'     [`get_bucket_versioning`][s3_get_bucket_versioning].
#' 
#'     This functionality is not supported for directory buckets.
#' 
#' ### Permissions
#' 
#' -   **General purpose bucket permissions** - The following permissions
#'     are required in your policies when your
#'     [`put_object`][s3_put_object] request includes specific headers.
#' 
#'     -   **`s3:PutObject`** - To successfully complete the
#'         [`put_object`][s3_put_object] request, you must always have the
#'         `s3:PutObject` permission on a bucket to add an object to it.
#' 
#'     -   **`s3:PutObjectAcl`** - To successfully change the objects ACL
#'         of your [`put_object`][s3_put_object] request, you must have the
#'         `s3:PutObjectAcl`.
#' 
#'     -   **`s3:PutObjectTagging`** - To successfully set the tag-set with
#'         your [`put_object`][s3_put_object] request, you must have the
#'         `s3:PutObjectTagging`.
#' 
#' -   **Directory bucket permissions** - To grant access to this API
#'     operation on a directory bucket, we recommend that you use the
#'     [`create_session`](https://docs.aws.amazon.com/AmazonS3/latest/API/API_CreateSession.html)
#'     API operation for session-based authorization. Specifically, you
#'     grant the `s3express:CreateSession` permission to the directory
#'     bucket in a bucket policy or an IAM identity-based policy. Then, you
#'     make the [`create_session`][s3_create_session] API call on the
#'     bucket to obtain a session token. With the session token in your
#'     request header, you can make API requests to this operation. After
#'     the session token expires, you make another
#'     [`create_session`][s3_create_session] API call to generate a new
#'     session token for use. Amazon Web Services CLI or SDKs create
#'     session and refresh the session token automatically to avoid service
#'     interruptions when a session expires. For more information about
#'     authorization, see
#'     [`create_session`](https://docs.aws.amazon.com/AmazonS3/latest/API/API_CreateSession.html)
#'     .
#' 
#'     If the object is encrypted with SSE-KMS, you must also have the
#'     `kms:GenerateDataKey` and `kms:Decrypt` permissions in IAM
#'     identity-based policies and KMS key policies for the KMS key.
#' 
#' ### Data integrity with Content-MD5
#' 
#' -   **General purpose bucket** - To ensure that data is not corrupted
#'     traversing the network, use the `Content-MD5` header. When you use
#'     this header, Amazon S3 checks the object against the provided MD5
#'     value and, if they do not match, Amazon S3 returns an error.
#'     Alternatively, when the object's ETag is its MD5 digest, you can
#'     calculate the MD5 while putting the object to Amazon S3 and compare
#'     the returned ETag to the calculated MD5 value.
#' 
#' -   **Directory bucket** - This functionality is not supported for
#'     directory buckets.
#' 
#' ### HTTP Host header syntax
#' 
#' **Directory buckets** - The HTTP Host header syntax is
#' ` Bucket-name.s3express-zone-id.region-code.amazonaws.com`.
#' 
#' For more information about related Amazon S3 APIs, see the following:
#' 
#' -   [`copy_object`][s3_copy_object]
#' 
#' -   [`delete_object`][s3_delete_object]
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#'
#' @usage
#' s3_put_object(ACL, Body, Bucket, CacheControl, ContentDisposition,
#'   ContentEncoding, ContentLanguage, ContentLength, ContentMD5,
#'   ContentType, ChecksumAlgorithm, ChecksumCRC32, ChecksumCRC32C,
#'   ChecksumCRC64NVME, ChecksumSHA1, ChecksumSHA256, Expires, IfMatch,
#'   IfNoneMatch, GrantFullControl, GrantRead, GrantReadACP, GrantWriteACP,
#'   Key, WriteOffsetBytes, Metadata, ServerSideEncryption, StorageClass,
#'   WebsiteRedirectLocation, SSECustomerAlgorithm, SSECustomerKey,
#'   SSECustomerKeyMD5, SSEKMSKeyId, SSEKMSEncryptionContext,
#'   BucketKeyEnabled, RequestPayer, Tagging, ObjectLockMode,
#'   ObjectLockRetainUntilDate, ObjectLockLegalHoldStatus,
#'   ExpectedBucketOwner)
#'
#' @param ACL The canned ACL to apply to the object. For more information, see [Canned
#' ACL](https://docs.aws.amazon.com/AmazonS3/latest/userguide/acl-overview.html#CannedACL)
#' in the *Amazon S3 User Guide*.
#' 
#' When adding a new object, you can use headers to grant ACL-based
#' permissions to individual Amazon Web Services accounts or to predefined
#' groups defined by Amazon S3. These permissions are then added to the ACL
#' on the object. By default, all objects are private. Only the owner has
#' full access control. For more information, see [Access Control List
#' (ACL)
#' Overview](https://docs.aws.amazon.com/AmazonS3/latest/userguide/acl-overview.html)
#' and [Managing ACLs Using the REST
#' API](https://docs.aws.amazon.com/AmazonS3/latest/userguide/managing-acls.html)
#' in the *Amazon S3 User Guide*.
#' 
#' If the bucket that you're uploading objects to uses the bucket owner
#' enforced setting for S3 Object Ownership, ACLs are disabled and no
#' longer affect permissions. Buckets that use this setting only accept PUT
#' requests that don't specify an ACL or PUT requests that specify bucket
#' owner full control ACLs, such as the `bucket-owner-full-control` canned
#' ACL or an equivalent form of this ACL expressed in the XML format. PUT
#' requests that contain other ACLs (for example, custom grants to certain
#' Amazon Web Services accounts) fail and return a `400` error with the
#' error code `AccessControlListNotSupported`. For more information, see
#' [Controlling ownership of objects and disabling
#' ACLs](https://docs.aws.amazon.com/AmazonS3/latest/userguide/about-object-ownership.html)
#' in the *Amazon S3 User Guide*.
#' 
#' -   This functionality is not supported for directory buckets.
#' 
#' -   This functionality is not supported for Amazon S3 on Outposts.
#' @param Body Object data.
#' @param Bucket &#91;required&#93; The bucket name to which the PUT action was initiated.
#' 
#' **Directory buckets** - When you use this operation with a directory
#' bucket, you must use virtual-hosted-style requests in the format
#' ` Bucket-name.s3express-zone-id.region-code.amazonaws.com`. Path-style
#' requests are not supported. Directory bucket names must be unique in the
#' chosen Zone (Availability Zone or Local Zone). Bucket names must follow
#' the format ` bucket-base-name--zone-id--x-s3` (for example,
#' ` amzn-s3-demo-bucket--usw2-az1--x-s3`). For information about bucket
#' naming restrictions, see [Directory bucket naming
#' rules](https://docs.aws.amazon.com/AmazonS3/latest/userguide/directory-bucket-naming-rules.html)
#' in the *Amazon S3 User Guide*.
#' 
#' **Access points** - When you use this action with an access point for
#' general purpose buckets, you must provide the alias of the access point
#' in place of the bucket name or specify the access point ARN. When you
#' use this action with an access point for directory buckets, you must
#' provide the access point name in place of the bucket name. When using
#' the access point ARN, you must direct requests to the access point
#' hostname. The access point hostname takes the form
#' *AccessPointName*-*AccountId*.s3-accesspoint.*Region*.amazonaws.com.
#' When using this action with an access point through the Amazon Web
#' Services SDKs, you provide the access point ARN in place of the bucket
#' name. For more information about access point ARNs, see [Using access
#' points](https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-access-points.html)
#' in the *Amazon S3 User Guide*.
#' 
#' Object Lambda access points are not supported by directory buckets.
#' 
#' **S3 on Outposts** - When you use this action with S3 on Outposts, you
#' must direct requests to the S3 on Outposts hostname. The S3 on Outposts
#' hostname takes the form
#' ` AccessPointName-AccountId.outpostID.s3-outposts.Region.amazonaws.com`.
#' When you use this action with S3 on Outposts, the destination bucket
#' must be the Outposts access point ARN or the access point alias. For
#' more information about S3 on Outposts, see [What is S3 on
#' Outposts?](https://docs.aws.amazon.com/AmazonS3/latest/s3-outposts/S3onOutposts.html)
#' in the *Amazon S3 User Guide*.
#' @param CacheControl Can be used to specify caching behavior along the request/reply chain.
#' For more information, see
#' [http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.9](https://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.9).
#' @param ContentDisposition Specifies presentational information for the object. For more
#' information, see <https://www.rfc-editor.org/rfc/rfc6266#section-4>.
#' @param ContentEncoding Specifies what content encodings have been applied to the object and
#' thus what decoding mechanisms must be applied to obtain the media-type
#' referenced by the Content-Type header field. For more information, see
#' <https://www.rfc-editor.org/rfc/rfc9110.html#field.content-encoding>.
#' @param ContentLanguage The language the content is in.
#' @param ContentLength Size of the body in bytes. This parameter is useful when the size of the
#' body cannot be determined automatically. For more information, see
#' <https://www.rfc-editor.org/rfc/rfc9110.html#name-content-length>.
#' @param ContentMD5 The Base64 encoded 128-bit `MD5` digest of the message (without the
#' headers) according to RFC 1864. This header can be used as a message
#' integrity check to verify that the data is the same data that was
#' originally sent. Although it is optional, we recommend using the
#' Content-MD5 mechanism as an end-to-end integrity check. For more
#' information about REST request authentication, see [REST
#' Authentication](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTAuthentication.html).
#' 
#' The `Content-MD5` or `x-amz-sdk-checksum-algorithm` header is required
#' for any request to upload an object with a retention period configured
#' using Amazon S3 Object Lock. For more information, see [Uploading
#' objects to an Object Lock enabled
#' bucket](https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock-managing.html#object-lock-put-object)
#' in the *Amazon S3 User Guide*.
#' 
#' This functionality is not supported for directory buckets.
#' @param ContentType A standard MIME type describing the format of the contents. For more
#' information, see
#' <https://www.rfc-editor.org/rfc/rfc9110.html#name-content-type>.
#' @param ChecksumAlgorithm Indicates the algorithm used to create the checksum for the object when
#' you use the SDK. This header will not provide any additional
#' functionality if you don't use the SDK. When you send this header, there
#' must be a corresponding `x-amz-checksum-algorithm ` or `x-amz-trailer`
#' header sent. Otherwise, Amazon S3 fails the request with the HTTP status
#' code `400 Bad Request`.
#' 
#' For the `x-amz-checksum-algorithm ` header, replace ` algorithm ` with
#' the supported algorithm from the following list:
#' 
#' -   `CRC32`
#' 
#' -   `CRC32C`
#' 
#' -   `CRC64NVME`
#' 
#' -   `SHA1`
#' 
#' -   `SHA256`
#' 
#' For more information, see [Checking object
#' integrity](https://docs.aws.amazon.com/AmazonS3/latest/userguide/checking-object-integrity.html)
#' in the *Amazon S3 User Guide*.
#' 
#' If the individual checksum value you provide through
#' `x-amz-checksum-algorithm ` doesn't match the checksum algorithm you set
#' through `x-amz-sdk-checksum-algorithm`, Amazon S3 fails the request with
#' a `BadDigest` error.
#' 
#' The `Content-MD5` or `x-amz-sdk-checksum-algorithm` header is required
#' for any request to upload an object with a retention period configured
#' using Amazon S3 Object Lock. For more information, see [Uploading
#' objects to an Object Lock enabled
#' bucket](https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock-managing.html#object-lock-put-object)
#' in the *Amazon S3 User Guide*.
#' 
#' For directory buckets, when you use Amazon Web Services SDKs, `CRC32` is
#' the default checksum algorithm that's used for performance.
#' @param ChecksumCRC32 This header can be used as a data integrity check to verify that the
#' data received is the same data that was originally sent. This header
#' specifies the Base64 encoded, 32-bit `CRC32` checksum of the object. For
#' more information, see [Checking object
#' integrity](https://docs.aws.amazon.com/AmazonS3/latest/userguide/checking-object-integrity.html)
#' in the *Amazon S3 User Guide*.
#' @param ChecksumCRC32C This header can be used as a data integrity check to verify that the
#' data received is the same data that was originally sent. This header
#' specifies the Base64 encoded, 32-bit `CRC32C` checksum of the object.
#' For more information, see [Checking object
#' integrity](https://docs.aws.amazon.com/AmazonS3/latest/userguide/checking-object-integrity.html)
#' in the *Amazon S3 User Guide*.
#' @param ChecksumCRC64NVME This header can be used as a data integrity check to verify that the
#' data received is the same data that was originally sent. This header
#' specifies the Base64 encoded, 64-bit `CRC64NVME` checksum of the object.
#' The `CRC64NVME` checksum is always a full object checksum. For more
#' information, see [Checking object integrity in the Amazon S3 User
#' Guide](https://docs.aws.amazon.com/AmazonS3/latest/userguide/checking-object-integrity.html).
#' @param ChecksumSHA1 This header can be used as a data integrity check to verify that the
#' data received is the same data that was originally sent. This header
#' specifies the Base64 encoded, 160-bit `SHA1` digest of the object. For
#' more information, see [Checking object
#' integrity](https://docs.aws.amazon.com/AmazonS3/latest/userguide/checking-object-integrity.html)
#' in the *Amazon S3 User Guide*.
#' @param ChecksumSHA256 This header can be used as a data integrity check to verify that the
#' data received is the same data that was originally sent. This header
#' specifies the Base64 encoded, 256-bit `SHA256` digest of the object. For
#' more information, see [Checking object
#' integrity](https://docs.aws.amazon.com/AmazonS3/latest/userguide/checking-object-integrity.html)
#' in the *Amazon S3 User Guide*.
#' @param Expires The date and time at which the object is no longer cacheable. For more
#' information, see <https://www.rfc-editor.org/rfc/rfc7234#section-5.3>.
#' @param IfMatch Uploads the object only if the ETag (entity tag) value provided during
#' the WRITE operation matches the ETag of the object in S3. If the ETag
#' values do not match, the operation returns a `412 Precondition Failed`
#' error.
#' 
#' If a conflicting operation occurs during the upload S3 returns a
#' `409 ConditionalRequestConflict` response. On a 409 failure you should
#' fetch the object's ETag and retry the upload.
#' 
#' Expects the ETag value as a string.
#' 
#' For more information about conditional requests, see [RFC
#' 7232](https://datatracker.ietf.org/doc/html/rfc7232), or [Conditional
#' requests](https://docs.aws.amazon.com/AmazonS3/latest/userguide/conditional-requests.html)
#' in the *Amazon S3 User Guide*.
#' @param IfNoneMatch Uploads the object only if the object key name does not already exist in
#' the bucket specified. Otherwise, Amazon S3 returns a
#' `412 Precondition Failed` error.
#' 
#' If a conflicting operation occurs during the upload S3 returns a
#' `409 ConditionalRequestConflict` response. On a 409 failure you should
#' retry the upload.
#' 
#' Expects the '*' (asterisk) character.
#' 
#' For more information about conditional requests, see [RFC
#' 7232](https://datatracker.ietf.org/doc/html/rfc7232), or [Conditional
#' requests](https://docs.aws.amazon.com/AmazonS3/latest/userguide/conditional-requests.html)
#' in the *Amazon S3 User Guide*.
#' @param GrantFullControl Gives the grantee READ, READ_ACP, and WRITE_ACP permissions on the
#' object.
#' 
#' -   This functionality is not supported for directory buckets.
#' 
#' -   This functionality is not supported for Amazon S3 on Outposts.
#' @param GrantRead Allows grantee to read the object data and its metadata.
#' 
#' -   This functionality is not supported for directory buckets.
#' 
#' -   This functionality is not supported for Amazon S3 on Outposts.
#' @param GrantReadACP Allows grantee to read the object ACL.
#' 
#' -   This functionality is not supported for directory buckets.
#' 
#' -   This functionality is not supported for Amazon S3 on Outposts.
#' @param GrantWriteACP Allows grantee to write the ACL for the applicable object.
#' 
#' -   This functionality is not supported for directory buckets.
#' 
#' -   This functionality is not supported for Amazon S3 on Outposts.
#' @param Key &#91;required&#93; Object key for which the PUT action was initiated.
#' @param WriteOffsetBytes Specifies the offset for appending data to existing objects in bytes.
#' The offset must be equal to the size of the existing object being
#' appended to. If no object exists, setting this header to 0 will create a
#' new object.
#' 
#' This functionality is only supported for objects in the Amazon S3
#' Express One Zone storage class in directory buckets.
#' @param Metadata A map of metadata to store with the object in S3.
#' @param ServerSideEncryption The server-side encryption algorithm that was used when you store this
#' object in Amazon S3 or Amazon FSx.
#' 
#' -   **General purpose buckets** - You have four mutually exclusive
#'     options to protect data using server-side encryption in Amazon S3,
#'     depending on how you choose to manage the encryption keys.
#'     Specifically, the encryption key options are Amazon S3 managed keys
#'     (SSE-S3), Amazon Web Services KMS keys (SSE-KMS or DSSE-KMS), and
#'     customer-provided keys (SSE-C). Amazon S3 encrypts data with
#'     server-side encryption by using Amazon S3 managed keys (SSE-S3) by
#'     default. You can optionally tell Amazon S3 to encrypt data at rest
#'     by using server-side encryption with other key options. For more
#'     information, see [Using Server-Side
#'     Encryption](https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingServerSideEncryption.html)
#'     in the *Amazon S3 User Guide*.
#' 
#' -   **Directory buckets** - For directory buckets, there are only two
#'     supported options for server-side encryption: server-side encryption
#'     with Amazon S3 managed keys (SSE-S3) (`AES256`) and server-side
#'     encryption with KMS keys (SSE-KMS) (`aws:kms`). We recommend that
#'     the bucket's default encryption uses the desired encryption
#'     configuration and you don't override the bucket default encryption
#'     in your [`create_session`][s3_create_session] requests or `PUT`
#'     object requests. Then, new objects are automatically encrypted with
#'     the desired encryption settings. For more information, see
#'     [Protecting data with server-side
#'     encryption](https://docs.aws.amazon.com/AmazonS3/latest/userguide/)
#'     in the *Amazon S3 User Guide*. For more information about the
#'     encryption overriding behaviors in directory buckets, see
#'     [Specifying server-side encryption with KMS for new object
#'     uploads](https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-express-specifying-kms-encryption.html).
#' 
#'     In the Zonal endpoint API calls (except
#'     [`copy_object`][s3_copy_object] and
#'     [`upload_part_copy`][s3_upload_part_copy]) using the REST API, the
#'     encryption request headers must match the encryption settings that
#'     are specified in the [`create_session`][s3_create_session] request.
#'     You can't override the values of the encryption settings
#'     (`x-amz-server-side-encryption`,
#'     `x-amz-server-side-encryption-aws-kms-key-id`,
#'     `x-amz-server-side-encryption-context`, and
#'     `x-amz-server-side-encryption-bucket-key-enabled`) that are
#'     specified in the [`create_session`][s3_create_session] request. You
#'     don't need to explicitly specify these encryption settings values in
#'     Zonal endpoint API calls, and Amazon S3 will use the encryption
#'     settings values from the [`create_session`][s3_create_session]
#'     request to protect new objects in the directory bucket.
#' 
#'     When you use the CLI or the Amazon Web Services SDKs, for
#'     [`create_session`][s3_create_session], the session token refreshes
#'     automatically to avoid service interruptions when a session expires.
#'     The CLI or the Amazon Web Services SDKs use the bucket's default
#'     encryption configuration for the
#'     [`create_session`][s3_create_session] request. It's not supported to
#'     override the encryption settings values in the
#'     [`create_session`][s3_create_session] request. So in the Zonal
#'     endpoint API calls (except [`copy_object`][s3_copy_object] and
#'     [`upload_part_copy`][s3_upload_part_copy]), the encryption request
#'     headers must match the default encryption configuration of the
#'     directory bucket.
#' 
#' -   **S3 access points for Amazon FSx** - When accessing data stored in
#'     Amazon FSx file systems using S3 access points, the only valid
#'     server side encryption option is `aws:fsx`. All Amazon FSx file
#'     systems have encryption configured by default and are encrypted at
#'     rest. Data is automatically encrypted before being written to the
#'     file system, and automatically decrypted as it is read. These
#'     processes are handled transparently by Amazon FSx.
#' @param StorageClass By default, Amazon S3 uses the STANDARD Storage Class to store newly
#' created objects. The STANDARD storage class provides high durability and
#' high availability. Depending on performance needs, you can specify a
#' different Storage Class. For more information, see [Storage
#' Classes](https://docs.aws.amazon.com/AmazonS3/latest/userguide/storage-class-intro.html)
#' in the *Amazon S3 User Guide*.
#' 
#' -   Directory buckets only support `EXPRESS_ONEZONE` (the S3 Express One
#'     Zone storage class) in Availability Zones and `ONEZONE_IA` (the S3
#'     One Zone-Infrequent Access storage class) in Dedicated Local Zones.
#' 
#' -   Amazon S3 on Outposts only uses the OUTPOSTS Storage Class.
#' @param WebsiteRedirectLocation If the bucket is configured as a website, redirects requests for this
#' object to another object in the same bucket or to an external URL.
#' Amazon S3 stores the value of this header in the object metadata. For
#' information about object metadata, see [Object Key and
#' Metadata](https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingMetadata.html)
#' in the *Amazon S3 User Guide*.
#' 
#' In the following example, the request header sets the redirect to an
#' object (anotherPage.html) in the same bucket:
#' 
#' `x-amz-website-redirect-location: /anotherPage.html`
#' 
#' In the following example, the request header sets the object redirect to
#' another website:
#' 
#' `x-amz-website-redirect-location: http://www.example.com/`
#' 
#' For more information about website hosting in Amazon S3, see [Hosting
#' Websites on Amazon
#' S3](https://docs.aws.amazon.com/AmazonS3/latest/userguide/WebsiteHosting.html)
#' and [How to Configure Website Page
#' Redirects](https://docs.aws.amazon.com/AmazonS3/latest/userguide/how-to-page-redirect.html)
#' in the *Amazon S3 User Guide*.
#' 
#' This functionality is not supported for directory buckets.
#' @param SSECustomerAlgorithm Specifies the algorithm to use when encrypting the object (for example,
#' `AES256`).
#' 
#' This functionality is not supported for directory buckets.
#' @param SSECustomerKey Specifies the customer-provided encryption key for Amazon S3 to use in
#' encrypting data. This value is used to store the object and then it is
#' discarded; Amazon S3 does not store the encryption key. The key must be
#' appropriate for use with the algorithm specified in the
#' `x-amz-server-side-encryption-customer-algorithm` header.
#' 
#' This functionality is not supported for directory buckets.
#' @param SSECustomerKeyMD5 Specifies the 128-bit MD5 digest of the encryption key according to RFC
#' 1321. Amazon S3 uses this header for a message integrity check to ensure
#' that the encryption key was transmitted without error.
#' 
#' This functionality is not supported for directory buckets.
#' @param SSEKMSKeyId Specifies the KMS key ID (Key ID, Key ARN, or Key Alias) to use for
#' object encryption. If the KMS key doesn't exist in the same account
#' that's issuing the command, you must use the full Key ARN not the Key
#' ID.
#' 
#' **General purpose buckets** - If you specify
#' `x-amz-server-side-encryption` with `aws:kms` or `aws:kms:dsse`, this
#' header specifies the ID (Key ID, Key ARN, or Key Alias) of the KMS key
#' to use. If you specify `x-amz-server-side-encryption:aws:kms` or
#' `x-amz-server-side-encryption:aws:kms:dsse`, but do not provide
#' `x-amz-server-side-encryption-aws-kms-key-id`, Amazon S3 uses the Amazon
#' Web Services managed key (`aws/s3`) to protect the data.
#' 
#' **Directory buckets** - To encrypt data using SSE-KMS, it's recommended
#' to specify the `x-amz-server-side-encryption` header to `aws:kms`. Then,
#' the `x-amz-server-side-encryption-aws-kms-key-id` header implicitly uses
#' the bucket's default KMS customer managed key ID. If you want to
#' explicitly set the ` x-amz-server-side-encryption-aws-kms-key-id`
#' header, it must match the bucket's default customer managed key (using
#' key ID or ARN, not alias). Your SSE-KMS configuration can only support 1
#' [customer managed
#' key](https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#customer-cmk)
#' per directory bucket's lifetime. The [Amazon Web Services managed
#' key](https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html#aws-managed-cmk)
#' (`aws/s3`) isn't supported. Incorrect key specification results in an
#' HTTP `400 Bad Request` error.
#' @param SSEKMSEncryptionContext Specifies the Amazon Web Services KMS Encryption Context as an
#' additional encryption context to use for object encryption. The value of
#' this header is a Base64 encoded string of a UTF-8 encoded JSON, which
#' contains the encryption context as key-value pairs. This value is stored
#' as object metadata and automatically gets passed on to Amazon Web
#' Services KMS for future [`get_object`][s3_get_object] operations on this
#' object.
#' 
#' **General purpose buckets** - This value must be explicitly added during
#' [`copy_object`][s3_copy_object] operations if you want an additional
#' encryption context for your object. For more information, see
#' [Encryption
#' context](https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingKMSEncryption.html#encryption-context)
#' in the *Amazon S3 User Guide*.
#' 
#' **Directory buckets** - You can optionally provide an explicit
#' encryption context value. The value must match the default encryption
#' context - the bucket Amazon Resource Name (ARN). An additional
#' encryption context value is not supported.
#' @param BucketKeyEnabled Specifies whether Amazon S3 should use an S3 Bucket Key for object
#' encryption with server-side encryption using Key Management Service
#' (KMS) keys (SSE-KMS).
#' 
#' **General purpose buckets** - Setting this header to `true` causes
#' Amazon S3 to use an S3 Bucket Key for object encryption with SSE-KMS.
#' Also, specifying this header with a PUT action doesn't affect
#' bucket-level settings for S3 Bucket Key.
#' 
#' **Directory buckets** - S3 Bucket Keys are always enabled for `GET` and
#' `PUT` operations in a directory bucket and can’t be disabled. S3 Bucket
#' Keys aren't supported, when you copy SSE-KMS encrypted objects from
#' general purpose buckets to directory buckets, from directory buckets to
#' general purpose buckets, or between directory buckets, through
#' [`copy_object`][s3_copy_object],
#' [`upload_part_copy`][s3_upload_part_copy], [the Copy operation in Batch
#' Operations](https://docs.aws.amazon.com/AmazonS3/latest/userguide/directory-buckets-objects-Batch-Ops.html),
#' or [the import
#' jobs](https://docs.aws.amazon.com/AmazonS3/latest/userguide/create-import-job.html).
#' In this case, Amazon S3 makes a call to KMS every time a copy request is
#' made for a KMS-encrypted object.
#' @param RequestPayer 
#' @param Tagging The tag-set for the object. The tag-set must be encoded as URL Query
#' parameters. (For example, "Key1=Value1")
#' 
#' This functionality is not supported for directory buckets.
#' @param ObjectLockMode The Object Lock mode that you want to apply to this object.
#' 
#' This functionality is not supported for directory buckets.
#' @param ObjectLockRetainUntilDate The date and time when you want this object's Object Lock to expire.
#' Must be formatted as a timestamp parameter.
#' 
#' This functionality is not supported for directory buckets.
#' @param ObjectLockLegalHoldStatus Specifies whether a legal hold will be applied to this object. For more
#' information about S3 Object Lock, see [Object
#' Lock](https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock.html)
#' in the *Amazon S3 User Guide*.
#' 
#' This functionality is not supported for directory buckets.
#' @param ExpectedBucketOwner The account ID of the expected bucket owner. If the account ID that you
#' provide does not match the actual owner of the bucket, the request fails
#' with the HTTP status code `403 Forbidden` (access denied).
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   Expiration = "string",
#'   ETag = "string",
#'   ChecksumCRC32 = "string",
#'   ChecksumCRC32C = "string",
#'   ChecksumCRC64NVME = "string",
#'   ChecksumSHA1 = "string",
#'   ChecksumSHA256 = "string",
#'   ChecksumType = "COMPOSITE"|"FULL_OBJECT",
#'   ServerSideEncryption = "AES256"|"aws:fsx"|"aws:kms"|"aws:kms:dsse",
#'   VersionId = "string",
#'   SSECustomerAlgorithm = "string",
#'   SSECustomerKeyMD5 = "string",
#'   SSEKMSKeyId = "string",
#'   SSEKMSEncryptionContext = "string",
#'   BucketKeyEnabled = TRUE|FALSE,
#'   Size = 123,
#'   RequestCharged = "requester"
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$put_object(
#'   ACL = "private"|"public-read"|"public-read-write"|"authenticated-read"|"aws-exec-read"|"bucket-owner-read"|"bucket-owner-full-control",
#'   Body = raw,
#'   Bucket = "string",
#'   CacheControl = "string",
#'   ContentDisposition = "string",
#'   ContentEncoding = "string",
#'   ContentLanguage = "string",
#'   ContentLength = 123,
#'   ContentMD5 = "string",
#'   ContentType = "string",
#'   ChecksumAlgorithm = "CRC32"|"CRC32C"|"SHA1"|"SHA256"|"CRC64NVME",
#'   ChecksumCRC32 = "string",
#'   ChecksumCRC32C = "string",
#'   ChecksumCRC64NVME = "string",
#'   ChecksumSHA1 = "string",
#'   ChecksumSHA256 = "string",
#'   Expires = "string",
#'   IfMatch = "string",
#'   IfNoneMatch = "string",
#'   GrantFullControl = "string",
#'   GrantRead = "string",
#'   GrantReadACP = "string",
#'   GrantWriteACP = "string",
#'   Key = "string",
#'   WriteOffsetBytes = 123,
#'   Metadata = list(
#'     "string"
#'   ),
#'   ServerSideEncryption = "AES256"|"aws:fsx"|"aws:kms"|"aws:kms:dsse",
#'   StorageClass = "STANDARD"|"REDUCED_REDUNDANCY"|"STANDARD_IA"|"ONEZONE_IA"|"INTELLIGENT_TIERING"|"GLACIER"|"DEEP_ARCHIVE"|"OUTPOSTS"|"GLACIER_IR"|"SNOW"|"EXPRESS_ONEZONE"|"FSX_OPENZFS"|"FSX_ONTAP",
#'   WebsiteRedirectLocation = "string",
#'   SSECustomerAlgorithm = "string",
#'   SSECustomerKey = "string",
#'   SSECustomerKeyMD5 = "string",
#'   SSEKMSKeyId = "string",
#'   SSEKMSEncryptionContext = "string",
#'   BucketKeyEnabled = TRUE|FALSE,
#'   RequestPayer = "requester",
#'   Tagging = "string",
#'   ObjectLockMode = "GOVERNANCE"|"COMPLIANCE",
#'   ObjectLockRetainUntilDate = as.POSIXct(
#'     "2015-01-01"
#'   ),
#'   ObjectLockLegalHoldStatus = "ON"|"OFF",
#'   ExpectedBucketOwner = "string"
#' )
#' ```
#'
#' @examples
#' \dontrun{
#' # The following example creates an object. If the bucket is versioning
#' # enabled, S3 returns version ID in response.
#' svc$put_object(
#'   Body = "filetoupload",
#'   Bucket = "examplebucket",
#'   Key = "objectkey"
#' )
#' 
#' # The following example uploads an object. The request specifies optional
#' # request headers to directs S3 to use specific storage class and use
#' # server-side encryption.
#' svc$put_object(
#'   Body = "HappyFace.jpg",
#'   Bucket = "examplebucket",
#'   Key = "HappyFace.jpg",
#'   ServerSideEncryption = "AES256",
#'   StorageClass = "STANDARD_IA"
#' )
#' 
#' # The following example uploads and object. The request specifies optional
#' # canned ACL (access control list) to all READ access to authenticated
#' # users. If the bucket is versioning enabled, S3 returns version ID in
#' # response.
#' svc$put_object(
#'   ACL = "authenticated-read",
#'   Body = "filetoupload",
#'   Bucket = "examplebucket",
#'   Key = "exampleobject"
#' )
#' 
#' # The following example uploads an object to a versioning-enabled bucket.
#' # The source file is specified using Windows file syntax. S3 returns
#' # VersionId of the newly created object.
#' svc$put_object(
#'   Body = "HappyFace.jpg",
#'   Bucket = "examplebucket",
#'   Key = "HappyFace.jpg"
#' )
#' 
#' # The following example creates an object. The request also specifies
#' # optional metadata. If the bucket is versioning enabled, S3 returns
#' # version ID in response.
#' svc$put_object(
#'   Body = "filetoupload",
#'   Bucket = "examplebucket",
#'   Key = "exampleobject",
#'   Metadata = list(
#'     metadata1 = "value1",
#'     metadata2 = "value2"
#'   )
#' )
#' 
#' # The following example uploads an object. The request specifies optional
#' # object tags. The bucket is versioned, therefore S3 returns version ID of
#' # the newly created object.
#' svc$put_object(
#'   Body = "c:\\HappyFace.jpg",
#'   Bucket = "examplebucket",
#'   Key = "HappyFace.jpg",
#'   Tagging = "key1=value1&key2=value2"
#' )
#' 
#' # The following example uploads and object. The request specifies the
#' # optional server-side encryption option. The request also specifies
#' # optional object tags. If the bucket is versioning enabled, S3 returns
#' # version ID in response.
#' svc$put_object(
#'   Body = "filetoupload",
#'   Bucket = "examplebucket",
#'   Key = "exampleobject",
#'   ServerSideEncryption = "AES256",
#'   Tagging = "key1=value1&key2=value2"
#' )
#' }
#'
#' @keywords internal
#'
#' @rdname s3_put_object
#'
#' @aliases s3_put_object
s3_put_object <- function(ACL = NULL, Body = NULL, Bucket, CacheControl = NULL, ContentDisposition = NULL, ContentEncoding = NULL, ContentLanguage = NULL, ContentLength = NULL, ContentMD5 = NULL, ContentType = NULL, ChecksumAlgorithm = NULL, ChecksumCRC32 = NULL, ChecksumCRC32C = NULL, ChecksumCRC64NVME = NULL, ChecksumSHA1 = NULL, ChecksumSHA256 = NULL, Expires = NULL, IfMatch = NULL, IfNoneMatch = NULL, GrantFullControl = NULL, GrantRead = NULL, GrantReadACP = NULL, GrantWriteACP = NULL, Key, WriteOffsetBytes = NULL, Metadata = NULL, ServerSideEncryption = NULL, StorageClass = NULL, WebsiteRedirectLocation = NULL, SSECustomerAlgorithm = NULL, SSECustomerKey = NULL, SSECustomerKeyMD5 = NULL, SSEKMSKeyId = NULL, SSEKMSEncryptionContext = NULL, BucketKeyEnabled = NULL, RequestPayer = NULL, Tagging = NULL, ObjectLockMode = NULL, ObjectLockRetainUntilDate = NULL, ObjectLockLegalHoldStatus = NULL, ExpectedBucketOwner = NULL) {
  op <- new_operation(
    name = "PutObject",
    http_method = "PUT",
    http_path = "/{Bucket}/{Key+}",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .s3$put_object_input(ACL = ACL, Body = Body, Bucket = Bucket, CacheControl = CacheControl, ContentDisposition = ContentDisposition, ContentEncoding = ContentEncoding, ContentLanguage = ContentLanguage, ContentLength = ContentLength, ContentMD5 = ContentMD5, ContentType = ContentType, ChecksumAlgorithm = ChecksumAlgorithm, ChecksumCRC32 = ChecksumCRC32, ChecksumCRC32C = ChecksumCRC32C, ChecksumCRC64NVME = ChecksumCRC64NVME, ChecksumSHA1 = ChecksumSHA1, ChecksumSHA256 = ChecksumSHA256, Expires = Expires, IfMatch = IfMatch, IfNoneMatch = IfNoneMatch, GrantFullControl = GrantFullControl, GrantRead = GrantRead, GrantReadACP = GrantReadACP, GrantWriteACP = GrantWriteACP, Key = Key, WriteOffsetBytes = WriteOffsetBytes, Metadata = Metadata, ServerSideEncryption = ServerSideEncryption, StorageClass = StorageClass, WebsiteRedirectLocation = WebsiteRedirectLocation, SSECustomerAlgorithm = SSECustomerAlgorithm, SSECustomerKey = SSECustomerKey, SSECustomerKeyMD5 = SSECustomerKeyMD5, SSEKMSKeyId = SSEKMSKeyId, SSEKMSEncryptionContext = SSEKMSEncryptionContext, BucketKeyEnabled = BucketKeyEnabled, RequestPayer = RequestPayer, Tagging = Tagging, ObjectLockMode = ObjectLockMode, ObjectLockRetainUntilDate = ObjectLockRetainUntilDate, ObjectLockLegalHoldStatus = ObjectLockLegalHoldStatus, ExpectedBucketOwner = ExpectedBucketOwner)
  output <- .s3$put_object_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$put_object <- s3_put_object

#' End of support notice: As of October 1, 2025, Amazon S3 has discontinued
#' support for Email Grantee Access Control Lists (ACLs)
#'
#' @description
#' End of support notice: As of October 1, 2025, Amazon S3 has discontinued
#' support for Email Grantee Access Control Lists (ACLs). If you attempt to
#' use an Email Grantee ACL in a request after October 1, 2025, the request
#' will receive an `HTTP 405` (Method Not Allowed) error.
#' 
#' This change affects the following Amazon Web Services Regions: US East
#' (N. Virginia), US West (N. California), US West (Oregon), Asia Pacific
#' (Singapore), Asia Pacific (Sydney), Asia Pacific (Tokyo), Europe
#' (Ireland), and South America (São Paulo).
#' 
#' This operation is not supported for directory buckets.
#' 
#' Uses the `acl` subresource to set the access control list (ACL)
#' permissions for a new or existing object in an S3 bucket. You must have
#' the `WRITE_ACP` permission to set the ACL of an object. For more
#' information, see [What permissions can I
#' grant?](https://docs.aws.amazon.com/AmazonS3/latest/userguide/acl-overview.html#permissions)
#' in the *Amazon S3 User Guide*.
#' 
#' This functionality is not supported for Amazon S3 on Outposts.
#' 
#' Depending on your application needs, you can choose to set the ACL on an
#' object using either the request body or the headers. For example, if you
#' have an existing application that updates a bucket ACL using the request
#' body, you can continue to use that approach. For more information, see
#' [Access Control List (ACL)
#' Overview](https://docs.aws.amazon.com/AmazonS3/latest/userguide/acl-overview.html)
#' in the *Amazon S3 User Guide*.
#' 
#' If your bucket uses the bucket owner enforced setting for S3 Object
#' Ownership, ACLs are disabled and no longer affect permissions. You must
#' use policies to grant access to your bucket and the objects in it.
#' Requests to set ACLs or update ACLs fail and return the
#' `AccessControlListNotSupported` error code. Requests to read ACLs are
#' still supported. For more information, see [Controlling object
#' ownership](https://docs.aws.amazon.com/AmazonS3/latest/userguide/about-object-ownership.html)
#' in the *Amazon S3 User Guide*.
#' 
#' ### Permissions
#' 
#' You can set access permissions using one of the following methods:
#' 
#' -   Specify a canned ACL with the `x-amz-acl` request header. Amazon S3
#'     supports a set of predefined ACLs, known as canned ACLs. Each canned
#'     ACL has a predefined set of grantees and permissions. Specify the
#'     canned ACL name as the value of `x-amz-ac`l. If you use this header,
#'     you cannot use other access control-specific headers in your
#'     request. For more information, see [Canned
#'     ACL](https://docs.aws.amazon.com/AmazonS3/latest/userguide/acl-overview.html#CannedACL).
#' 
#' -   Specify access permissions explicitly with the `x-amz-grant-read`,
#'     `x-amz-grant-read-acp`, `x-amz-grant-write-acp`, and
#'     `x-amz-grant-full-control` headers. When using these headers, you
#'     specify explicit access permissions and grantees (Amazon Web
#'     Services accounts or Amazon S3 groups) who will receive the
#'     permission. If you use these ACL-specific headers, you cannot use
#'     `x-amz-acl` header to set a canned ACL. These parameters map to the
#'     set of permissions that Amazon S3 supports in an ACL. For more
#'     information, see [Access Control List (ACL)
#'     Overview](https://docs.aws.amazon.com/AmazonS3/latest/userguide/acl-overview.html).
#' 
#'     You specify each grantee as a type=value pair, where the type is one
#'     of the following:
#' 
#'     -   `id` – if the value specified is the canonical user ID of an
#'         Amazon Web Services account
#' 
#'     -   `uri` – if you are granting permissions to a predefined group
#' 
#'     -   `emailAddress` – if the value specified is the email address of
#'         an Amazon Web Services account
#' 
#'         Using email addresses to specify a grantee is only supported in
#'         the following Amazon Web Services Regions:
#' 
#'         -   US East (N. Virginia)
#' 
#'         -   US West (N. California)
#' 
#'         -   US West (Oregon)
#' 
#'         -   Asia Pacific (Singapore)
#' 
#'         -   Asia Pacific (Sydney)
#' 
#'         -   Asia Pacific (Tokyo)
#' 
#'         -   Europe (Ireland)
#' 
#'         -   South America (São Paulo)
#' 
#'         For a list of all the Amazon S3 supported Regions and endpoints,
#'         see [Regions and
#'         Endpoints](https://docs.aws.amazon.com/general/latest/gr/rande.html#s3_region)
#'         in the Amazon Web Services General Reference.
#' 
#'     For example, the following `x-amz-grant-read` header grants list
#'     objects permission to the two Amazon Web Services accounts
#'     identified by their email addresses.
#' 
#'     `x-amz-grant-read: emailAddress="xyz@@amazon.com", emailAddress="abc@@amazon.com" `
#' 
#' You can use either a canned ACL or specify access permissions
#' explicitly. You cannot do both.
#' 
#' ### Grantee Values
#' 
#' You can specify the person (grantee) to whom you're assigning access
#' rights (using request elements) in the following ways. For examples of
#' how to specify these grantee values in JSON format, see the Amazon Web
#' Services CLI example in [Enabling Amazon S3 server access
#' logging](https://docs.aws.amazon.com/AmazonS3/latest/userguide/enable-server-access-logging.html)
#' in the *Amazon S3 User Guide*.
#' 
#' -   By the person's ID:
#' 
#'     `<Grantee xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:type="CanonicalUser"><ID><>ID<></ID><DisplayName><>GranteesEmail<></DisplayName> </Grantee>`
#' 
#'     DisplayName is optional and ignored in the request.
#' 
#' -   By URI:
#' 
#'     `<Grantee xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:type="Group"><URI><>http://acs.amazonaws.com/groups/global/AuthenticatedUsers<></URI></Grantee>`
#' 
#' -   By Email address:
#' 
#'     `<Grantee xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:type="AmazonCustomerByEmail"><EmailAddress><>Grantees@@email.com<></EmailAddress>lt;/Grantee>`
#' 
#'     The grantee is resolved to the CanonicalUser and, in a response to a
#'     GET Object acl request, appears as the CanonicalUser.
#' 
#'     Using email addresses to specify a grantee is only supported in the
#'     following Amazon Web Services Regions:
#' 
#'     -   US East (N. Virginia)
#' 
#'     -   US West (N. California)
#' 
#'     -   US West (Oregon)
#' 
#'     -   Asia Pacific (Singapore)
#' 
#'     -   Asia Pacific (Sydney)
#' 
#'     -   Asia Pacific (Tokyo)
#' 
#'     -   Europe (Ireland)
#' 
#'     -   South America (São Paulo)
#' 
#'     For a list of all the Amazon S3 supported Regions and endpoints, see
#'     [Regions and
#'     Endpoints](https://docs.aws.amazon.com/general/latest/gr/rande.html#s3_region)
#'     in the Amazon Web Services General Reference.
#' 
#' ### Versioning
#' 
#' The ACL of an object is set at the object version level. By default, PUT
#' sets the ACL of the current version of an object. To set the ACL of a
#' different version, use the `versionId` subresource.
#' 
#' The following operations are related to
#' [`put_object_acl`][s3_put_object_acl]:
#' 
#' -   [`copy_object`][s3_copy_object]
#' 
#' -   [`get_object`][s3_get_object]
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#'
#' @usage
#' s3_put_object_acl(ACL, AccessControlPolicy, Bucket, ContentMD5,
#'   ChecksumAlgorithm, GrantFullControl, GrantRead, GrantReadACP,
#'   GrantWrite, GrantWriteACP, Key, RequestPayer, VersionId,
#'   ExpectedBucketOwner)
#'
#' @param ACL The canned ACL to apply to the object. For more information, see [Canned
#' ACL](https://docs.aws.amazon.com/AmazonS3/latest/userguide/acl-overview.html#CannedACL).
#' @param AccessControlPolicy Contains the elements that set the ACL permissions for an object per
#' grantee.
#' @param Bucket &#91;required&#93; The bucket name that contains the object to which you want to attach the
#' ACL.
#' 
#' **Access points** - When you use this action with an access point for
#' general purpose buckets, you must provide the alias of the access point
#' in place of the bucket name or specify the access point ARN. When you
#' use this action with an access point for directory buckets, you must
#' provide the access point name in place of the bucket name. When using
#' the access point ARN, you must direct requests to the access point
#' hostname. The access point hostname takes the form
#' *AccessPointName*-*AccountId*.s3-accesspoint.*Region*.amazonaws.com.
#' When using this action with an access point through the Amazon Web
#' Services SDKs, you provide the access point ARN in place of the bucket
#' name. For more information about access point ARNs, see [Using access
#' points](https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-access-points.html)
#' in the *Amazon S3 User Guide*.
#' 
#' **S3 on Outposts** - When you use this action with S3 on Outposts, you
#' must direct requests to the S3 on Outposts hostname. The S3 on Outposts
#' hostname takes the form
#' ` AccessPointName-AccountId.outpostID.s3-outposts.Region.amazonaws.com`.
#' When you use this action with S3 on Outposts, the destination bucket
#' must be the Outposts access point ARN or the access point alias. For
#' more information about S3 on Outposts, see [What is S3 on
#' Outposts?](https://docs.aws.amazon.com/AmazonS3/latest/s3-outposts/S3onOutposts.html)
#' in the *Amazon S3 User Guide*.
#' @param ContentMD5 The Base64 encoded 128-bit `MD5` digest of the data. This header must be
#' used as a message integrity check to verify that the request body was
#' not corrupted in transit. For more information, go to [RFC
#' 1864.\>](https://www.ietf.org/rfc/rfc1864.txt)
#' 
#' For requests made using the Amazon Web Services Command Line Interface
#' (CLI) or Amazon Web Services SDKs, this field is calculated
#' automatically.
#' @param ChecksumAlgorithm Indicates the algorithm used to create the checksum for the object when
#' you use the SDK. This header will not provide any additional
#' functionality if you don't use the SDK. When you send this header, there
#' must be a corresponding `x-amz-checksum` or `x-amz-trailer` header sent.
#' Otherwise, Amazon S3 fails the request with the HTTP status code
#' `400 Bad Request`. For more information, see [Checking object
#' integrity](https://docs.aws.amazon.com/AmazonS3/latest/userguide/checking-object-integrity.html)
#' in the *Amazon S3 User Guide*.
#' 
#' If you provide an individual checksum, Amazon S3 ignores any provided
#' `ChecksumAlgorithm` parameter.
#' @param GrantFullControl Allows grantee the read, write, read ACP, and write ACP permissions on
#' the bucket.
#' 
#' This functionality is not supported for Amazon S3 on Outposts.
#' @param GrantRead Allows grantee to list the objects in the bucket.
#' 
#' This functionality is not supported for Amazon S3 on Outposts.
#' @param GrantReadACP Allows grantee to read the bucket ACL.
#' 
#' This functionality is not supported for Amazon S3 on Outposts.
#' @param GrantWrite Allows grantee to create new objects in the bucket.
#' 
#' For the bucket and object owners of existing objects, also allows
#' deletions and overwrites of those objects.
#' @param GrantWriteACP Allows grantee to write the ACL for the applicable bucket.
#' 
#' This functionality is not supported for Amazon S3 on Outposts.
#' @param Key &#91;required&#93; Key for which the PUT action was initiated.
#' @param RequestPayer 
#' @param VersionId Version ID used to reference a specific version of the object.
#' 
#' This functionality is not supported for directory buckets.
#' @param ExpectedBucketOwner The account ID of the expected bucket owner. If the account ID that you
#' provide does not match the actual owner of the bucket, the request fails
#' with the HTTP status code `403 Forbidden` (access denied).
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   RequestCharged = "requester"
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$put_object_acl(
#'   ACL = "private"|"public-read"|"public-read-write"|"authenticated-read"|"aws-exec-read"|"bucket-owner-read"|"bucket-owner-full-control",
#'   AccessControlPolicy = list(
#'     Grants = list(
#'       list(
#'         Grantee = list(
#'           DisplayName = "string",
#'           EmailAddress = "string",
#'           ID = "string",
#'           Type = "CanonicalUser"|"AmazonCustomerByEmail"|"Group",
#'           URI = "string"
#'         ),
#'         Permission = "FULL_CONTROL"|"WRITE"|"WRITE_ACP"|"READ"|"READ_ACP"
#'       )
#'     ),
#'     Owner = list(
#'       DisplayName = "string",
#'       ID = "string"
#'     )
#'   ),
#'   Bucket = "string",
#'   ContentMD5 = "string",
#'   ChecksumAlgorithm = "CRC32"|"CRC32C"|"SHA1"|"SHA256"|"CRC64NVME",
#'   GrantFullControl = "string",
#'   GrantRead = "string",
#'   GrantReadACP = "string",
#'   GrantWrite = "string",
#'   GrantWriteACP = "string",
#'   Key = "string",
#'   RequestPayer = "requester",
#'   VersionId = "string",
#'   ExpectedBucketOwner = "string"
#' )
#' ```
#'
#' @examples
#' \dontrun{
#' # The following example adds grants to an object ACL. The first permission
#' # grants user1 and user2 FULL_CONTROL and the AllUsers group READ
#' # permission.
#' svc$put_object_acl(
#'   AccessControlPolicy = structure(
#'     list(),
#'     names = character(
#'       0
#'     )
#'   ),
#'   Bucket = "examplebucket",
#'   GrantFullControl = "emailaddress=user1@example.com,emailaddress=user2@example.com",
#'   GrantRead = "uri=http://acs.amazonaws.com/groups/global/AllUsers",
#'   Key = "HappyFace.jpg"
#' )
#' }
#'
#' @keywords internal
#'
#' @rdname s3_put_object_acl
#'
#' @aliases s3_put_object_acl
s3_put_object_acl <- function(ACL = NULL, AccessControlPolicy = NULL, Bucket, ContentMD5 = NULL, ChecksumAlgorithm = NULL, GrantFullControl = NULL, GrantRead = NULL, GrantReadACP = NULL, GrantWrite = NULL, GrantWriteACP = NULL, Key, RequestPayer = NULL, VersionId = NULL, ExpectedBucketOwner = NULL) {
  op <- new_operation(
    name = "PutObjectAcl",
    http_method = "PUT",
    http_path = "/{Bucket}/{Key+}?acl",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .s3$put_object_acl_input(ACL = ACL, AccessControlPolicy = AccessControlPolicy, Bucket = Bucket, ContentMD5 = ContentMD5, ChecksumAlgorithm = ChecksumAlgorithm, GrantFullControl = GrantFullControl, GrantRead = GrantRead, GrantReadACP = GrantReadACP, GrantWrite = GrantWrite, GrantWriteACP = GrantWriteACP, Key = Key, RequestPayer = RequestPayer, VersionId = VersionId, ExpectedBucketOwner = ExpectedBucketOwner)
  output <- .s3$put_object_acl_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$put_object_acl <- s3_put_object_acl

#' This operation is not supported for directory buckets
#'
#' @description
#' This operation is not supported for directory buckets.
#' 
#' Applies a legal hold configuration to the specified object. For more
#' information, see [Locking
#' Objects](https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock.html).
#' 
#' This functionality is not supported for Amazon S3 on Outposts.
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#'
#' @usage
#' s3_put_object_legal_hold(Bucket, Key, LegalHold, RequestPayer,
#'   VersionId, ContentMD5, ChecksumAlgorithm, ExpectedBucketOwner)
#'
#' @param Bucket &#91;required&#93; The bucket name containing the object that you want to place a legal
#' hold on.
#' 
#' **Access points** - When you use this action with an access point for
#' general purpose buckets, you must provide the alias of the access point
#' in place of the bucket name or specify the access point ARN. When you
#' use this action with an access point for directory buckets, you must
#' provide the access point name in place of the bucket name. When using
#' the access point ARN, you must direct requests to the access point
#' hostname. The access point hostname takes the form
#' *AccessPointName*-*AccountId*.s3-accesspoint.*Region*.amazonaws.com.
#' When using this action with an access point through the Amazon Web
#' Services SDKs, you provide the access point ARN in place of the bucket
#' name. For more information about access point ARNs, see [Using access
#' points](https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-access-points.html)
#' in the *Amazon S3 User Guide*.
#' @param Key &#91;required&#93; The key name for the object that you want to place a legal hold on.
#' @param LegalHold Container element for the legal hold configuration you want to apply to
#' the specified object.
#' @param RequestPayer 
#' @param VersionId The version ID of the object that you want to place a legal hold on.
#' @param ContentMD5 The MD5 hash for the request body.
#' 
#' For requests made using the Amazon Web Services Command Line Interface
#' (CLI) or Amazon Web Services SDKs, this field is calculated
#' automatically.
#' @param ChecksumAlgorithm Indicates the algorithm used to create the checksum for the object when
#' you use the SDK. This header will not provide any additional
#' functionality if you don't use the SDK. When you send this header, there
#' must be a corresponding `x-amz-checksum` or `x-amz-trailer` header sent.
#' Otherwise, Amazon S3 fails the request with the HTTP status code
#' `400 Bad Request`. For more information, see [Checking object
#' integrity](https://docs.aws.amazon.com/AmazonS3/latest/userguide/checking-object-integrity.html)
#' in the *Amazon S3 User Guide*.
#' 
#' If you provide an individual checksum, Amazon S3 ignores any provided
#' `ChecksumAlgorithm` parameter.
#' @param ExpectedBucketOwner The account ID of the expected bucket owner. If the account ID that you
#' provide does not match the actual owner of the bucket, the request fails
#' with the HTTP status code `403 Forbidden` (access denied).
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   RequestCharged = "requester"
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$put_object_legal_hold(
#'   Bucket = "string",
#'   Key = "string",
#'   LegalHold = list(
#'     Status = "ON"|"OFF"
#'   ),
#'   RequestPayer = "requester",
#'   VersionId = "string",
#'   ContentMD5 = "string",
#'   ChecksumAlgorithm = "CRC32"|"CRC32C"|"SHA1"|"SHA256"|"CRC64NVME",
#'   ExpectedBucketOwner = "string"
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname s3_put_object_legal_hold
#'
#' @aliases s3_put_object_legal_hold
s3_put_object_legal_hold <- function(Bucket, Key, LegalHold = NULL, RequestPayer = NULL, VersionId = NULL, ContentMD5 = NULL, ChecksumAlgorithm = NULL, ExpectedBucketOwner = NULL) {
  op <- new_operation(
    name = "PutObjectLegalHold",
    http_method = "PUT",
    http_path = "/{Bucket}/{Key+}?legal-hold",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .s3$put_object_legal_hold_input(Bucket = Bucket, Key = Key, LegalHold = LegalHold, RequestPayer = RequestPayer, VersionId = VersionId, ContentMD5 = ContentMD5, ChecksumAlgorithm = ChecksumAlgorithm, ExpectedBucketOwner = ExpectedBucketOwner)
  output <- .s3$put_object_legal_hold_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$put_object_legal_hold <- s3_put_object_legal_hold

#' This operation is not supported for directory buckets
#'
#' @description
#' This operation is not supported for directory buckets.
#' 
#' Places an Object Lock configuration on the specified bucket. The rule
#' specified in the Object Lock configuration will be applied by default to
#' every new object placed in the specified bucket. For more information,
#' see [Locking
#' Objects](https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock.html).
#' 
#' -   The `DefaultRetention` settings require both a mode and a period.
#' 
#' -   The `DefaultRetention` period can be either `Days` or `Years` but
#'     you must select one. You cannot specify `Days` and `Years` at the
#'     same time.
#' 
#' -   You can enable Object Lock for new or existing buckets. For more
#'     information, see [Configuring Object
#'     Lock](https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock-configure.html).
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#'
#' @usage
#' s3_put_object_lock_configuration(Bucket, ObjectLockConfiguration,
#'   RequestPayer, Token, ContentMD5, ChecksumAlgorithm, ExpectedBucketOwner)
#'
#' @param Bucket &#91;required&#93; The bucket whose Object Lock configuration you want to create or
#' replace.
#' @param ObjectLockConfiguration The Object Lock configuration that you want to apply to the specified
#' bucket.
#' @param RequestPayer 
#' @param Token A token to allow Object Lock to be enabled for an existing bucket.
#' @param ContentMD5 The MD5 hash for the request body.
#' 
#' For requests made using the Amazon Web Services Command Line Interface
#' (CLI) or Amazon Web Services SDKs, this field is calculated
#' automatically.
#' @param ChecksumAlgorithm Indicates the algorithm used to create the checksum for the object when
#' you use the SDK. This header will not provide any additional
#' functionality if you don't use the SDK. When you send this header, there
#' must be a corresponding `x-amz-checksum` or `x-amz-trailer` header sent.
#' Otherwise, Amazon S3 fails the request with the HTTP status code
#' `400 Bad Request`. For more information, see [Checking object
#' integrity](https://docs.aws.amazon.com/AmazonS3/latest/userguide/checking-object-integrity.html)
#' in the *Amazon S3 User Guide*.
#' 
#' If you provide an individual checksum, Amazon S3 ignores any provided
#' `ChecksumAlgorithm` parameter.
#' @param ExpectedBucketOwner The account ID of the expected bucket owner. If the account ID that you
#' provide does not match the actual owner of the bucket, the request fails
#' with the HTTP status code `403 Forbidden` (access denied).
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   RequestCharged = "requester"
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$put_object_lock_configuration(
#'   Bucket = "string",
#'   ObjectLockConfiguration = list(
#'     ObjectLockEnabled = "Enabled",
#'     Rule = list(
#'       DefaultRetention = list(
#'         Mode = "GOVERNANCE"|"COMPLIANCE",
#'         Days = 123,
#'         Years = 123
#'       )
#'     )
#'   ),
#'   RequestPayer = "requester",
#'   Token = "string",
#'   ContentMD5 = "string",
#'   ChecksumAlgorithm = "CRC32"|"CRC32C"|"SHA1"|"SHA256"|"CRC64NVME",
#'   ExpectedBucketOwner = "string"
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname s3_put_object_lock_configuration
#'
#' @aliases s3_put_object_lock_configuration
s3_put_object_lock_configuration <- function(Bucket, ObjectLockConfiguration = NULL, RequestPayer = NULL, Token = NULL, ContentMD5 = NULL, ChecksumAlgorithm = NULL, ExpectedBucketOwner = NULL) {
  op <- new_operation(
    name = "PutObjectLockConfiguration",
    http_method = "PUT",
    http_path = "/{Bucket}?object-lock",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .s3$put_object_lock_configuration_input(Bucket = Bucket, ObjectLockConfiguration = ObjectLockConfiguration, RequestPayer = RequestPayer, Token = Token, ContentMD5 = ContentMD5, ChecksumAlgorithm = ChecksumAlgorithm, ExpectedBucketOwner = ExpectedBucketOwner)
  output <- .s3$put_object_lock_configuration_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$put_object_lock_configuration <- s3_put_object_lock_configuration

#' This operation is not supported for directory buckets
#'
#' @description
#' This operation is not supported for directory buckets.
#' 
#' Places an Object Retention configuration on an object. For more
#' information, see [Locking
#' Objects](https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock.html).
#' Users or accounts require the `s3:PutObjectRetention` permission in
#' order to place an Object Retention configuration on objects. Bypassing a
#' Governance Retention configuration requires the
#' `s3:BypassGovernanceRetention` permission.
#' 
#' This functionality is not supported for Amazon S3 on Outposts.
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#'
#' @usage
#' s3_put_object_retention(Bucket, Key, Retention, RequestPayer, VersionId,
#'   BypassGovernanceRetention, ContentMD5, ChecksumAlgorithm,
#'   ExpectedBucketOwner)
#'
#' @param Bucket &#91;required&#93; The bucket name that contains the object you want to apply this Object
#' Retention configuration to.
#' 
#' **Access points** - When you use this action with an access point for
#' general purpose buckets, you must provide the alias of the access point
#' in place of the bucket name or specify the access point ARN. When you
#' use this action with an access point for directory buckets, you must
#' provide the access point name in place of the bucket name. When using
#' the access point ARN, you must direct requests to the access point
#' hostname. The access point hostname takes the form
#' *AccessPointName*-*AccountId*.s3-accesspoint.*Region*.amazonaws.com.
#' When using this action with an access point through the Amazon Web
#' Services SDKs, you provide the access point ARN in place of the bucket
#' name. For more information about access point ARNs, see [Using access
#' points](https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-access-points.html)
#' in the *Amazon S3 User Guide*.
#' @param Key &#91;required&#93; The key name for the object that you want to apply this Object Retention
#' configuration to.
#' @param Retention The container element for the Object Retention configuration.
#' @param RequestPayer 
#' @param VersionId The version ID for the object that you want to apply this Object
#' Retention configuration to.
#' @param BypassGovernanceRetention Indicates whether this action should bypass Governance-mode
#' restrictions.
#' @param ContentMD5 The MD5 hash for the request body.
#' 
#' For requests made using the Amazon Web Services Command Line Interface
#' (CLI) or Amazon Web Services SDKs, this field is calculated
#' automatically.
#' @param ChecksumAlgorithm Indicates the algorithm used to create the checksum for the object when
#' you use the SDK. This header will not provide any additional
#' functionality if you don't use the SDK. When you send this header, there
#' must be a corresponding `x-amz-checksum` or `x-amz-trailer` header sent.
#' Otherwise, Amazon S3 fails the request with the HTTP status code
#' `400 Bad Request`. For more information, see [Checking object
#' integrity](https://docs.aws.amazon.com/AmazonS3/latest/userguide/checking-object-integrity.html)
#' in the *Amazon S3 User Guide*.
#' 
#' If you provide an individual checksum, Amazon S3 ignores any provided
#' `ChecksumAlgorithm` parameter.
#' @param ExpectedBucketOwner The account ID of the expected bucket owner. If the account ID that you
#' provide does not match the actual owner of the bucket, the request fails
#' with the HTTP status code `403 Forbidden` (access denied).
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   RequestCharged = "requester"
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$put_object_retention(
#'   Bucket = "string",
#'   Key = "string",
#'   Retention = list(
#'     Mode = "GOVERNANCE"|"COMPLIANCE",
#'     RetainUntilDate = as.POSIXct(
#'       "2015-01-01"
#'     )
#'   ),
#'   RequestPayer = "requester",
#'   VersionId = "string",
#'   BypassGovernanceRetention = TRUE|FALSE,
#'   ContentMD5 = "string",
#'   ChecksumAlgorithm = "CRC32"|"CRC32C"|"SHA1"|"SHA256"|"CRC64NVME",
#'   ExpectedBucketOwner = "string"
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname s3_put_object_retention
#'
#' @aliases s3_put_object_retention
s3_put_object_retention <- function(Bucket, Key, Retention = NULL, RequestPayer = NULL, VersionId = NULL, BypassGovernanceRetention = NULL, ContentMD5 = NULL, ChecksumAlgorithm = NULL, ExpectedBucketOwner = NULL) {
  op <- new_operation(
    name = "PutObjectRetention",
    http_method = "PUT",
    http_path = "/{Bucket}/{Key+}?retention",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .s3$put_object_retention_input(Bucket = Bucket, Key = Key, Retention = Retention, RequestPayer = RequestPayer, VersionId = VersionId, BypassGovernanceRetention = BypassGovernanceRetention, ContentMD5 = ContentMD5, ChecksumAlgorithm = ChecksumAlgorithm, ExpectedBucketOwner = ExpectedBucketOwner)
  output <- .s3$put_object_retention_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$put_object_retention <- s3_put_object_retention

#' This operation is not supported for directory buckets
#'
#' @description
#' This operation is not supported for directory buckets.
#' 
#' Sets the supplied tag-set to an object that already exists in a bucket.
#' A tag is a key-value pair. For more information, see [Object
#' Tagging](https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-tagging.html).
#' 
#' You can associate tags with an object by sending a PUT request against
#' the tagging subresource that is associated with the object. You can
#' retrieve tags by sending a GET request. For more information, see
#' [`get_object_tagging`][s3_get_object_tagging].
#' 
#' For tagging-related restrictions related to characters and encodings,
#' see [Tag
#' Restrictions](https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/).
#' Note that Amazon S3 limits the maximum number of tags to 10 tags per
#' object.
#' 
#' To use this operation, you must have permission to perform the
#' `s3:PutObjectTagging` action. By default, the bucket owner has this
#' permission and can grant this permission to others.
#' 
#' To put tags of any other version, use the `versionId` query parameter.
#' You also need permission for the `s3:PutObjectVersionTagging` action.
#' 
#' [`put_object_tagging`][s3_put_object_tagging] has the following special
#' errors. For more Amazon S3 errors see, [Error
#' Responses](https://docs.aws.amazon.com/AmazonS3/latest/API/ErrorResponses.html).
#' 
#' -   `InvalidTag` - The tag provided was not a valid tag. This error can
#'     occur if the tag did not pass input validation. For more
#'     information, see [Object
#'     Tagging](https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-tagging.html).
#' 
#' -   `MalformedXML` - The XML provided does not match the schema.
#' 
#' -   `OperationAborted` - A conflicting conditional action is currently
#'     in progress against this resource. Please try again.
#' 
#' -   `InternalError` - The service was unable to apply the provided tag
#'     to the object.
#' 
#' The following operations are related to
#' [`put_object_tagging`][s3_put_object_tagging]:
#' 
#' -   [`get_object_tagging`][s3_get_object_tagging]
#' 
#' -   [`delete_object_tagging`][s3_delete_object_tagging]
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#'
#' @usage
#' s3_put_object_tagging(Bucket, Key, VersionId, ContentMD5,
#'   ChecksumAlgorithm, Tagging, ExpectedBucketOwner, RequestPayer)
#'
#' @param Bucket &#91;required&#93; The bucket name containing the object.
#' 
#' **Access points** - When you use this action with an access point for
#' general purpose buckets, you must provide the alias of the access point
#' in place of the bucket name or specify the access point ARN. When you
#' use this action with an access point for directory buckets, you must
#' provide the access point name in place of the bucket name. When using
#' the access point ARN, you must direct requests to the access point
#' hostname. The access point hostname takes the form
#' *AccessPointName*-*AccountId*.s3-accesspoint.*Region*.amazonaws.com.
#' When using this action with an access point through the Amazon Web
#' Services SDKs, you provide the access point ARN in place of the bucket
#' name. For more information about access point ARNs, see [Using access
#' points](https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-access-points.html)
#' in the *Amazon S3 User Guide*.
#' 
#' **S3 on Outposts** - When you use this action with S3 on Outposts, you
#' must direct requests to the S3 on Outposts hostname. The S3 on Outposts
#' hostname takes the form
#' ` AccessPointName-AccountId.outpostID.s3-outposts.Region.amazonaws.com`.
#' When you use this action with S3 on Outposts, the destination bucket
#' must be the Outposts access point ARN or the access point alias. For
#' more information about S3 on Outposts, see [What is S3 on
#' Outposts?](https://docs.aws.amazon.com/AmazonS3/latest/s3-outposts/S3onOutposts.html)
#' in the *Amazon S3 User Guide*.
#' @param Key &#91;required&#93; Name of the object key.
#' @param VersionId The versionId of the object that the tag-set will be added to.
#' @param ContentMD5 The MD5 hash for the request body.
#' 
#' For requests made using the Amazon Web Services Command Line Interface
#' (CLI) or Amazon Web Services SDKs, this field is calculated
#' automatically.
#' @param ChecksumAlgorithm Indicates the algorithm used to create the checksum for the object when
#' you use the SDK. This header will not provide any additional
#' functionality if you don't use the SDK. When you send this header, there
#' must be a corresponding `x-amz-checksum` or `x-amz-trailer` header sent.
#' Otherwise, Amazon S3 fails the request with the HTTP status code
#' `400 Bad Request`. For more information, see [Checking object
#' integrity](https://docs.aws.amazon.com/AmazonS3/latest/userguide/checking-object-integrity.html)
#' in the *Amazon S3 User Guide*.
#' 
#' If you provide an individual checksum, Amazon S3 ignores any provided
#' `ChecksumAlgorithm` parameter.
#' @param Tagging &#91;required&#93; Container for the `TagSet` and `Tag` elements
#' @param ExpectedBucketOwner The account ID of the expected bucket owner. If the account ID that you
#' provide does not match the actual owner of the bucket, the request fails
#' with the HTTP status code `403 Forbidden` (access denied).
#' @param RequestPayer Confirms that the requester knows that she or he will be charged for the
#' tagging object request. Bucket owners need not specify this parameter in
#' their requests.
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   VersionId = "string"
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$put_object_tagging(
#'   Bucket = "string",
#'   Key = "string",
#'   VersionId = "string",
#'   ContentMD5 = "string",
#'   ChecksumAlgorithm = "CRC32"|"CRC32C"|"SHA1"|"SHA256"|"CRC64NVME",
#'   Tagging = list(
#'     TagSet = list(
#'       list(
#'         Key = "string",
#'         Value = "string"
#'       )
#'     )
#'   ),
#'   ExpectedBucketOwner = "string",
#'   RequestPayer = "requester"
#' )
#' ```
#'
#' @examples
#' \dontrun{
#' # The following example adds tags to an existing object.
#' svc$put_object_tagging(
#'   Bucket = "examplebucket",
#'   Key = "HappyFace.jpg",
#'   Tagging = list(
#'     TagSet = list(
#'       list(
#'         Key = "Key3",
#'         Value = "Value3"
#'       ),
#'       list(
#'         Key = "Key4",
#'         Value = "Value4"
#'       )
#'     )
#'   )
#' )
#' }
#'
#' @keywords internal
#'
#' @rdname s3_put_object_tagging
#'
#' @aliases s3_put_object_tagging
s3_put_object_tagging <- function(Bucket, Key, VersionId = NULL, ContentMD5 = NULL, ChecksumAlgorithm = NULL, Tagging, ExpectedBucketOwner = NULL, RequestPayer = NULL) {
  op <- new_operation(
    name = "PutObjectTagging",
    http_method = "PUT",
    http_path = "/{Bucket}/{Key+}?tagging",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .s3$put_object_tagging_input(Bucket = Bucket, Key = Key, VersionId = VersionId, ContentMD5 = ContentMD5, ChecksumAlgorithm = ChecksumAlgorithm, Tagging = Tagging, ExpectedBucketOwner = ExpectedBucketOwner, RequestPayer = RequestPayer)
  output <- .s3$put_object_tagging_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$put_object_tagging <- s3_put_object_tagging

#' This operation is not supported for directory buckets
#'
#' @description
#' This operation is not supported for directory buckets.
#' 
#' Creates or modifies the `PublicAccessBlock` configuration for an Amazon
#' S3 bucket. To use this operation, you must have the
#' `s3:PutBucketPublicAccessBlock` permission. For more information about
#' Amazon S3 permissions, see [Specifying Permissions in a
#' Policy](https://docs.aws.amazon.com/AmazonS3/latest/userguide/security_iam_service-with-iam.html#security_iam_service-with-iam-id-based-policies-actions).
#' 
#' When Amazon S3 evaluates the `PublicAccessBlock` configuration for a
#' bucket or an object, it checks the `PublicAccessBlock` configuration for
#' both the bucket (or the bucket that contains the object) and the bucket
#' owner's account. Account-level settings automatically inherit from
#' organization-level policies when present. If the `PublicAccessBlock`
#' configurations are different between the bucket and the account, Amazon
#' S3 uses the most restrictive combination of the bucket-level and
#' account-level settings.
#' 
#' For more information about when Amazon S3 considers a bucket or an
#' object public, see [The Meaning of
#' "Public"](https://docs.aws.amazon.com/AmazonS3/latest/userguide/access-control-block-public-access.html#access-control-block-public-access-policy-status).
#' 
#' The following operations are related to
#' [`put_public_access_block`][s3_put_public_access_block]:
#' 
#' -   [`get_public_access_block`][s3_get_public_access_block]
#' 
#' -   [`delete_public_access_block`][s3_delete_public_access_block]
#' 
#' -   [`get_bucket_policy_status`][s3_get_bucket_policy_status]
#' 
#' -   [Using Amazon S3 Block Public
#'     Access](https://docs.aws.amazon.com/AmazonS3/latest/userguide/access-control-block-public-access.html)
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#'
#' @usage
#' s3_put_public_access_block(Bucket, ContentMD5, ChecksumAlgorithm,
#'   PublicAccessBlockConfiguration, ExpectedBucketOwner)
#'
#' @param Bucket &#91;required&#93; The name of the Amazon S3 bucket whose `PublicAccessBlock` configuration
#' you want to set.
#' @param ContentMD5 The MD5 hash of the
#' [`put_public_access_block`][s3_put_public_access_block] request body.
#' 
#' For requests made using the Amazon Web Services Command Line Interface
#' (CLI) or Amazon Web Services SDKs, this field is calculated
#' automatically.
#' @param ChecksumAlgorithm Indicates the algorithm used to create the checksum for the object when
#' you use the SDK. This header will not provide any additional
#' functionality if you don't use the SDK. When you send this header, there
#' must be a corresponding `x-amz-checksum` or `x-amz-trailer` header sent.
#' Otherwise, Amazon S3 fails the request with the HTTP status code
#' `400 Bad Request`. For more information, see [Checking object
#' integrity](https://docs.aws.amazon.com/AmazonS3/latest/userguide/checking-object-integrity.html)
#' in the *Amazon S3 User Guide*.
#' 
#' If you provide an individual checksum, Amazon S3 ignores any provided
#' `ChecksumAlgorithm` parameter.
#' @param PublicAccessBlockConfiguration &#91;required&#93; The `PublicAccessBlock` configuration that you want to apply to this
#' Amazon S3 bucket. You can enable the configuration options in any
#' combination. For more information about when Amazon S3 considers a
#' bucket or object public, see [The Meaning of
#' "Public"](https://docs.aws.amazon.com/AmazonS3/latest/userguide/access-control-block-public-access.html#access-control-block-public-access-policy-status)
#' in the *Amazon S3 User Guide*.
#' @param ExpectedBucketOwner The account ID of the expected bucket owner. If the account ID that you
#' provide does not match the actual owner of the bucket, the request fails
#' with the HTTP status code `403 Forbidden` (access denied).
#'
#' @return
#' An empty list.
#'
#' @section Request syntax:
#' ```
#' svc$put_public_access_block(
#'   Bucket = "string",
#'   ContentMD5 = "string",
#'   ChecksumAlgorithm = "CRC32"|"CRC32C"|"SHA1"|"SHA256"|"CRC64NVME",
#'   PublicAccessBlockConfiguration = list(
#'     BlockPublicAcls = TRUE|FALSE,
#'     IgnorePublicAcls = TRUE|FALSE,
#'     BlockPublicPolicy = TRUE|FALSE,
#'     RestrictPublicBuckets = TRUE|FALSE
#'   ),
#'   ExpectedBucketOwner = "string"
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname s3_put_public_access_block
#'
#' @aliases s3_put_public_access_block
s3_put_public_access_block <- function(Bucket, ContentMD5 = NULL, ChecksumAlgorithm = NULL, PublicAccessBlockConfiguration, ExpectedBucketOwner = NULL) {
  op <- new_operation(
    name = "PutPublicAccessBlock",
    http_method = "PUT",
    http_path = "/{Bucket}?publicAccessBlock",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .s3$put_public_access_block_input(Bucket = Bucket, ContentMD5 = ContentMD5, ChecksumAlgorithm = ChecksumAlgorithm, PublicAccessBlockConfiguration = PublicAccessBlockConfiguration, ExpectedBucketOwner = ExpectedBucketOwner)
  output <- .s3$put_public_access_block_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$put_public_access_block <- s3_put_public_access_block

#' Renames an existing object in a directory bucket that uses the S3
#' Express One Zone storage class
#'
#' @description
#' Renames an existing object in a directory bucket that uses the S3
#' Express One Zone storage class. You can use
#' [`rename_object`][s3_rename_object] by specifying an existing object’s
#' name as the source and the new name of the object as the destination
#' within the same directory bucket.
#' 
#' [`rename_object`][s3_rename_object] is only supported for objects stored
#' in the S3 Express One Zone storage class.
#' 
#' To prevent overwriting an object, you can use the `If-None-Match`
#' conditional header.
#' 
#' -   **If-None-Match** - Renames the object only if an object with the
#'     specified name does not already exist in the directory bucket. If
#'     you don't want to overwrite an existing object, you can add the
#'     `If-None-Match` conditional header with the value `‘*’` in the
#'     [`rename_object`][s3_rename_object] request. Amazon S3 then returns
#'     a `412 Precondition Failed` error if the object with the specified
#'     name already exists. For more information, see [RFC
#'     7232](https://datatracker.ietf.org/doc/rfc7232/).
#' 
#' ### Permissions
#' 
#' To grant access to the [`rename_object`][s3_rename_object] operation on
#' a directory bucket, we recommend that you use the
#' [`create_session`][s3_create_session] operation for session-based
#' authorization. Specifically, you grant the `s3express:CreateSession`
#' permission to the directory bucket in a bucket policy or an IAM
#' identity-based policy. Then, you make the
#' [`create_session`][s3_create_session] API call on the directory bucket
#' to obtain a session token. With the session token in your request
#' header, you can make API requests to this operation. After the session
#' token expires, you make another [`create_session`][s3_create_session]
#' API call to generate a new session token for use. The Amazon Web
#' Services CLI and SDKs will create and manage your session including
#' refreshing the session token automatically to avoid service
#' interruptions when a session expires. In your bucket policy, you can
#' specify the `s3express:SessionMode` condition key to control who can
#' create a `ReadWrite` or `ReadOnly` session. A `ReadWrite` session is
#' required for executing all the Zonal endpoint API operations, including
#' [`rename_object`][s3_rename_object]. For more information about
#' authorization, see
#' [`create_session`](https://docs.aws.amazon.com/AmazonS3/latest/API/API_CreateSession.html)
#' . To learn more about Zonal endpoint API operations, see [Authorizing
#' Zonal endpoint API operations with
#' CreateSession](https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-express-create-session.html)
#' in the *Amazon S3 User Guide*.
#' 
#' ### HTTP Host header syntax
#' 
#' **Directory buckets** - The HTTP Host header syntax is
#' ` Bucket-name.s3express-zone-id.region-code.amazonaws.com`.
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#'
#' @usage
#' s3_rename_object(Bucket, Key, RenameSource, DestinationIfMatch,
#'   DestinationIfNoneMatch, DestinationIfModifiedSince,
#'   DestinationIfUnmodifiedSince, SourceIfMatch, SourceIfNoneMatch,
#'   SourceIfModifiedSince, SourceIfUnmodifiedSince, ClientToken)
#'
#' @param Bucket &#91;required&#93; The bucket name of the directory bucket containing the object.
#' 
#' You must use virtual-hosted-style requests in the format
#' `Bucket-name.s3express-zone-id.region-code.amazonaws.com`. Path-style
#' requests are not supported. Directory bucket names must be unique in the
#' chosen Availability Zone. Bucket names must follow the format
#' `bucket-base-name--zone-id--x-s3 ` (for example,
#' `amzn-s3-demo-bucket--usw2-az1--x-s3`). For information about bucket
#' naming restrictions, see [Directory bucket naming
#' rules](https://docs.aws.amazon.com/AmazonS3/latest/userguide/directory-bucket-naming-rules.html)
#' in the *Amazon S3 User Guide*.
#' @param Key &#91;required&#93; Key name of the object to rename.
#' @param RenameSource &#91;required&#93; Specifies the source for the rename operation. The value must be URL
#' encoded.
#' @param DestinationIfMatch Renames the object only if the ETag (entity tag) value provided during
#' the operation matches the ETag of the object in S3. The `If-Match`
#' header field makes the request method conditional on ETags. If the ETag
#' values do not match, the operation returns a `412 Precondition Failed`
#' error.
#' 
#' Expects the ETag value as a string.
#' @param DestinationIfNoneMatch Renames the object only if the destination does not already exist in the
#' specified directory bucket. If the object does exist when you send a
#' request with `If-None-Match:*`, the S3 API will return a
#' `412 Precondition Failed` error, preventing an overwrite. The
#' `If-None-Match` header prevents overwrites of existing data by
#' validating that there's not an object with the same key name already in
#' your directory bucket.
#' 
#' Expects the `*` character (asterisk).
#' @param DestinationIfModifiedSince Renames the object if the destination exists and if it has been modified
#' since the specified time.
#' @param DestinationIfUnmodifiedSince Renames the object if it hasn't been modified since the specified time.
#' @param SourceIfMatch Renames the object if the source exists and if its entity tag (ETag)
#' matches the specified ETag.
#' @param SourceIfNoneMatch Renames the object if the source exists and if its entity tag (ETag) is
#' different than the specified ETag. If an asterisk (`*`) character is
#' provided, the operation will fail and return a `412 Precondition Failed`
#' error.
#' @param SourceIfModifiedSince Renames the object if the source exists and if it has been modified
#' since the specified time.
#' @param SourceIfUnmodifiedSince Renames the object if the source exists and hasn't been modified since
#' the specified time.
#' @param ClientToken A unique string with a max of 64 ASCII characters in the ASCII range of
#' 33 - 126.
#' 
#' [`rename_object`][s3_rename_object] supports idempotency using a client
#' token. To make an idempotent API request using
#' [`rename_object`][s3_rename_object], specify a client token in the
#' request. You should not reuse the same client token for other API
#' requests. If you retry a request that completed successfully using the
#' same client token and the same parameters, the retry succeeds without
#' performing any further actions. If you retry a successful request using
#' the same client token, but one or more of the parameters are different,
#' the retry fails and an `IdempotentParameterMismatch` error is returned.
#'
#' @return
#' An empty list.
#'
#' @section Request syntax:
#' ```
#' svc$rename_object(
#'   Bucket = "string",
#'   Key = "string",
#'   RenameSource = "string",
#'   DestinationIfMatch = "string",
#'   DestinationIfNoneMatch = "string",
#'   DestinationIfModifiedSince = as.POSIXct(
#'     "2015-01-01"
#'   ),
#'   DestinationIfUnmodifiedSince = as.POSIXct(
#'     "2015-01-01"
#'   ),
#'   SourceIfMatch = "string",
#'   SourceIfNoneMatch = "string",
#'   SourceIfModifiedSince = as.POSIXct(
#'     "2015-01-01"
#'   ),
#'   SourceIfUnmodifiedSince = as.POSIXct(
#'     "2015-01-01"
#'   ),
#'   ClientToken = "string"
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname s3_rename_object
#'
#' @aliases s3_rename_object
s3_rename_object <- function(Bucket, Key, RenameSource, DestinationIfMatch = NULL, DestinationIfNoneMatch = NULL, DestinationIfModifiedSince = NULL, DestinationIfUnmodifiedSince = NULL, SourceIfMatch = NULL, SourceIfNoneMatch = NULL, SourceIfModifiedSince = NULL, SourceIfUnmodifiedSince = NULL, ClientToken = NULL) {
  op <- new_operation(
    name = "RenameObject",
    http_method = "PUT",
    http_path = "/{Bucket}/{Key+}?renameObject",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .s3$rename_object_input(Bucket = Bucket, Key = Key, RenameSource = RenameSource, DestinationIfMatch = DestinationIfMatch, DestinationIfNoneMatch = DestinationIfNoneMatch, DestinationIfModifiedSince = DestinationIfModifiedSince, DestinationIfUnmodifiedSince = DestinationIfUnmodifiedSince, SourceIfMatch = SourceIfMatch, SourceIfNoneMatch = SourceIfNoneMatch, SourceIfModifiedSince = SourceIfModifiedSince, SourceIfUnmodifiedSince = SourceIfUnmodifiedSince, ClientToken = ClientToken)
  output <- .s3$rename_object_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$rename_object <- s3_rename_object

#' This operation is not supported for directory buckets
#'
#' @description
#' This operation is not supported for directory buckets.
#' 
#' Restores an archived copy of an object back into Amazon S3
#' 
#' This functionality is not supported for Amazon S3 on Outposts.
#' 
#' This action performs the following types of requests:
#' 
#' -   `restore an archive` - Restore an archived object
#' 
#' For more information about the `S3` structure in the request body, see
#' the following:
#' 
#' -   [`put_object`][s3_put_object]
#' 
#' -   [Managing Access with
#'     ACLs](https://docs.aws.amazon.com/AmazonS3/latest/userguide/acl-overview.html)
#'     in the *Amazon S3 User Guide*
#' 
#' -   [Protecting Data Using Server-Side
#'     Encryption](https://docs.aws.amazon.com/AmazonS3/latest/userguide/serv-side-encryption.html)
#'     in the *Amazon S3 User Guide*
#' 
#' ### Permissions
#' 
#' To use this operation, you must have permissions to perform the
#' `s3:RestoreObject` action. The bucket owner has this permission by
#' default and can grant this permission to others. For more information
#' about permissions, see [Permissions Related to Bucket Subresource
#' Operations](https://docs.aws.amazon.com/AmazonS3/latest/userguide/security_iam_service-with-iam.html#security_iam_service-with-iam-id-based-policies-actions)
#' and [Managing Access Permissions to Your Amazon S3
#' Resources](https://docs.aws.amazon.com/AmazonS3/latest/userguide/security-iam.html)
#' in the *Amazon S3 User Guide*.
#' 
#' ### Restoring objects
#' 
#' Objects that you archive to the S3 Glacier Flexible Retrieval or S3
#' Glacier Deep Archive storage class, and S3 Intelligent-Tiering Archive
#' or S3 Intelligent-Tiering Deep Archive tiers, are not accessible in real
#' time. For objects in the S3 Glacier Flexible Retrieval or S3 Glacier
#' Deep Archive storage classes, you must first initiate a restore request,
#' and then wait until a temporary copy of the object is available. If you
#' want a permanent copy of the object, create a copy of it in the Amazon
#' S3 Standard storage class in your S3 bucket. To access an archived
#' object, you must restore the object for the duration (number of days)
#' that you specify. For objects in the Archive Access or Deep Archive
#' Access tiers of S3 Intelligent-Tiering, you must first initiate a
#' restore request, and then wait until the object is moved into the
#' Frequent Access tier.
#' 
#' To restore a specific object version, you can provide a version ID. If
#' you don't provide a version ID, Amazon S3 restores the current version.
#' 
#' When restoring an archived object, you can specify one of the following
#' data access tier options in the `Tier` element of the request body:
#' 
#' -   `Expedited` - Expedited retrievals allow you to quickly access your
#'     data stored in the S3 Glacier Flexible Retrieval storage class or S3
#'     Intelligent-Tiering Archive tier when occasional urgent requests for
#'     restoring archives are required. For all but the largest archived
#'     objects (250 MB+), data accessed using Expedited retrievals is
#'     typically made available within 1–5 minutes. Provisioned capacity
#'     ensures that retrieval capacity for Expedited retrievals is
#'     available when you need it. Expedited retrievals and provisioned
#'     capacity are not available for objects stored in the S3 Glacier Deep
#'     Archive storage class or S3 Intelligent-Tiering Deep Archive tier.
#' 
#' -   `Standard` - Standard retrievals allow you to access any of your
#'     archived objects within several hours. This is the default option
#'     for retrieval requests that do not specify the retrieval option.
#'     Standard retrievals typically finish within 3–5 hours for objects
#'     stored in the S3 Glacier Flexible Retrieval storage class or S3
#'     Intelligent-Tiering Archive tier. They typically finish within 12
#'     hours for objects stored in the S3 Glacier Deep Archive storage
#'     class or S3 Intelligent-Tiering Deep Archive tier. Standard
#'     retrievals are free for objects stored in S3 Intelligent-Tiering.
#' 
#' -   `Bulk` - Bulk retrievals free for objects stored in the S3 Glacier
#'     Flexible Retrieval and S3 Intelligent-Tiering storage classes,
#'     enabling you to retrieve large amounts, even petabytes, of data at
#'     no cost. Bulk retrievals typically finish within 5–12 hours for
#'     objects stored in the S3 Glacier Flexible Retrieval storage class or
#'     S3 Intelligent-Tiering Archive tier. Bulk retrievals are also the
#'     lowest-cost retrieval option when restoring objects from S3 Glacier
#'     Deep Archive. They typically finish within 48 hours for objects
#'     stored in the S3 Glacier Deep Archive storage class or S3
#'     Intelligent-Tiering Deep Archive tier.
#' 
#' For more information about archive retrieval options and provisioned
#' capacity for `Expedited` data access, see [Restoring Archived
#' Objects](https://docs.aws.amazon.com/AmazonS3/latest/userguide/restoring-objects.html)
#' in the *Amazon S3 User Guide*.
#' 
#' You can use Amazon S3 restore speed upgrade to change the restore speed
#' to a faster speed while it is in progress. For more information, see
#' [Upgrading the speed of an in-progress
#' restore](https://docs.aws.amazon.com/AmazonS3/latest/userguide/restoring-objects.html#restoring-objects-upgrade-tier.title.html)
#' in the *Amazon S3 User Guide*.
#' 
#' To get the status of object restoration, you can send a `HEAD` request.
#' Operations return the `x-amz-restore` header, which provides information
#' about the restoration status, in the response. You can use Amazon S3
#' event notifications to notify you when a restore is initiated or
#' completed. For more information, see [Configuring Amazon S3 Event
#' Notifications](https://docs.aws.amazon.com/AmazonS3/latest/userguide/EventNotifications.html)
#' in the *Amazon S3 User Guide*.
#' 
#' After restoring an archived object, you can update the restoration
#' period by reissuing the request with a new period. Amazon S3 updates the
#' restoration period relative to the current time and charges only for the
#' request-there are no data transfer charges. You cannot update the
#' restoration period when Amazon S3 is actively processing your current
#' restore request for the object.
#' 
#' If your bucket has a lifecycle configuration with a rule that includes
#' an expiration action, the object expiration overrides the life span that
#' you specify in a restore request. For example, if you restore an object
#' copy for 10 days, but the object is scheduled to expire in 3 days,
#' Amazon S3 deletes the object in 3 days. For more information about
#' lifecycle configuration, see
#' [`put_bucket_lifecycle_configuration`][s3_put_bucket_lifecycle_configuration]
#' and [Object Lifecycle
#' Management](https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lifecycle-mgmt.html)
#' in *Amazon S3 User Guide*.
#' 
#' ### Responses
#' 
#' A successful action returns either the `200 OK` or `202 Accepted` status
#' code.
#' 
#' -   If the object is not previously restored, then Amazon S3 returns
#'     `202 Accepted` in the response.
#' 
#' -   If the object is previously restored, Amazon S3 returns `200 OK` in
#'     the response.
#' 
#' 
#' -   Special errors:
#' 
#'     -   *Code: RestoreAlreadyInProgress*
#' 
#'     -   *Cause: Object restore is already in progress.*
#' 
#'     -   *HTTP Status Code: 409 Conflict*
#' 
#'     -   *SOAP Fault Code Prefix: Client*
#' 
#' -   -   *Code: GlacierExpeditedRetrievalNotAvailable*
#' 
#'     -   *Cause: expedited retrievals are currently not available. Try
#'         again later. (Returned if there is insufficient capacity to
#'         process the Expedited request. This error applies only to
#'         Expedited retrievals and not to S3 Standard or Bulk
#'         retrievals.)*
#' 
#'     -   *HTTP Status Code: 503*
#' 
#'     -   *SOAP Fault Code Prefix: N/A*
#' 
#' The following operations are related to
#' [`restore_object`][s3_restore_object]:
#' 
#' -   [`put_bucket_lifecycle_configuration`][s3_put_bucket_lifecycle_configuration]
#' 
#' -   [`get_bucket_notification_configuration`][s3_get_bucket_notification_configuration]
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#'
#' @usage
#' s3_restore_object(Bucket, Key, VersionId, RestoreRequest, RequestPayer,
#'   ChecksumAlgorithm, ExpectedBucketOwner)
#'
#' @param Bucket &#91;required&#93; The bucket name containing the object to restore.
#' 
#' **Access points** - When you use this action with an access point for
#' general purpose buckets, you must provide the alias of the access point
#' in place of the bucket name or specify the access point ARN. When you
#' use this action with an access point for directory buckets, you must
#' provide the access point name in place of the bucket name. When using
#' the access point ARN, you must direct requests to the access point
#' hostname. The access point hostname takes the form
#' *AccessPointName*-*AccountId*.s3-accesspoint.*Region*.amazonaws.com.
#' When using this action with an access point through the Amazon Web
#' Services SDKs, you provide the access point ARN in place of the bucket
#' name. For more information about access point ARNs, see [Using access
#' points](https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-access-points.html)
#' in the *Amazon S3 User Guide*.
#' 
#' **S3 on Outposts** - When you use this action with S3 on Outposts, you
#' must direct requests to the S3 on Outposts hostname. The S3 on Outposts
#' hostname takes the form
#' ` AccessPointName-AccountId.outpostID.s3-outposts.Region.amazonaws.com`.
#' When you use this action with S3 on Outposts, the destination bucket
#' must be the Outposts access point ARN or the access point alias. For
#' more information about S3 on Outposts, see [What is S3 on
#' Outposts?](https://docs.aws.amazon.com/AmazonS3/latest/s3-outposts/S3onOutposts.html)
#' in the *Amazon S3 User Guide*.
#' @param Key &#91;required&#93; Object key for which the action was initiated.
#' @param VersionId VersionId used to reference a specific version of the object.
#' @param RestoreRequest 
#' @param RequestPayer 
#' @param ChecksumAlgorithm Indicates the algorithm used to create the checksum for the object when
#' you use the SDK. This header will not provide any additional
#' functionality if you don't use the SDK. When you send this header, there
#' must be a corresponding `x-amz-checksum` or `x-amz-trailer` header sent.
#' Otherwise, Amazon S3 fails the request with the HTTP status code
#' `400 Bad Request`. For more information, see [Checking object
#' integrity](https://docs.aws.amazon.com/AmazonS3/latest/userguide/checking-object-integrity.html)
#' in the *Amazon S3 User Guide*.
#' 
#' If you provide an individual checksum, Amazon S3 ignores any provided
#' `ChecksumAlgorithm` parameter.
#' @param ExpectedBucketOwner The account ID of the expected bucket owner. If the account ID that you
#' provide does not match the actual owner of the bucket, the request fails
#' with the HTTP status code `403 Forbidden` (access denied).
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   RequestCharged = "requester",
#'   RestoreOutputPath = "string"
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$restore_object(
#'   Bucket = "string",
#'   Key = "string",
#'   VersionId = "string",
#'   RestoreRequest = list(
#'     Days = 123,
#'     GlacierJobParameters = list(
#'       Tier = "Standard"|"Bulk"|"Expedited"
#'     ),
#'     Type = "SELECT",
#'     Tier = "Standard"|"Bulk"|"Expedited",
#'     Description = "string",
#'     SelectParameters = list(
#'       InputSerialization = list(
#'         CSV = list(
#'           FileHeaderInfo = "USE"|"IGNORE"|"NONE",
#'           Comments = "string",
#'           QuoteEscapeCharacter = "string",
#'           RecordDelimiter = "string",
#'           FieldDelimiter = "string",
#'           QuoteCharacter = "string",
#'           AllowQuotedRecordDelimiter = TRUE|FALSE
#'         ),
#'         CompressionType = "NONE"|"GZIP"|"BZIP2",
#'         JSON = list(
#'           Type = "DOCUMENT"|"LINES"
#'         ),
#'         Parquet = list()
#'       ),
#'       ExpressionType = "SQL",
#'       Expression = "string",
#'       OutputSerialization = list(
#'         CSV = list(
#'           QuoteFields = "ALWAYS"|"ASNEEDED",
#'           QuoteEscapeCharacter = "string",
#'           RecordDelimiter = "string",
#'           FieldDelimiter = "string",
#'           QuoteCharacter = "string"
#'         ),
#'         JSON = list(
#'           RecordDelimiter = "string"
#'         )
#'       )
#'     ),
#'     OutputLocation = list(
#'       S3 = list(
#'         BucketName = "string",
#'         Prefix = "string",
#'         Encryption = list(
#'           EncryptionType = "AES256"|"aws:fsx"|"aws:kms"|"aws:kms:dsse",
#'           KMSKeyId = "string",
#'           KMSContext = "string"
#'         ),
#'         CannedACL = "private"|"public-read"|"public-read-write"|"authenticated-read"|"aws-exec-read"|"bucket-owner-read"|"bucket-owner-full-control",
#'         AccessControlList = list(
#'           list(
#'             Grantee = list(
#'               DisplayName = "string",
#'               EmailAddress = "string",
#'               ID = "string",
#'               Type = "CanonicalUser"|"AmazonCustomerByEmail"|"Group",
#'               URI = "string"
#'             ),
#'             Permission = "FULL_CONTROL"|"WRITE"|"WRITE_ACP"|"READ"|"READ_ACP"
#'           )
#'         ),
#'         Tagging = list(
#'           TagSet = list(
#'             list(
#'               Key = "string",
#'               Value = "string"
#'             )
#'           )
#'         ),
#'         UserMetadata = list(
#'           list(
#'             Name = "string",
#'             Value = "string"
#'           )
#'         ),
#'         StorageClass = "STANDARD"|"REDUCED_REDUNDANCY"|"STANDARD_IA"|"ONEZONE_IA"|"INTELLIGENT_TIERING"|"GLACIER"|"DEEP_ARCHIVE"|"OUTPOSTS"|"GLACIER_IR"|"SNOW"|"EXPRESS_ONEZONE"|"FSX_OPENZFS"|"FSX_ONTAP"
#'       )
#'     )
#'   ),
#'   RequestPayer = "requester",
#'   ChecksumAlgorithm = "CRC32"|"CRC32C"|"SHA1"|"SHA256"|"CRC64NVME",
#'   ExpectedBucketOwner = "string"
#' )
#' ```
#'
#' @examples
#' \dontrun{
#' # The following example restores for one day an archived copy of an object
#' # back into Amazon S3 bucket.
#' svc$restore_object(
#'   Bucket = "examplebucket",
#'   Key = "archivedobjectkey",
#'   RestoreRequest = list(
#'     Days = 1L,
#'     GlacierJobParameters = list(
#'       Tier = "Expedited"
#'     )
#'   )
#' )
#' }
#'
#' @keywords internal
#'
#' @rdname s3_restore_object
#'
#' @aliases s3_restore_object
s3_restore_object <- function(Bucket, Key, VersionId = NULL, RestoreRequest = NULL, RequestPayer = NULL, ChecksumAlgorithm = NULL, ExpectedBucketOwner = NULL) {
  op <- new_operation(
    name = "RestoreObject",
    http_method = "POST",
    http_path = "/{Bucket}/{Key+}?restore",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .s3$restore_object_input(Bucket = Bucket, Key = Key, VersionId = VersionId, RestoreRequest = RestoreRequest, RequestPayer = RequestPayer, ChecksumAlgorithm = ChecksumAlgorithm, ExpectedBucketOwner = ExpectedBucketOwner)
  output <- .s3$restore_object_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$restore_object <- s3_restore_object

#' This operation is not supported for directory buckets
#'
#' @description
#' This operation is not supported for directory buckets.
#' 
#' This action filters the contents of an Amazon S3 object based on a
#' simple structured query language (SQL) statement. In the request, along
#' with the SQL expression, you must also specify a data serialization
#' format (JSON, CSV, or Apache Parquet) of the object. Amazon S3 uses this
#' format to parse object data into records, and returns only records that
#' match the specified SQL expression. You must also specify the data
#' serialization format for the response.
#' 
#' This functionality is not supported for Amazon S3 on Outposts.
#' 
#' For more information about Amazon S3 Select, see [Selecting Content from
#' Objects](https://docs.aws.amazon.com/AmazonS3/latest/userguide/selecting-content-from-objects.html)
#' and [SELECT
#' Command](https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-select-sql-reference-select.html)
#' in the *Amazon S3 User Guide*.
#' 
#' ### Permissions
#' 
#' You must have the `s3:GetObject` permission for this operation. Amazon
#' S3 Select does not support anonymous access. For more information about
#' permissions, see [Specifying Permissions in a
#' Policy](https://docs.aws.amazon.com/AmazonS3/latest/userguide/security_iam_service-with-iam.html#security_iam_service-with-iam-id-based-policies-actions)
#' in the *Amazon S3 User Guide*.
#' 
#' ### Object Data Formats
#' 
#' You can use Amazon S3 Select to query objects that have the following
#' format properties:
#' 
#' -   *CSV, JSON, and Parquet* - Objects must be in CSV, JSON, or Parquet
#'     format.
#' 
#' -   *UTF-8* - UTF-8 is the only encoding type Amazon S3 Select supports.
#' 
#' -   *GZIP or BZIP2* - CSV and JSON files can be compressed using GZIP or
#'     BZIP2. GZIP and BZIP2 are the only compression formats that Amazon
#'     S3 Select supports for CSV and JSON files. Amazon S3 Select supports
#'     columnar compression for Parquet using GZIP or Snappy. Amazon S3
#'     Select does not support whole-object compression for Parquet
#'     objects.
#' 
#' -   *Server-side encryption* - Amazon S3 Select supports querying
#'     objects that are protected with server-side encryption.
#' 
#'     For objects that are encrypted with customer-provided encryption
#'     keys (SSE-C), you must use HTTPS, and you must use the headers that
#'     are documented in the [`get_object`][s3_get_object]. For more
#'     information about SSE-C, see [Server-Side Encryption (Using
#'     Customer-Provided Encryption
#'     Keys)](https://docs.aws.amazon.com/AmazonS3/latest/userguide/ServerSideEncryptionCustomerKeys.html)
#'     in the *Amazon S3 User Guide*.
#' 
#'     For objects that are encrypted with Amazon S3 managed keys (SSE-S3)
#'     and Amazon Web Services KMS keys (SSE-KMS), server-side encryption
#'     is handled transparently, so you don't need to specify anything. For
#'     more information about server-side encryption, including SSE-S3 and
#'     SSE-KMS, see [Protecting Data Using Server-Side
#'     Encryption](https://docs.aws.amazon.com/AmazonS3/latest/userguide/serv-side-encryption.html)
#'     in the *Amazon S3 User Guide*.
#' 
#' ### Working with the Response Body
#' 
#' Given the response size is unknown, Amazon S3 Select streams the
#' response as a series of messages and includes a `Transfer-Encoding`
#' header with `chunked` as its value in the response. For more
#' information, see [Appendix: SelectObjectContent
#' Response](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTSelectObjectAppendix.html).
#' 
#' ### GetObject Support
#' 
#' The [`select_object_content`][s3_select_object_content] action does not
#' support the following [`get_object`][s3_get_object] functionality. For
#' more information, see [`get_object`][s3_get_object].
#' 
#' -   `Range`: Although you can specify a scan range for an Amazon S3
#'     Select request (see [SelectObjectContentRequest -
#'     ScanRange](https://docs.aws.amazon.com/AmazonS3/latest/API/API_SelectObjectContent.html#AmazonS3-SelectObjectContent-request-ScanRange)
#'     in the request parameters), you cannot specify the range of bytes of
#'     an object to return.
#' 
#' -   The `GLACIER`, `DEEP_ARCHIVE`, and `REDUCED_REDUNDANCY` storage
#'     classes, or the `ARCHIVE_ACCESS` and `DEEP_ARCHIVE_ACCESS` access
#'     tiers of the `INTELLIGENT_TIERING` storage class: You cannot query
#'     objects in the `GLACIER`, `DEEP_ARCHIVE`, or `REDUCED_REDUNDANCY`
#'     storage classes, nor objects in the `ARCHIVE_ACCESS` or
#'     `DEEP_ARCHIVE_ACCESS` access tiers of the `INTELLIGENT_TIERING`
#'     storage class. For more information about storage classes, see
#'     [Using Amazon S3 storage
#'     classes](https://docs.aws.amazon.com/AmazonS3/latest/userguide/storage-class-intro.html)
#'     in the *Amazon S3 User Guide*.
#' 
#' ### Special Errors
#' 
#' For a list of special errors for this operation, see [List of SELECT
#' Object Content Error
#' Codes](https://docs.aws.amazon.com/AmazonS3/latest/API/ErrorResponses.html#SelectObjectContentErrorCodeList)
#' 
#' The following operations are related to
#' [`select_object_content`][s3_select_object_content]:
#' 
#' -   [`get_object`][s3_get_object]
#' 
#' -   [`get_bucket_lifecycle_configuration`][s3_get_bucket_lifecycle_configuration]
#' 
#' -   [`put_bucket_lifecycle_configuration`][s3_put_bucket_lifecycle_configuration]
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#'
#' @usage
#' s3_select_object_content(Bucket, Key, SSECustomerAlgorithm,
#'   SSECustomerKey, SSECustomerKeyMD5, Expression, ExpressionType,
#'   RequestProgress, InputSerialization, OutputSerialization, ScanRange,
#'   ExpectedBucketOwner)
#'
#' @param Bucket &#91;required&#93; The S3 bucket.
#' @param Key &#91;required&#93; The object key.
#' @param SSECustomerAlgorithm The server-side encryption (SSE) algorithm used to encrypt the object.
#' This parameter is needed only when the object was created using a
#' checksum algorithm. For more information, see [Protecting data using
#' SSE-C
#' keys](https://docs.aws.amazon.com/AmazonS3/latest/userguide/ServerSideEncryptionCustomerKeys.html)
#' in the *Amazon S3 User Guide*.
#' @param SSECustomerKey The server-side encryption (SSE) customer managed key. This parameter is
#' needed only when the object was created using a checksum algorithm. For
#' more information, see [Protecting data using SSE-C
#' keys](https://docs.aws.amazon.com/AmazonS3/latest/userguide/ServerSideEncryptionCustomerKeys.html)
#' in the *Amazon S3 User Guide*.
#' @param SSECustomerKeyMD5 The MD5 server-side encryption (SSE) customer managed key. This
#' parameter is needed only when the object was created using a checksum
#' algorithm. For more information, see [Protecting data using SSE-C
#' keys](https://docs.aws.amazon.com/AmazonS3/latest/userguide/ServerSideEncryptionCustomerKeys.html)
#' in the *Amazon S3 User Guide*.
#' @param Expression &#91;required&#93; The expression that is used to query the object.
#' @param ExpressionType &#91;required&#93; The type of the provided expression (for example, SQL).
#' @param RequestProgress Specifies if periodic request progress information should be enabled.
#' @param InputSerialization &#91;required&#93; Describes the format of the data in the object that is being queried.
#' @param OutputSerialization &#91;required&#93; Describes the format of the data that you want Amazon S3 to return in
#' response.
#' @param ScanRange Specifies the byte range of the object to get the records from. A record
#' is processed when its first byte is contained by the range. This
#' parameter is optional, but when specified, it must not be empty. See RFC
#' 2616, Section 14.35.1 about how to specify the start and end of the
#' range.
#' 
#' `ScanRange`may be used in the following ways:
#' 
#' -   `<scanrange><start>50</start><end>100</end></scanrange>` - process
#'     only the records starting between the bytes 50 and 100 (inclusive,
#'     counting from zero)
#' 
#' -   `<scanrange><start>50</start></scanrange>` - process only the
#'     records starting after the byte 50
#' 
#' -   `<scanrange><end>50</end></scanrange>` - process only the records
#'     within the last 50 bytes of the file.
#' @param ExpectedBucketOwner The account ID of the expected bucket owner. If the account ID that you
#' provide does not match the actual owner of the bucket, the request fails
#' with the HTTP status code `403 Forbidden` (access denied).
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   Payload = list(
#'     Records = list(
#'       Payload = raw
#'     ),
#'     Stats = list(
#'       Details = list(
#'         BytesScanned = 123,
#'         BytesProcessed = 123,
#'         BytesReturned = 123
#'       )
#'     ),
#'     Progress = list(
#'       Details = list(
#'         BytesScanned = 123,
#'         BytesProcessed = 123,
#'         BytesReturned = 123
#'       )
#'     ),
#'     Cont = list(),
#'     End = list()
#'   )
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$select_object_content(
#'   Bucket = "string",
#'   Key = "string",
#'   SSECustomerAlgorithm = "string",
#'   SSECustomerKey = "string",
#'   SSECustomerKeyMD5 = "string",
#'   Expression = "string",
#'   ExpressionType = "SQL",
#'   RequestProgress = list(
#'     Enabled = TRUE|FALSE
#'   ),
#'   InputSerialization = list(
#'     CSV = list(
#'       FileHeaderInfo = "USE"|"IGNORE"|"NONE",
#'       Comments = "string",
#'       QuoteEscapeCharacter = "string",
#'       RecordDelimiter = "string",
#'       FieldDelimiter = "string",
#'       QuoteCharacter = "string",
#'       AllowQuotedRecordDelimiter = TRUE|FALSE
#'     ),
#'     CompressionType = "NONE"|"GZIP"|"BZIP2",
#'     JSON = list(
#'       Type = "DOCUMENT"|"LINES"
#'     ),
#'     Parquet = list()
#'   ),
#'   OutputSerialization = list(
#'     CSV = list(
#'       QuoteFields = "ALWAYS"|"ASNEEDED",
#'       QuoteEscapeCharacter = "string",
#'       RecordDelimiter = "string",
#'       FieldDelimiter = "string",
#'       QuoteCharacter = "string"
#'     ),
#'     JSON = list(
#'       RecordDelimiter = "string"
#'     )
#'   ),
#'   ScanRange = list(
#'     Start = 123,
#'     End = 123
#'   ),
#'   ExpectedBucketOwner = "string"
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname s3_select_object_content
#'
#' @aliases s3_select_object_content
s3_select_object_content <- function(Bucket, Key, SSECustomerAlgorithm = NULL, SSECustomerKey = NULL, SSECustomerKeyMD5 = NULL, Expression, ExpressionType, RequestProgress = NULL, InputSerialization, OutputSerialization, ScanRange = NULL, ExpectedBucketOwner = NULL) {
  op <- new_operation(
    name = "SelectObjectContent",
    http_method = "POST",
    http_path = "/{Bucket}/{Key+}?select&select-type=2",
    host_prefix = "",
    paginator = list(),
    stream_api = TRUE
  )
  input <- .s3$select_object_content_input(Bucket = Bucket, Key = Key, SSECustomerAlgorithm = SSECustomerAlgorithm, SSECustomerKey = SSECustomerKey, SSECustomerKeyMD5 = SSECustomerKeyMD5, Expression = Expression, ExpressionType = ExpressionType, RequestProgress = RequestProgress, InputSerialization = InputSerialization, OutputSerialization = OutputSerialization, ScanRange = ScanRange, ExpectedBucketOwner = ExpectedBucketOwner)
  output <- .s3$select_object_content_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$select_object_content <- s3_select_object_content

#' Enables or disables a live inventory table for an S3 Metadata
#' configuration on a general purpose bucket
#'
#' @description
#' Enables or disables a live inventory table for an S3 Metadata
#' configuration on a general purpose bucket. For more information, see
#' [Accelerating data discovery with S3
#' Metadata](https://docs.aws.amazon.com/AmazonS3/latest/userguide/metadata-tables-overview.html)
#' in the *Amazon S3 User Guide*.
#' 
#' ### Permissions
#' 
#' To use this operation, you must have the following permissions. For more
#' information, see [Setting up permissions for configuring metadata
#' tables](https://docs.aws.amazon.com/AmazonS3/latest/userguide/metadata-tables-permissions.html)
#' in the *Amazon S3 User Guide*.
#' 
#' If you want to encrypt your inventory table with server-side encryption
#' with Key Management Service (KMS) keys (SSE-KMS), you need additional
#' permissions in your KMS key policy. For more information, see [Setting
#' up permissions for configuring metadata
#' tables](https://docs.aws.amazon.com/AmazonS3/latest/userguide/metadata-tables-permissions.html)
#' in the *Amazon S3 User Guide*.
#' 
#' -   `s3:UpdateBucketMetadataInventoryTableConfiguration`
#' 
#' -   `s3tables:CreateTableBucket`
#' 
#' -   `s3tables:CreateNamespace`
#' 
#' -   `s3tables:GetTable`
#' 
#' -   `s3tables:CreateTable`
#' 
#' -   `s3tables:PutTablePolicy`
#' 
#' -   `s3tables:PutTableEncryption`
#' 
#' -   `kms:DescribeKey`
#' 
#' The following operations are related to
#' [`update_bucket_metadata_inventory_table_configuration`][s3_update_bucket_metadata_inventory_table_configuration]:
#' 
#' -   [`create_bucket_metadata_configuration`][s3_create_bucket_metadata_configuration]
#' 
#' -   [`delete_bucket_metadata_configuration`][s3_delete_bucket_metadata_configuration]
#' 
#' -   [`get_bucket_metadata_configuration`][s3_get_bucket_metadata_configuration]
#' 
#' -   [`update_bucket_metadata_journal_table_configuration`][s3_update_bucket_metadata_journal_table_configuration]
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#'
#' @usage
#' s3_update_bucket_metadata_inventory_table_configuration(Bucket,
#'   ContentMD5, ChecksumAlgorithm, InventoryTableConfiguration,
#'   ExpectedBucketOwner)
#'
#' @param Bucket &#91;required&#93; The general purpose bucket that corresponds to the metadata
#' configuration that you want to enable or disable an inventory table for.
#' @param ContentMD5 The `Content-MD5` header for the inventory table configuration.
#' @param ChecksumAlgorithm The checksum algorithm to use with your inventory table configuration.
#' @param InventoryTableConfiguration &#91;required&#93; The contents of your inventory table configuration.
#' @param ExpectedBucketOwner The expected owner of the general purpose bucket that corresponds to the
#' metadata table configuration that you want to enable or disable an
#' inventory table for.
#'
#' @return
#' An empty list.
#'
#' @section Request syntax:
#' ```
#' svc$update_bucket_metadata_inventory_table_configuration(
#'   Bucket = "string",
#'   ContentMD5 = "string",
#'   ChecksumAlgorithm = "CRC32"|"CRC32C"|"SHA1"|"SHA256"|"CRC64NVME",
#'   InventoryTableConfiguration = list(
#'     ConfigurationState = "ENABLED"|"DISABLED",
#'     EncryptionConfiguration = list(
#'       SseAlgorithm = "aws:kms"|"AES256",
#'       KmsKeyArn = "string"
#'     )
#'   ),
#'   ExpectedBucketOwner = "string"
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname s3_update_bucket_metadata_inventory_table_configuration
#'
#' @aliases s3_update_bucket_metadata_inventory_table_configuration
s3_update_bucket_metadata_inventory_table_configuration <- function(Bucket, ContentMD5 = NULL, ChecksumAlgorithm = NULL, InventoryTableConfiguration, ExpectedBucketOwner = NULL) {
  op <- new_operation(
    name = "UpdateBucketMetadataInventoryTableConfiguration",
    http_method = "PUT",
    http_path = "/{Bucket}?metadataInventoryTable",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .s3$update_bucket_metadata_inventory_table_configuration_input(Bucket = Bucket, ContentMD5 = ContentMD5, ChecksumAlgorithm = ChecksumAlgorithm, InventoryTableConfiguration = InventoryTableConfiguration, ExpectedBucketOwner = ExpectedBucketOwner)
  output <- .s3$update_bucket_metadata_inventory_table_configuration_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$update_bucket_metadata_inventory_table_configuration <- s3_update_bucket_metadata_inventory_table_configuration

#' Enables or disables journal table record expiration for an S3 Metadata
#' configuration on a general purpose bucket
#'
#' @description
#' Enables or disables journal table record expiration for an S3 Metadata
#' configuration on a general purpose bucket. For more information, see
#' [Accelerating data discovery with S3
#' Metadata](https://docs.aws.amazon.com/AmazonS3/latest/userguide/metadata-tables-overview.html)
#' in the *Amazon S3 User Guide*.
#' 
#' ### Permissions
#' 
#' To use this operation, you must have the
#' `s3:UpdateBucketMetadataJournalTableConfiguration` permission. For more
#' information, see [Setting up permissions for configuring metadata
#' tables](https://docs.aws.amazon.com/AmazonS3/latest/userguide/metadata-tables-permissions.html)
#' in the *Amazon S3 User Guide*.
#' 
#' The following operations are related to
#' [`update_bucket_metadata_journal_table_configuration`][s3_update_bucket_metadata_journal_table_configuration]:
#' 
#' -   [`create_bucket_metadata_configuration`][s3_create_bucket_metadata_configuration]
#' 
#' -   [`delete_bucket_metadata_configuration`][s3_delete_bucket_metadata_configuration]
#' 
#' -   [`get_bucket_metadata_configuration`][s3_get_bucket_metadata_configuration]
#' 
#' -   [`update_bucket_metadata_inventory_table_configuration`][s3_update_bucket_metadata_inventory_table_configuration]
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#'
#' @usage
#' s3_update_bucket_metadata_journal_table_configuration(Bucket,
#'   ContentMD5, ChecksumAlgorithm, JournalTableConfiguration,
#'   ExpectedBucketOwner)
#'
#' @param Bucket &#91;required&#93; The general purpose bucket that corresponds to the metadata
#' configuration that you want to enable or disable journal table record
#' expiration for.
#' @param ContentMD5 The `Content-MD5` header for the journal table configuration.
#' @param ChecksumAlgorithm The checksum algorithm to use with your journal table configuration.
#' @param JournalTableConfiguration &#91;required&#93; The contents of your journal table configuration.
#' @param ExpectedBucketOwner The expected owner of the general purpose bucket that corresponds to the
#' metadata table configuration that you want to enable or disable journal
#' table record expiration for.
#'
#' @return
#' An empty list.
#'
#' @section Request syntax:
#' ```
#' svc$update_bucket_metadata_journal_table_configuration(
#'   Bucket = "string",
#'   ContentMD5 = "string",
#'   ChecksumAlgorithm = "CRC32"|"CRC32C"|"SHA1"|"SHA256"|"CRC64NVME",
#'   JournalTableConfiguration = list(
#'     RecordExpiration = list(
#'       Expiration = "ENABLED"|"DISABLED",
#'       Days = 123
#'     )
#'   ),
#'   ExpectedBucketOwner = "string"
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname s3_update_bucket_metadata_journal_table_configuration
#'
#' @aliases s3_update_bucket_metadata_journal_table_configuration
s3_update_bucket_metadata_journal_table_configuration <- function(Bucket, ContentMD5 = NULL, ChecksumAlgorithm = NULL, JournalTableConfiguration, ExpectedBucketOwner = NULL) {
  op <- new_operation(
    name = "UpdateBucketMetadataJournalTableConfiguration",
    http_method = "PUT",
    http_path = "/{Bucket}?metadataJournalTable",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .s3$update_bucket_metadata_journal_table_configuration_input(Bucket = Bucket, ContentMD5 = ContentMD5, ChecksumAlgorithm = ChecksumAlgorithm, JournalTableConfiguration = JournalTableConfiguration, ExpectedBucketOwner = ExpectedBucketOwner)
  output <- .s3$update_bucket_metadata_journal_table_configuration_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$update_bucket_metadata_journal_table_configuration <- s3_update_bucket_metadata_journal_table_configuration

#' Uploads a part in a multipart upload
#'
#' @description
#' Uploads a part in a multipart upload.
#' 
#' In this operation, you provide new data as a part of an object in your
#' request. However, you have an option to specify your existing Amazon S3
#' object as a data source for the part you are uploading. To upload a part
#' from an existing object, you use the
#' [`upload_part_copy`][s3_upload_part_copy] operation.
#' 
#' You must initiate a multipart upload (see
#' [`create_multipart_upload`][s3_create_multipart_upload]) before you can
#' upload any part. In response to your initiate request, Amazon S3 returns
#' an upload ID, a unique identifier that you must include in your upload
#' part request.
#' 
#' Part numbers can be any number from 1 to 10,000, inclusive. A part
#' number uniquely identifies a part and also defines its position within
#' the object being created. If you upload a new part using the same part
#' number that was used with a previous part, the previously uploaded part
#' is overwritten.
#' 
#' For information about maximum and minimum part sizes and other multipart
#' upload specifications, see [Multipart upload
#' limits](https://docs.aws.amazon.com/AmazonS3/latest/userguide/qfacts.html)
#' in the *Amazon S3 User Guide*.
#' 
#' After you initiate multipart upload and upload one or more parts, you
#' must either complete or abort multipart upload in order to stop getting
#' charged for storage of the uploaded parts. Only after you either
#' complete or abort multipart upload, Amazon S3 frees up the parts storage
#' and stops charging you for the parts storage.
#' 
#' For more information on multipart uploads, go to [Multipart Upload
#' Overview](https://docs.aws.amazon.com/AmazonS3/latest/userguide/mpuoverview.html)
#' in the *Amazon S3 User Guide* .
#' 
#' **Directory buckets** - For directory buckets, you must make requests
#' for this API operation to the Zonal endpoint. These endpoints support
#' virtual-hosted-style requests in the format
#' `https://amzn-s3-demo-bucket.s3express-zone-id.region-code.amazonaws.com/key-name `.
#' Path-style requests are not supported. For more information about
#' endpoints in Availability Zones, see [Regional and Zonal endpoints for
#' directory buckets in Availability
#' Zones](https://docs.aws.amazon.com/AmazonS3/latest/userguide/endpoint-directory-buckets-AZ.html)
#' in the *Amazon S3 User Guide*. For more information about endpoints in
#' Local Zones, see [Concepts for directory buckets in Local
#' Zones](https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-lzs-for-directory-buckets.html)
#' in the *Amazon S3 User Guide*.
#' 
#' ### Permissions
#' 
#' -   **General purpose bucket permissions** - To perform a multipart
#'     upload with encryption using an Key Management Service key, the
#'     requester must have permission to the `kms:Decrypt` and
#'     `kms:GenerateDataKey` actions on the key. The requester must also
#'     have permissions for the `kms:GenerateDataKey` action for the
#'     [`create_multipart_upload`][s3_create_multipart_upload] API. Then,
#'     the requester needs permissions for the `kms:Decrypt` action on the
#'     [`upload_part`][s3_upload_part] and
#'     [`upload_part_copy`][s3_upload_part_copy] APIs.
#' 
#'     These permissions are required because Amazon S3 must decrypt and
#'     read data from the encrypted file parts before it completes the
#'     multipart upload. For more information about KMS permissions, see
#'     [Protecting data using server-side encryption with
#'     KMS](https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingKMSEncryption.html)
#'     in the *Amazon S3 User Guide*. For information about the permissions
#'     required to use the multipart upload API, see [Multipart upload and
#'     permissions](https://docs.aws.amazon.com/AmazonS3/latest/userguide/mpuoverview.html)
#'     and [Multipart upload API and
#'     permissions](https://docs.aws.amazon.com/AmazonS3/latest/userguide/mpuoverview.html#mpuAndPermissions)
#'     in the *Amazon S3 User Guide*.
#' 
#' -   **Directory bucket permissions** - To grant access to this API
#'     operation on a directory bucket, we recommend that you use the
#'     [`create_session`](https://docs.aws.amazon.com/AmazonS3/latest/API/API_CreateSession.html)
#'     API operation for session-based authorization. Specifically, you
#'     grant the `s3express:CreateSession` permission to the directory
#'     bucket in a bucket policy or an IAM identity-based policy. Then, you
#'     make the [`create_session`][s3_create_session] API call on the
#'     bucket to obtain a session token. With the session token in your
#'     request header, you can make API requests to this operation. After
#'     the session token expires, you make another
#'     [`create_session`][s3_create_session] API call to generate a new
#'     session token for use. Amazon Web Services CLI or SDKs create
#'     session and refresh the session token automatically to avoid service
#'     interruptions when a session expires. For more information about
#'     authorization, see
#'     [`create_session`](https://docs.aws.amazon.com/AmazonS3/latest/API/API_CreateSession.html)
#'     .
#' 
#'     If the object is encrypted with SSE-KMS, you must also have the
#'     `kms:GenerateDataKey` and `kms:Decrypt` permissions in IAM
#'     identity-based policies and KMS key policies for the KMS key.
#' 
#' ### Data integrity
#' 
#' **General purpose bucket** - To ensure that data is not corrupted
#' traversing the network, specify the `Content-MD5` header in the upload
#' part request. Amazon S3 checks the part data against the provided MD5
#' value. If they do not match, Amazon S3 returns an error. If the upload
#' request is signed with Signature Version 4, then Amazon Web Services S3
#' uses the `x-amz-content-sha256` header as a checksum instead of
#' `Content-MD5`. For more information see [Authenticating Requests: Using
#' the Authorization Header (Amazon Web Services Signature Version
#' 4)](https://docs.aws.amazon.com/AmazonS3/latest/API/sigv4-auth-using-authorization-header.html).
#' 
#' **Directory buckets** - MD5 is not supported by directory buckets. You
#' can use checksum algorithms to check object integrity.
#' 
#' ### Encryption
#' 
#' -   **General purpose bucket** - Server-side encryption is for data
#'     encryption at rest. Amazon S3 encrypts your data as it writes it to
#'     disks in its data centers and decrypts it when you access it. You
#'     have mutually exclusive options to protect data using server-side
#'     encryption in Amazon S3, depending on how you choose to manage the
#'     encryption keys. Specifically, the encryption key options are Amazon
#'     S3 managed keys (SSE-S3), Amazon Web Services KMS keys (SSE-KMS),
#'     and Customer-Provided Keys (SSE-C). Amazon S3 encrypts data with
#'     server-side encryption using Amazon S3 managed keys (SSE-S3) by
#'     default. You can optionally tell Amazon S3 to encrypt data at rest
#'     using server-side encryption with other key options. The option you
#'     use depends on whether you want to use KMS keys (SSE-KMS) or provide
#'     your own encryption key (SSE-C).
#' 
#'     Server-side encryption is supported by the S3 Multipart Upload
#'     operations. Unless you are using a customer-provided encryption key
#'     (SSE-C), you don't need to specify the encryption parameters in each
#'     UploadPart request. Instead, you only need to specify the
#'     server-side encryption parameters in the initial Initiate Multipart
#'     request. For more information, see
#'     [`create_multipart_upload`][s3_create_multipart_upload].
#' 
#'     If you have server-side encryption with customer-provided keys
#'     (SSE-C) blocked for your general purpose bucket, you will get an
#'     HTTP 403 Access Denied error when you specify the SSE-C request
#'     headers while writing new data to your bucket. For more information,
#'     see [Blocking or unblocking SSE-C for a general purpose
#'     bucket](https://docs.aws.amazon.com/AmazonS3/latest/userguide/blocking-unblocking-s3-c-encryption-gpb.html).
#' 
#'     If you request server-side encryption using a customer-provided
#'     encryption key (SSE-C) in your initiate multipart upload request,
#'     you must provide identical encryption information in each part
#'     upload using the following request headers.
#' 
#'     -   x-amz-server-side-encryption-customer-algorithm
#' 
#'     -   x-amz-server-side-encryption-customer-key
#' 
#'     -   x-amz-server-side-encryption-customer-key-MD5
#' 
#'     For more information, see [Using Server-Side
#'     Encryption](https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingServerSideEncryption.html)
#'     in the *Amazon S3 User Guide*.
#' 
#' -   **Directory buckets** - For directory buckets, there are only two
#'     supported options for server-side encryption: server-side encryption
#'     with Amazon S3 managed keys (SSE-S3) (`AES256`) and server-side
#'     encryption with KMS keys (SSE-KMS) (`aws:kms`).
#' 
#' ### Special errors
#' 
#' -   Error Code: `NoSuchUpload`
#' 
#'     -   Description: The specified multipart upload does not exist. The
#'         upload ID might be invalid, or the multipart upload might have
#'         been aborted or completed.
#' 
#'     -   HTTP Status Code: 404 Not Found
#' 
#'     -   SOAP Fault Code Prefix: Client
#' 
#' ### HTTP Host header syntax
#' 
#' **Directory buckets** - The HTTP Host header syntax is
#' ` Bucket-name.s3express-zone-id.region-code.amazonaws.com`.
#' 
#' The following operations are related to [`upload_part`][s3_upload_part]:
#' 
#' -   [`create_multipart_upload`][s3_create_multipart_upload]
#' 
#' -   [`complete_multipart_upload`][s3_complete_multipart_upload]
#' 
#' -   [`abort_multipart_upload`][s3_abort_multipart_upload]
#' 
#' -   [`list_parts`][s3_list_parts]
#' 
#' -   [`list_multipart_uploads`][s3_list_multipart_uploads]
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#'
#' @usage
#' s3_upload_part(Body, Bucket, ContentLength, ContentMD5,
#'   ChecksumAlgorithm, ChecksumCRC32, ChecksumCRC32C, ChecksumCRC64NVME,
#'   ChecksumSHA1, ChecksumSHA256, Key, PartNumber, UploadId,
#'   SSECustomerAlgorithm, SSECustomerKey, SSECustomerKeyMD5, RequestPayer,
#'   ExpectedBucketOwner)
#'
#' @param Body Object data.
#' @param Bucket &#91;required&#93; The name of the bucket to which the multipart upload was initiated.
#' 
#' **Directory buckets** - When you use this operation with a directory
#' bucket, you must use virtual-hosted-style requests in the format
#' ` Bucket-name.s3express-zone-id.region-code.amazonaws.com`. Path-style
#' requests are not supported. Directory bucket names must be unique in the
#' chosen Zone (Availability Zone or Local Zone). Bucket names must follow
#' the format ` bucket-base-name--zone-id--x-s3` (for example,
#' ` amzn-s3-demo-bucket--usw2-az1--x-s3`). For information about bucket
#' naming restrictions, see [Directory bucket naming
#' rules](https://docs.aws.amazon.com/AmazonS3/latest/userguide/directory-bucket-naming-rules.html)
#' in the *Amazon S3 User Guide*.
#' 
#' **Access points** - When you use this action with an access point for
#' general purpose buckets, you must provide the alias of the access point
#' in place of the bucket name or specify the access point ARN. When you
#' use this action with an access point for directory buckets, you must
#' provide the access point name in place of the bucket name. When using
#' the access point ARN, you must direct requests to the access point
#' hostname. The access point hostname takes the form
#' *AccessPointName*-*AccountId*.s3-accesspoint.*Region*.amazonaws.com.
#' When using this action with an access point through the Amazon Web
#' Services SDKs, you provide the access point ARN in place of the bucket
#' name. For more information about access point ARNs, see [Using access
#' points](https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-access-points.html)
#' in the *Amazon S3 User Guide*.
#' 
#' Object Lambda access points are not supported by directory buckets.
#' 
#' **S3 on Outposts** - When you use this action with S3 on Outposts, you
#' must direct requests to the S3 on Outposts hostname. The S3 on Outposts
#' hostname takes the form
#' ` AccessPointName-AccountId.outpostID.s3-outposts.Region.amazonaws.com`.
#' When you use this action with S3 on Outposts, the destination bucket
#' must be the Outposts access point ARN or the access point alias. For
#' more information about S3 on Outposts, see [What is S3 on
#' Outposts?](https://docs.aws.amazon.com/AmazonS3/latest/s3-outposts/S3onOutposts.html)
#' in the *Amazon S3 User Guide*.
#' @param ContentLength Size of the body in bytes. This parameter is useful when the size of the
#' body cannot be determined automatically.
#' @param ContentMD5 The Base64 encoded 128-bit MD5 digest of the part data. This parameter
#' is auto-populated when using the command from the CLI. This parameter is
#' required if object lock parameters are specified.
#' 
#' This functionality is not supported for directory buckets.
#' @param ChecksumAlgorithm Indicates the algorithm used to create the checksum for the object when
#' you use the SDK. This header will not provide any additional
#' functionality if you don't use the SDK. When you send this header, there
#' must be a corresponding `x-amz-checksum` or `x-amz-trailer` header sent.
#' Otherwise, Amazon S3 fails the request with the HTTP status code
#' `400 Bad Request`. For more information, see [Checking object
#' integrity](https://docs.aws.amazon.com/AmazonS3/latest/userguide/checking-object-integrity.html)
#' in the *Amazon S3 User Guide*.
#' 
#' If you provide an individual checksum, Amazon S3 ignores any provided
#' `ChecksumAlgorithm` parameter.
#' 
#' This checksum algorithm must be the same for all parts and it match the
#' checksum value supplied in the
#' [`create_multipart_upload`][s3_create_multipart_upload] request.
#' @param ChecksumCRC32 This header can be used as a data integrity check to verify that the
#' data received is the same data that was originally sent. This header
#' specifies the Base64 encoded, 32-bit `CRC32` checksum of the object. For
#' more information, see [Checking object
#' integrity](https://docs.aws.amazon.com/AmazonS3/latest/userguide/checking-object-integrity.html)
#' in the *Amazon S3 User Guide*.
#' @param ChecksumCRC32C This header can be used as a data integrity check to verify that the
#' data received is the same data that was originally sent. This header
#' specifies the Base64 encoded, 32-bit `CRC32C` checksum of the object.
#' For more information, see [Checking object
#' integrity](https://docs.aws.amazon.com/AmazonS3/latest/userguide/checking-object-integrity.html)
#' in the *Amazon S3 User Guide*.
#' @param ChecksumCRC64NVME This header can be used as a data integrity check to verify that the
#' data received is the same data that was originally sent. This header
#' specifies the Base64 encoded, 64-bit `CRC64NVME` checksum of the part.
#' For more information, see [Checking object
#' integrity](https://docs.aws.amazon.com/AmazonS3/latest/userguide/checking-object-integrity.html)
#' in the *Amazon S3 User Guide*.
#' @param ChecksumSHA1 This header can be used as a data integrity check to verify that the
#' data received is the same data that was originally sent. This header
#' specifies the Base64 encoded, 160-bit `SHA1` digest of the object. For
#' more information, see [Checking object
#' integrity](https://docs.aws.amazon.com/AmazonS3/latest/userguide/checking-object-integrity.html)
#' in the *Amazon S3 User Guide*.
#' @param ChecksumSHA256 This header can be used as a data integrity check to verify that the
#' data received is the same data that was originally sent. This header
#' specifies the Base64 encoded, 256-bit `SHA256` digest of the object. For
#' more information, see [Checking object
#' integrity](https://docs.aws.amazon.com/AmazonS3/latest/userguide/checking-object-integrity.html)
#' in the *Amazon S3 User Guide*.
#' @param Key &#91;required&#93; Object key for which the multipart upload was initiated.
#' @param PartNumber &#91;required&#93; Part number of part being uploaded. This is a positive integer between 1
#' and 10,000.
#' @param UploadId &#91;required&#93; Upload ID identifying the multipart upload whose part is being uploaded.
#' @param SSECustomerAlgorithm Specifies the algorithm to use when encrypting the object (for example,
#' AES256).
#' 
#' This functionality is not supported for directory buckets.
#' @param SSECustomerKey Specifies the customer-provided encryption key for Amazon S3 to use in
#' encrypting data. This value is used to store the object and then it is
#' discarded; Amazon S3 does not store the encryption key. The key must be
#' appropriate for use with the algorithm specified in the
#' `x-amz-server-side-encryption-customer-algorithm header`. This must be
#' the same encryption key specified in the initiate multipart upload
#' request.
#' 
#' This functionality is not supported for directory buckets.
#' @param SSECustomerKeyMD5 Specifies the 128-bit MD5 digest of the encryption key according to RFC
#' 1321. Amazon S3 uses this header for a message integrity check to ensure
#' that the encryption key was transmitted without error.
#' 
#' This functionality is not supported for directory buckets.
#' @param RequestPayer 
#' @param ExpectedBucketOwner The account ID of the expected bucket owner. If the account ID that you
#' provide does not match the actual owner of the bucket, the request fails
#' with the HTTP status code `403 Forbidden` (access denied).
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   ServerSideEncryption = "AES256"|"aws:fsx"|"aws:kms"|"aws:kms:dsse",
#'   ETag = "string",
#'   ChecksumCRC32 = "string",
#'   ChecksumCRC32C = "string",
#'   ChecksumCRC64NVME = "string",
#'   ChecksumSHA1 = "string",
#'   ChecksumSHA256 = "string",
#'   SSECustomerAlgorithm = "string",
#'   SSECustomerKeyMD5 = "string",
#'   SSEKMSKeyId = "string",
#'   BucketKeyEnabled = TRUE|FALSE,
#'   RequestCharged = "requester"
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$upload_part(
#'   Body = raw,
#'   Bucket = "string",
#'   ContentLength = 123,
#'   ContentMD5 = "string",
#'   ChecksumAlgorithm = "CRC32"|"CRC32C"|"SHA1"|"SHA256"|"CRC64NVME",
#'   ChecksumCRC32 = "string",
#'   ChecksumCRC32C = "string",
#'   ChecksumCRC64NVME = "string",
#'   ChecksumSHA1 = "string",
#'   ChecksumSHA256 = "string",
#'   Key = "string",
#'   PartNumber = 123,
#'   UploadId = "string",
#'   SSECustomerAlgorithm = "string",
#'   SSECustomerKey = "string",
#'   SSECustomerKeyMD5 = "string",
#'   RequestPayer = "requester",
#'   ExpectedBucketOwner = "string"
#' )
#' ```
#'
#' @examples
#' \dontrun{
#' # The following example uploads part 1 of a multipart upload. The example
#' # specifies a file name for the part data. The Upload ID is same that is
#' # returned by the initiate multipart upload.
#' svc$upload_part(
#'   Body = "fileToUpload",
#'   Bucket = "examplebucket",
#'   Key = "examplelargeobject",
#'   PartNumber = "1",
#'   UploadId = "xadcOB_7YPBOJuoFiQ9cz4P3Pe6FIZwO4f7wN93uHsNBEw97pl5eNwzExg0LA..."
#' )
#' }
#'
#' @keywords internal
#'
#' @rdname s3_upload_part
#'
#' @aliases s3_upload_part
s3_upload_part <- function(Body = NULL, Bucket, ContentLength = NULL, ContentMD5 = NULL, ChecksumAlgorithm = NULL, ChecksumCRC32 = NULL, ChecksumCRC32C = NULL, ChecksumCRC64NVME = NULL, ChecksumSHA1 = NULL, ChecksumSHA256 = NULL, Key, PartNumber, UploadId, SSECustomerAlgorithm = NULL, SSECustomerKey = NULL, SSECustomerKeyMD5 = NULL, RequestPayer = NULL, ExpectedBucketOwner = NULL) {
  op <- new_operation(
    name = "UploadPart",
    http_method = "PUT",
    http_path = "/{Bucket}/{Key+}",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .s3$upload_part_input(Body = Body, Bucket = Bucket, ContentLength = ContentLength, ContentMD5 = ContentMD5, ChecksumAlgorithm = ChecksumAlgorithm, ChecksumCRC32 = ChecksumCRC32, ChecksumCRC32C = ChecksumCRC32C, ChecksumCRC64NVME = ChecksumCRC64NVME, ChecksumSHA1 = ChecksumSHA1, ChecksumSHA256 = ChecksumSHA256, Key = Key, PartNumber = PartNumber, UploadId = UploadId, SSECustomerAlgorithm = SSECustomerAlgorithm, SSECustomerKey = SSECustomerKey, SSECustomerKeyMD5 = SSECustomerKeyMD5, RequestPayer = RequestPayer, ExpectedBucketOwner = ExpectedBucketOwner)
  output <- .s3$upload_part_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$upload_part <- s3_upload_part

#' Uploads a part by copying data from an existing object as data source
#'
#' @description
#' Uploads a part by copying data from an existing object as data source.
#' To specify the data source, you add the request header
#' `x-amz-copy-source` in your request. To specify a byte range, you add
#' the request header `x-amz-copy-source-range` in your request.
#' 
#' For information about maximum and minimum part sizes and other multipart
#' upload specifications, see [Multipart upload
#' limits](https://docs.aws.amazon.com/AmazonS3/latest/userguide/qfacts.html)
#' in the *Amazon S3 User Guide*.
#' 
#' Instead of copying data from an existing object as part data, you might
#' use the [`upload_part`][s3_upload_part] action to upload new data as a
#' part of an object in your request.
#' 
#' You must initiate a multipart upload before you can upload any part. In
#' response to your initiate request, Amazon S3 returns the upload ID, a
#' unique identifier that you must include in your upload part request.
#' 
#' For conceptual information about multipart uploads, see [Uploading
#' Objects Using Multipart
#' Upload](https://docs.aws.amazon.com/AmazonS3/latest/userguide/mpuoverview.html)
#' in the *Amazon S3 User Guide*. For information about copying objects
#' using a single atomic action vs. a multipart upload, see [Operations on
#' Objects](https://docs.aws.amazon.com/AmazonS3/latest/userguide/uploading-downloading-objects.html)
#' in the *Amazon S3 User Guide*.
#' 
#' **Directory buckets** - For directory buckets, you must make requests
#' for this API operation to the Zonal endpoint. These endpoints support
#' virtual-hosted-style requests in the format
#' `https://amzn-s3-demo-bucket.s3express-zone-id.region-code.amazonaws.com/key-name `.
#' Path-style requests are not supported. For more information about
#' endpoints in Availability Zones, see [Regional and Zonal endpoints for
#' directory buckets in Availability
#' Zones](https://docs.aws.amazon.com/AmazonS3/latest/userguide/endpoint-directory-buckets-AZ.html)
#' in the *Amazon S3 User Guide*. For more information about endpoints in
#' Local Zones, see [Concepts for directory buckets in Local
#' Zones](https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-lzs-for-directory-buckets.html)
#' in the *Amazon S3 User Guide*.
#' 
#' ### Authentication and authorization
#' 
#' All [`upload_part_copy`][s3_upload_part_copy] requests must be
#' authenticated and signed by using IAM credentials (access key ID and
#' secret access key for the IAM identities). All headers with the `x-amz-`
#' prefix, including `x-amz-copy-source`, must be signed. For more
#' information, see [REST
#' Authentication](https://docs.aws.amazon.com/AmazonS3/latest/API/RESTAuthentication.html).
#' 
#' **Directory buckets** - You must use IAM credentials to authenticate and
#' authorize your access to the [`upload_part_copy`][s3_upload_part_copy]
#' API operation, instead of using the temporary security credentials
#' through the [`create_session`][s3_create_session] API operation.
#' 
#' Amazon Web Services CLI or SDKs handles authentication and authorization
#' on your behalf.
#' 
#' ### Permissions
#' 
#' You must have `READ` access to the source object and `WRITE` access to
#' the destination bucket.
#' 
#' -   **General purpose bucket permissions** - You must have the
#'     permissions in a policy based on the bucket types of your source
#'     bucket and destination bucket in an
#'     [`upload_part_copy`][s3_upload_part_copy] operation.
#' 
#'     -   If the source object is in a general purpose bucket, you must
#'         have the **`s3:GetObject`** permission to read the source object
#'         that is being copied.
#' 
#'     -   If the destination bucket is a general purpose bucket, you must
#'         have the **`s3:PutObject`** permission to write the object copy
#'         to the destination bucket.
#' 
#'     -   To perform a multipart upload with encryption using an Key
#'         Management Service key, the requester must have permission to
#'         the `kms:Decrypt` and `kms:GenerateDataKey` actions on the key.
#'         The requester must also have permissions for the
#'         `kms:GenerateDataKey` action for the
#'         [`create_multipart_upload`][s3_create_multipart_upload] API.
#'         Then, the requester needs permissions for the `kms:Decrypt`
#'         action on the [`upload_part`][s3_upload_part] and
#'         [`upload_part_copy`][s3_upload_part_copy] APIs. These
#'         permissions are required because Amazon S3 must decrypt and read
#'         data from the encrypted file parts before it completes the
#'         multipart upload. For more information about KMS permissions,
#'         see [Protecting data using server-side encryption with
#'         KMS](https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingKMSEncryption.html)
#'         in the *Amazon S3 User Guide*. For information about the
#'         permissions required to use the multipart upload API, see
#'         [Multipart upload and
#'         permissions](https://docs.aws.amazon.com/AmazonS3/latest/userguide/mpuoverview.html)
#'         and [Multipart upload API and
#'         permissions](https://docs.aws.amazon.com/AmazonS3/latest/userguide/mpuoverview.html#mpuAndPermissions)
#'         in the *Amazon S3 User Guide*.
#' 
#' -   **Directory bucket permissions** - You must have permissions in a
#'     bucket policy or an IAM identity-based policy based on the source
#'     and destination bucket types in an
#'     [`upload_part_copy`][s3_upload_part_copy] operation.
#' 
#'     -   If the source object that you want to copy is in a directory
#'         bucket, you must have the **`s3express:CreateSession`**
#'         permission in the `Action` element of a policy to read the
#'         object. By default, the session is in the `ReadWrite` mode. If
#'         you want to restrict the access, you can explicitly set the
#'         `s3express:SessionMode` condition key to `ReadOnly` on the copy
#'         source bucket.
#' 
#'     -   If the copy destination is a directory bucket, you must have the
#'         **`s3express:CreateSession`** permission in the `Action` element
#'         of a policy to write the object to the destination. The
#'         `s3express:SessionMode` condition key cannot be set to
#'         `ReadOnly` on the copy destination.
#' 
#'     If the object is encrypted with SSE-KMS, you must also have the
#'     `kms:GenerateDataKey` and `kms:Decrypt` permissions in IAM
#'     identity-based policies and KMS key policies for the KMS key.
#' 
#'     For example policies, see [Example bucket policies for S3 Express
#'     One
#'     Zone](https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-express-security-iam-example-bucket-policies.html)
#'     and [Amazon Web Services Identity and Access Management (IAM)
#'     identity-based policies for S3 Express One
#'     Zone](https://docs.aws.amazon.com/AmazonS3/latest/userguide/s3-express-security-iam-identity-policies.html)
#'     in the *Amazon S3 User Guide*.
#' 
#' ### Encryption
#' 
#' -   **General purpose buckets** - For information about using
#'     server-side encryption with customer-provided encryption keys with
#'     the [`upload_part_copy`][s3_upload_part_copy] operation, see
#'     [`copy_object`][s3_copy_object] and [`upload_part`][s3_upload_part].
#' 
#'     If you have server-side encryption with customer-provided keys
#'     (SSE-C) blocked for your general purpose bucket, you will get an
#'     HTTP 403 Access Denied error when you specify the SSE-C request
#'     headers while writing new data to your bucket. For more information,
#'     see [Blocking or unblocking SSE-C for a general purpose
#'     bucket](https://docs.aws.amazon.com/AmazonS3/latest/userguide/blocking-unblocking-s3-c-encryption-gpb.html).
#' 
#' -   **Directory buckets** - For directory buckets, there are only two
#'     supported options for server-side encryption: server-side encryption
#'     with Amazon S3 managed keys (SSE-S3) (`AES256`) and server-side
#'     encryption with KMS keys (SSE-KMS) (`aws:kms`). For more
#'     information, see [Protecting data with server-side
#'     encryption](https://docs.aws.amazon.com/AmazonS3/latest/userguide/)
#'     in the *Amazon S3 User Guide*.
#' 
#'     For directory buckets, when you perform a
#'     [`create_multipart_upload`][s3_create_multipart_upload] operation
#'     and an [`upload_part_copy`][s3_upload_part_copy] operation, the
#'     request headers you provide in the
#'     [`create_multipart_upload`][s3_create_multipart_upload] request must
#'     match the default encryption configuration of the destination
#'     bucket.
#' 
#'     S3 Bucket Keys aren't supported, when you copy SSE-KMS encrypted
#'     objects from general purpose buckets to directory buckets, from
#'     directory buckets to general purpose buckets, or between directory
#'     buckets, through [`upload_part_copy`][s3_upload_part_copy]. In this
#'     case, Amazon S3 makes a call to KMS every time a copy request is
#'     made for a KMS-encrypted object.
#' 
#' ### Special errors
#' 
#' -   Error Code: `NoSuchUpload`
#' 
#'     -   Description: The specified multipart upload does not exist. The
#'         upload ID might be invalid, or the multipart upload might have
#'         been aborted or completed.
#' 
#'     -   HTTP Status Code: 404 Not Found
#' 
#' -   Error Code: `InvalidRequest`
#' 
#'     -   Description: The specified copy source is not supported as a
#'         byte-range copy source.
#' 
#'     -   HTTP Status Code: 400 Bad Request
#' 
#' ### HTTP Host header syntax
#' 
#' **Directory buckets** - The HTTP Host header syntax is
#' ` Bucket-name.s3express-zone-id.region-code.amazonaws.com`.
#' 
#' The following operations are related to
#' [`upload_part_copy`][s3_upload_part_copy]:
#' 
#' -   [`create_multipart_upload`][s3_create_multipart_upload]
#' 
#' -   [`upload_part`][s3_upload_part]
#' 
#' -   [`complete_multipart_upload`][s3_complete_multipart_upload]
#' 
#' -   [`abort_multipart_upload`][s3_abort_multipart_upload]
#' 
#' -   [`list_parts`][s3_list_parts]
#' 
#' -   [`list_multipart_uploads`][s3_list_multipart_uploads]
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#'
#' @usage
#' s3_upload_part_copy(Bucket, CopySource, CopySourceIfMatch,
#'   CopySourceIfModifiedSince, CopySourceIfNoneMatch,
#'   CopySourceIfUnmodifiedSince, CopySourceRange, Key, PartNumber, UploadId,
#'   SSECustomerAlgorithm, SSECustomerKey, SSECustomerKeyMD5,
#'   CopySourceSSECustomerAlgorithm, CopySourceSSECustomerKey,
#'   CopySourceSSECustomerKeyMD5, RequestPayer, ExpectedBucketOwner,
#'   ExpectedSourceBucketOwner)
#'
#' @param Bucket &#91;required&#93; The bucket name.
#' 
#' **Directory buckets** - When you use this operation with a directory
#' bucket, you must use virtual-hosted-style requests in the format
#' ` Bucket-name.s3express-zone-id.region-code.amazonaws.com`. Path-style
#' requests are not supported. Directory bucket names must be unique in the
#' chosen Zone (Availability Zone or Local Zone). Bucket names must follow
#' the format ` bucket-base-name--zone-id--x-s3` (for example,
#' ` amzn-s3-demo-bucket--usw2-az1--x-s3`). For information about bucket
#' naming restrictions, see [Directory bucket naming
#' rules](https://docs.aws.amazon.com/AmazonS3/latest/userguide/directory-bucket-naming-rules.html)
#' in the *Amazon S3 User Guide*.
#' 
#' Copying objects across different Amazon Web Services Regions isn't
#' supported when the source or destination bucket is in Amazon Web
#' Services Local Zones. The source and destination buckets must have the
#' same parent Amazon Web Services Region. Otherwise, you get an HTTP
#' `400 Bad Request` error with the error code `InvalidRequest`.
#' 
#' **Access points** - When you use this action with an access point for
#' general purpose buckets, you must provide the alias of the access point
#' in place of the bucket name or specify the access point ARN. When you
#' use this action with an access point for directory buckets, you must
#' provide the access point name in place of the bucket name. When using
#' the access point ARN, you must direct requests to the access point
#' hostname. The access point hostname takes the form
#' *AccessPointName*-*AccountId*.s3-accesspoint.*Region*.amazonaws.com.
#' When using this action with an access point through the Amazon Web
#' Services SDKs, you provide the access point ARN in place of the bucket
#' name. For more information about access point ARNs, see [Using access
#' points](https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-access-points.html)
#' in the *Amazon S3 User Guide*.
#' 
#' Object Lambda access points are not supported by directory buckets.
#' 
#' **S3 on Outposts** - When you use this action with S3 on Outposts, you
#' must direct requests to the S3 on Outposts hostname. The S3 on Outposts
#' hostname takes the form
#' ` AccessPointName-AccountId.outpostID.s3-outposts.Region.amazonaws.com`.
#' When you use this action with S3 on Outposts, the destination bucket
#' must be the Outposts access point ARN or the access point alias. For
#' more information about S3 on Outposts, see [What is S3 on
#' Outposts?](https://docs.aws.amazon.com/AmazonS3/latest/s3-outposts/S3onOutposts.html)
#' in the *Amazon S3 User Guide*.
#' @param CopySource &#91;required&#93; Specifies the source object for the copy operation. You specify the
#' value in one of two formats, depending on whether you want to access the
#' source object through an [access
#' point](https://docs.aws.amazon.com/AmazonS3/latest/userguide/access-points.html):
#' 
#' -   For objects not accessed through an access point, specify the name
#'     of the source bucket and key of the source object, separated by a
#'     slash (/). For example, to copy the object `reports/january.pdf`
#'     from the bucket `awsexamplebucket`, use
#'     `awsexamplebucket/reports/january.pdf`. The value must be
#'     URL-encoded.
#' 
#' -   For objects accessed through access points, specify the Amazon
#'     Resource Name (ARN) of the object as accessed through the access
#'     point, in the format
#'     `arn:aws:s3:<Region>:<account-id>:accesspoint/<access-point-name>/object/<key>`.
#'     For example, to copy the object `reports/january.pdf` through access
#'     point `my-access-point` owned by account `123456789012` in Region
#'     `us-west-2`, use the URL encoding of
#'     `arn:aws:s3:us-west-2:123456789012:accesspoint/my-access-point/object/reports/january.pdf`.
#'     The value must be URL encoded.
#' 
#'     -   Amazon S3 supports copy operations using Access points only when
#'         the source and destination buckets are in the same Amazon Web
#'         Services Region.
#' 
#'     -   Access points are not supported by directory buckets.
#' 
#'     Alternatively, for objects accessed through Amazon S3 on Outposts,
#'     specify the ARN of the object as accessed in the format
#'     `arn:aws:s3-outposts:<Region>:<account-id>:outpost/<outpost-id>/object/<key>`.
#'     For example, to copy the object `reports/january.pdf` through
#'     outpost `my-outpost` owned by account `123456789012` in Region
#'     `us-west-2`, use the URL encoding of
#'     `arn:aws:s3-outposts:us-west-2:123456789012:outpost/my-outpost/object/reports/january.pdf`.
#'     The value must be URL-encoded.
#' 
#' If your bucket has versioning enabled, you could have multiple versions
#' of the same object. By default, `x-amz-copy-source` identifies the
#' current version of the source object to copy. To copy a specific version
#' of the source object to copy, append `?versionId=<version-id>` to the
#' `x-amz-copy-source` request header (for example,
#' `x-amz-copy-source: /awsexamplebucket/reports/january.pdf?versionId=QUpfdndhfd8438MNFDN93jdnJFkdmqnh893`).
#' 
#' If the current version is a delete marker and you don't specify a
#' versionId in the `x-amz-copy-source` request header, Amazon S3 returns a
#' `404 Not Found` error, because the object does not exist. If you specify
#' versionId in the `x-amz-copy-source` and the versionId is a delete
#' marker, Amazon S3 returns an HTTP `400 Bad Request` error, because you
#' are not allowed to specify a delete marker as a version for the
#' `x-amz-copy-source`.
#' 
#' **Directory buckets** - S3 Versioning isn't enabled and supported for
#' directory buckets.
#' @param CopySourceIfMatch Copies the object if its entity tag (ETag) matches the specified tag.
#' 
#' If both of the `x-amz-copy-source-if-match` and
#' `x-amz-copy-source-if-unmodified-since` headers are present in the
#' request as follows:
#' 
#' `x-amz-copy-source-if-match` condition evaluates to `true`, and;
#' 
#' `x-amz-copy-source-if-unmodified-since` condition evaluates to `false`;
#' 
#' Amazon S3 returns `200 OK` and copies the data.
#' @param CopySourceIfModifiedSince Copies the object if it has been modified since the specified time.
#' 
#' If both of the `x-amz-copy-source-if-none-match` and
#' `x-amz-copy-source-if-modified-since` headers are present in the request
#' as follows:
#' 
#' `x-amz-copy-source-if-none-match` condition evaluates to `false`, and;
#' 
#' `x-amz-copy-source-if-modified-since` condition evaluates to `true`;
#' 
#' Amazon S3 returns `412 Precondition Failed` response code.
#' @param CopySourceIfNoneMatch Copies the object if its entity tag (ETag) is different than the
#' specified ETag.
#' 
#' If both of the `x-amz-copy-source-if-none-match` and
#' `x-amz-copy-source-if-modified-since` headers are present in the request
#' as follows:
#' 
#' `x-amz-copy-source-if-none-match` condition evaluates to `false`, and;
#' 
#' `x-amz-copy-source-if-modified-since` condition evaluates to `true`;
#' 
#' Amazon S3 returns `412 Precondition Failed` response code.
#' @param CopySourceIfUnmodifiedSince Copies the object if it hasn't been modified since the specified time.
#' 
#' If both of the `x-amz-copy-source-if-match` and
#' `x-amz-copy-source-if-unmodified-since` headers are present in the
#' request as follows:
#' 
#' `x-amz-copy-source-if-match` condition evaluates to `true`, and;
#' 
#' `x-amz-copy-source-if-unmodified-since` condition evaluates to `false`;
#' 
#' Amazon S3 returns `200 OK` and copies the data.
#' @param CopySourceRange The range of bytes to copy from the source object. The range value must
#' use the form bytes=first-last, where the first and last are the
#' zero-based byte offsets to copy. For example, bytes=0-9 indicates that
#' you want to copy the first 10 bytes of the source. You can copy a range
#' only if the source object is greater than 5 MB.
#' @param Key &#91;required&#93; Object key for which the multipart upload was initiated.
#' @param PartNumber &#91;required&#93; Part number of part being copied. This is a positive integer between 1
#' and 10,000.
#' @param UploadId &#91;required&#93; Upload ID identifying the multipart upload whose part is being copied.
#' @param SSECustomerAlgorithm Specifies the algorithm to use when encrypting the object (for example,
#' AES256).
#' 
#' This functionality is not supported when the destination bucket is a
#' directory bucket.
#' @param SSECustomerKey Specifies the customer-provided encryption key for Amazon S3 to use in
#' encrypting data. This value is used to store the object and then it is
#' discarded; Amazon S3 does not store the encryption key. The key must be
#' appropriate for use with the algorithm specified in the
#' `x-amz-server-side-encryption-customer-algorithm` header. This must be
#' the same encryption key specified in the initiate multipart upload
#' request.
#' 
#' This functionality is not supported when the destination bucket is a
#' directory bucket.
#' @param SSECustomerKeyMD5 Specifies the 128-bit MD5 digest of the encryption key according to RFC
#' 1321. Amazon S3 uses this header for a message integrity check to ensure
#' that the encryption key was transmitted without error.
#' 
#' This functionality is not supported when the destination bucket is a
#' directory bucket.
#' @param CopySourceSSECustomerAlgorithm Specifies the algorithm to use when decrypting the source object (for
#' example, `AES256`).
#' 
#' This functionality is not supported when the source object is in a
#' directory bucket.
#' @param CopySourceSSECustomerKey Specifies the customer-provided encryption key for Amazon S3 to use to
#' decrypt the source object. The encryption key provided in this header
#' must be one that was used when the source object was created.
#' 
#' This functionality is not supported when the source object is in a
#' directory bucket.
#' @param CopySourceSSECustomerKeyMD5 Specifies the 128-bit MD5 digest of the encryption key according to RFC
#' 1321. Amazon S3 uses this header for a message integrity check to ensure
#' that the encryption key was transmitted without error.
#' 
#' This functionality is not supported when the source object is in a
#' directory bucket.
#' @param RequestPayer 
#' @param ExpectedBucketOwner The account ID of the expected destination bucket owner. If the account
#' ID that you provide does not match the actual owner of the destination
#' bucket, the request fails with the HTTP status code `403 Forbidden`
#' (access denied).
#' @param ExpectedSourceBucketOwner The account ID of the expected source bucket owner. If the account ID
#' that you provide does not match the actual owner of the source bucket,
#' the request fails with the HTTP status code `403 Forbidden` (access
#' denied).
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   CopySourceVersionId = "string",
#'   CopyPartResult = list(
#'     ETag = "string",
#'     LastModified = as.POSIXct(
#'       "2015-01-01"
#'     ),
#'     ChecksumCRC32 = "string",
#'     ChecksumCRC32C = "string",
#'     ChecksumCRC64NVME = "string",
#'     ChecksumSHA1 = "string",
#'     ChecksumSHA256 = "string"
#'   ),
#'   ServerSideEncryption = "AES256"|"aws:fsx"|"aws:kms"|"aws:kms:dsse",
#'   SSECustomerAlgorithm = "string",
#'   SSECustomerKeyMD5 = "string",
#'   SSEKMSKeyId = "string",
#'   BucketKeyEnabled = TRUE|FALSE,
#'   RequestCharged = "requester"
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$upload_part_copy(
#'   Bucket = "string",
#'   CopySource = "string",
#'   CopySourceIfMatch = "string",
#'   CopySourceIfModifiedSince = as.POSIXct(
#'     "2015-01-01"
#'   ),
#'   CopySourceIfNoneMatch = "string",
#'   CopySourceIfUnmodifiedSince = as.POSIXct(
#'     "2015-01-01"
#'   ),
#'   CopySourceRange = "string",
#'   Key = "string",
#'   PartNumber = 123,
#'   UploadId = "string",
#'   SSECustomerAlgorithm = "string",
#'   SSECustomerKey = "string",
#'   SSECustomerKeyMD5 = "string",
#'   CopySourceSSECustomerAlgorithm = "string",
#'   CopySourceSSECustomerKey = "string",
#'   CopySourceSSECustomerKeyMD5 = "string",
#'   RequestPayer = "requester",
#'   ExpectedBucketOwner = "string",
#'   ExpectedSourceBucketOwner = "string"
#' )
#' ```
#'
#' @examples
#' \dontrun{
#' # The following example uploads a part of a multipart upload by copying
#' # data from an existing object as data source.
#' svc$upload_part_copy(
#'   Bucket = "examplebucket",
#'   CopySource = "/bucketname/sourceobjectkey",
#'   Key = "examplelargeobject",
#'   PartNumber = "1",
#'   UploadId = "exampleuoh_10OhKhT7YukE9bjzTPRiuaCotmZM_pFngJFir9OZNrSr5cWa3c..."
#' )
#' 
#' # The following example uploads a part of a multipart upload by copying a
#' # specified byte range from an existing object as data source.
#' svc$upload_part_copy(
#'   Bucket = "examplebucket",
#'   CopySource = "/bucketname/sourceobjectkey",
#'   CopySourceRange = "bytes=1-100000",
#'   Key = "examplelargeobject",
#'   PartNumber = "2",
#'   UploadId = "exampleuoh_10OhKhT7YukE9bjzTPRiuaCotmZM_pFngJFir9OZNrSr5cWa3c..."
#' )
#' }
#'
#' @keywords internal
#'
#' @rdname s3_upload_part_copy
#'
#' @aliases s3_upload_part_copy
s3_upload_part_copy <- function(Bucket, CopySource, CopySourceIfMatch = NULL, CopySourceIfModifiedSince = NULL, CopySourceIfNoneMatch = NULL, CopySourceIfUnmodifiedSince = NULL, CopySourceRange = NULL, Key, PartNumber, UploadId, SSECustomerAlgorithm = NULL, SSECustomerKey = NULL, SSECustomerKeyMD5 = NULL, CopySourceSSECustomerAlgorithm = NULL, CopySourceSSECustomerKey = NULL, CopySourceSSECustomerKeyMD5 = NULL, RequestPayer = NULL, ExpectedBucketOwner = NULL, ExpectedSourceBucketOwner = NULL) {
  op <- new_operation(
    name = "CopyPart",
    http_method = "PUT",
    http_path = "/{Bucket}/{Key+}",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .s3$upload_part_copy_input(Bucket = Bucket, CopySource = CopySource, CopySourceIfMatch = CopySourceIfMatch, CopySourceIfModifiedSince = CopySourceIfModifiedSince, CopySourceIfNoneMatch = CopySourceIfNoneMatch, CopySourceIfUnmodifiedSince = CopySourceIfUnmodifiedSince, CopySourceRange = CopySourceRange, Key = Key, PartNumber = PartNumber, UploadId = UploadId, SSECustomerAlgorithm = SSECustomerAlgorithm, SSECustomerKey = SSECustomerKey, SSECustomerKeyMD5 = SSECustomerKeyMD5, CopySourceSSECustomerAlgorithm = CopySourceSSECustomerAlgorithm, CopySourceSSECustomerKey = CopySourceSSECustomerKey, CopySourceSSECustomerKeyMD5 = CopySourceSSECustomerKeyMD5, RequestPayer = RequestPayer, ExpectedBucketOwner = ExpectedBucketOwner, ExpectedSourceBucketOwner = ExpectedSourceBucketOwner)
  output <- .s3$upload_part_copy_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$upload_part_copy <- s3_upload_part_copy

#' This operation is not supported for directory buckets
#'
#' @description
#' This operation is not supported for directory buckets.
#' 
#' Passes transformed objects to a [`get_object`][s3_get_object] operation
#' when using Object Lambda access points. For information about Object
#' Lambda access points, see [Transforming objects with Object Lambda
#' access
#' points](https://docs.aws.amazon.com/AmazonS3/latest/userguide/transforming-objects.html)
#' in the *Amazon S3 User Guide*.
#' 
#' This operation supports metadata that can be returned by
#' [`get_object`][s3_get_object], in addition to `RequestRoute`,
#' `RequestToken`, `StatusCode`, `ErrorCode`, and `ErrorMessage`. The
#' [`get_object`][s3_get_object] response metadata is supported so that the
#' [`write_get_object_response`][s3_write_get_object_response] caller,
#' typically an Lambda function, can provide the same metadata when it
#' internally invokes [`get_object`][s3_get_object]. When
#' [`write_get_object_response`][s3_write_get_object_response] is called by
#' a customer-owned Lambda function, the metadata returned to the end user
#' [`get_object`][s3_get_object] call might differ from what Amazon S3
#' would normally return.
#' 
#' You can include any number of metadata headers. When including a
#' metadata header, it should be prefaced with `x-amz-meta`. For example,
#' `x-amz-meta-my-custom-header: MyCustomValue`. The primary use case for
#' this is to forward [`get_object`][s3_get_object] metadata.
#' 
#' Amazon Web Services provides some prebuilt Lambda functions that you can
#' use with S3 Object Lambda to detect and redact personally identifiable
#' information (PII) and decompress S3 objects. These Lambda functions are
#' available in the Amazon Web Services Serverless Application Repository,
#' and can be selected through the Amazon Web Services Management Console
#' when you create your Object Lambda access point.
#' 
#' Example 1: PII Access Control - This Lambda function uses Amazon
#' Comprehend, a natural language processing (NLP) service using machine
#' learning to find insights and relationships in text. It automatically
#' detects personally identifiable information (PII) such as names,
#' addresses, dates, credit card numbers, and social security numbers from
#' documents in your Amazon S3 bucket.
#' 
#' Example 2: PII Redaction - This Lambda function uses Amazon Comprehend,
#' a natural language processing (NLP) service using machine learning to
#' find insights and relationships in text. It automatically redacts
#' personally identifiable information (PII) such as names, addresses,
#' dates, credit card numbers, and social security numbers from documents
#' in your Amazon S3 bucket.
#' 
#' Example 3: Decompression - The Lambda function
#' S3ObjectLambdaDecompression, is equipped to decompress objects stored in
#' S3 in one of six compressed file formats including bzip2, gzip, snappy,
#' zlib, zstandard and ZIP.
#' 
#' For information on how to view and use these functions, see [Using
#' Amazon Web Services built Lambda
#' functions](https://docs.aws.amazon.com/AmazonS3/latest/userguide/olap-examples.html)
#' in the *Amazon S3 User Guide*.
#' 
#' You must URL encode any signed header values that contain spaces. For
#' example, if your header value is `my file.txt`, containing two spaces
#' after `my`, you must URL encode this value to `my%20%20file.txt`.
#'
#' @usage
#' s3_write_get_object_response(RequestRoute, RequestToken, Body,
#'   StatusCode, ErrorCode, ErrorMessage, AcceptRanges, CacheControl,
#'   ContentDisposition, ContentEncoding, ContentLanguage, ContentLength,
#'   ContentRange, ContentType, ChecksumCRC32, ChecksumCRC32C,
#'   ChecksumCRC64NVME, ChecksumSHA1, ChecksumSHA256, DeleteMarker, ETag,
#'   Expires, Expiration, LastModified, MissingMeta, Metadata,
#'   ObjectLockMode, ObjectLockLegalHoldStatus, ObjectLockRetainUntilDate,
#'   PartsCount, ReplicationStatus, RequestCharged, Restore,
#'   ServerSideEncryption, SSECustomerAlgorithm, SSEKMSKeyId,
#'   SSECustomerKeyMD5, StorageClass, TagCount, VersionId, BucketKeyEnabled)
#'
#' @param RequestRoute &#91;required&#93; Route prefix to the HTTP URL generated.
#' @param RequestToken &#91;required&#93; A single use encrypted token that maps
#' [`write_get_object_response`][s3_write_get_object_response] to the end
#' user [`get_object`][s3_get_object] request.
#' @param Body The object data.
#' @param StatusCode The integer status code for an HTTP response of a corresponding
#' [`get_object`][s3_get_object] request. The following is a list of status
#' codes.
#' 
#' -   `200 - OK`
#' 
#' -   `206 - Partial Content`
#' 
#' -   `304 - Not Modified`
#' 
#' -   `400 - Bad Request`
#' 
#' -   `401 - Unauthorized`
#' 
#' -   `403 - Forbidden`
#' 
#' -   `404 - Not Found`
#' 
#' -   `405 - Method Not Allowed`
#' 
#' -   `409 - Conflict`
#' 
#' -   `411 - Length Required`
#' 
#' -   `412 - Precondition Failed`
#' 
#' -   `416 - Range Not Satisfiable`
#' 
#' -   `500 - Internal Server Error`
#' 
#' -   `503 - Service Unavailable`
#' @param ErrorCode A string that uniquely identifies an error condition. Returned in the
#' \<Code\> tag of the error XML response for a corresponding
#' [`get_object`][s3_get_object] call. Cannot be used with a successful
#' `StatusCode` header or when the transformed object is provided in the
#' body. All error codes from S3 are sentence-cased. The regular expression
#' (regex) value is `"^[A-Z][a-zA-Z]+$"`.
#' @param ErrorMessage Contains a generic description of the error condition. Returned in the
#' \<Message\> tag of the error XML response for a corresponding
#' [`get_object`][s3_get_object] call. Cannot be used with a successful
#' `StatusCode` header or when the transformed object is provided in body.
#' @param AcceptRanges Indicates that a range of bytes was specified.
#' @param CacheControl Specifies caching behavior along the request/reply chain.
#' @param ContentDisposition Specifies presentational information for the object.
#' @param ContentEncoding Specifies what content encodings have been applied to the object and
#' thus what decoding mechanisms must be applied to obtain the media-type
#' referenced by the Content-Type header field.
#' @param ContentLanguage The language the content is in.
#' @param ContentLength The size of the content body in bytes.
#' @param ContentRange The portion of the object returned in the response.
#' @param ContentType A standard MIME type describing the format of the object data.
#' @param ChecksumCRC32 This header can be used as a data integrity check to verify that the
#' data received is the same data that was originally sent. This specifies
#' the Base64 encoded, 32-bit `CRC32` checksum of the object returned by
#' the Object Lambda function. This may not match the checksum for the
#' object stored in Amazon S3. Amazon S3 will perform validation of the
#' checksum values only when the original [`get_object`][s3_get_object]
#' request required checksum validation. For more information about
#' checksums, see [Checking object
#' integrity](https://docs.aws.amazon.com/AmazonS3/latest/userguide/checking-object-integrity.html)
#' in the *Amazon S3 User Guide*.
#' 
#' Only one checksum header can be specified at a time. If you supply
#' multiple checksum headers, this request will fail.
#' @param ChecksumCRC32C This header can be used as a data integrity check to verify that the
#' data received is the same data that was originally sent. This specifies
#' the Base64 encoded, 32-bit `CRC32C` checksum of the object returned by
#' the Object Lambda function. This may not match the checksum for the
#' object stored in Amazon S3. Amazon S3 will perform validation of the
#' checksum values only when the original [`get_object`][s3_get_object]
#' request required checksum validation. For more information about
#' checksums, see [Checking object
#' integrity](https://docs.aws.amazon.com/AmazonS3/latest/userguide/checking-object-integrity.html)
#' in the *Amazon S3 User Guide*.
#' 
#' Only one checksum header can be specified at a time. If you supply
#' multiple checksum headers, this request will fail.
#' @param ChecksumCRC64NVME This header can be used as a data integrity check to verify that the
#' data received is the same data that was originally sent. This header
#' specifies the Base64 encoded, 64-bit `CRC64NVME` checksum of the part.
#' For more information, see [Checking object
#' integrity](https://docs.aws.amazon.com/AmazonS3/latest/userguide/checking-object-integrity.html)
#' in the *Amazon S3 User Guide*.
#' @param ChecksumSHA1 This header can be used as a data integrity check to verify that the
#' data received is the same data that was originally sent. This specifies
#' the Base64 encoded, 160-bit `SHA1` digest of the object returned by the
#' Object Lambda function. This may not match the checksum for the object
#' stored in Amazon S3. Amazon S3 will perform validation of the checksum
#' values only when the original [`get_object`][s3_get_object] request
#' required checksum validation. For more information about checksums, see
#' [Checking object
#' integrity](https://docs.aws.amazon.com/AmazonS3/latest/userguide/checking-object-integrity.html)
#' in the *Amazon S3 User Guide*.
#' 
#' Only one checksum header can be specified at a time. If you supply
#' multiple checksum headers, this request will fail.
#' @param ChecksumSHA256 This header can be used as a data integrity check to verify that the
#' data received is the same data that was originally sent. This specifies
#' the Base64 encoded, 256-bit `SHA256` digest of the object returned by
#' the Object Lambda function. This may not match the checksum for the
#' object stored in Amazon S3. Amazon S3 will perform validation of the
#' checksum values only when the original [`get_object`][s3_get_object]
#' request required checksum validation. For more information about
#' checksums, see [Checking object
#' integrity](https://docs.aws.amazon.com/AmazonS3/latest/userguide/checking-object-integrity.html)
#' in the *Amazon S3 User Guide*.
#' 
#' Only one checksum header can be specified at a time. If you supply
#' multiple checksum headers, this request will fail.
#' @param DeleteMarker Specifies whether an object stored in Amazon S3 is (`true`) or is not
#' (`false`) a delete marker. To learn more about delete markers, see
#' [Working with delete
#' markers](https://docs.aws.amazon.com/AmazonS3/latest/userguide/DeleteMarker.html).
#' @param ETag An opaque identifier assigned by a web server to a specific version of a
#' resource found at a URL.
#' @param Expires The date and time at which the object is no longer cacheable.
#' @param Expiration If the object expiration is configured (see PUT Bucket lifecycle), the
#' response includes this header. It includes the `expiry-date` and
#' `rule-id` key-value pairs that provide the object expiration
#' information. The value of the `rule-id` is URL-encoded.
#' @param LastModified The date and time that the object was last modified.
#' @param MissingMeta Set to the number of metadata entries not returned in `x-amz-meta`
#' headers. This can happen if you create metadata using an API like SOAP
#' that supports more flexible metadata than the REST API. For example,
#' using SOAP, you can create metadata whose values are not legal HTTP
#' headers.
#' @param Metadata A map of metadata to store with the object in S3.
#' @param ObjectLockMode Indicates whether an object stored in Amazon S3 has Object Lock enabled.
#' For more information about S3 Object Lock, see [Object
#' Lock](https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lock.html).
#' @param ObjectLockLegalHoldStatus Indicates whether an object stored in Amazon S3 has an active legal
#' hold.
#' @param ObjectLockRetainUntilDate The date and time when Object Lock is configured to expire.
#' @param PartsCount The count of parts this object has.
#' @param ReplicationStatus Indicates if request involves bucket that is either a source or
#' destination in a Replication rule. For more information about S3
#' Replication, see
#' [Replication](https://docs.aws.amazon.com/AmazonS3/latest/userguide/replication.html).
#' @param RequestCharged 
#' @param Restore Provides information about object restoration operation and expiration
#' time of the restored object copy.
#' @param ServerSideEncryption The server-side encryption algorithm used when storing requested object
#' in Amazon S3 or Amazon FSx.
#' 
#' When accessing data stored in Amazon FSx file systems using S3 access
#' points, the only valid server side encryption option is `aws:fsx`.
#' @param SSECustomerAlgorithm Encryption algorithm used if server-side encryption with a
#' customer-provided encryption key was specified for object stored in
#' Amazon S3.
#' @param SSEKMSKeyId If present, specifies the ID (Key ID, Key ARN, or Key Alias) of the
#' Amazon Web Services Key Management Service (Amazon Web Services KMS)
#' symmetric encryption customer managed key that was used for stored in
#' Amazon S3 object.
#' @param SSECustomerKeyMD5 128-bit MD5 digest of customer-provided encryption key used in Amazon S3
#' to encrypt data stored in S3. For more information, see [Protecting data
#' using server-side encryption with customer-provided encryption keys
#' (SSE-C)](https://docs.aws.amazon.com/AmazonS3/latest/userguide/ServerSideEncryptionCustomerKeys.html).
#' @param StorageClass Provides storage class information of the object. Amazon S3 returns this
#' header for all objects except for S3 Standard storage class objects.
#' 
#' For more information, see [Storage
#' Classes](https://docs.aws.amazon.com/AmazonS3/latest/userguide/storage-class-intro.html).
#' @param TagCount The number of tags, if any, on the object.
#' @param VersionId An ID used to reference a specific version of the object.
#' @param BucketKeyEnabled Indicates whether the object stored in Amazon S3 uses an S3 bucket key
#' for server-side encryption with Amazon Web Services KMS (SSE-KMS).
#'
#' @return
#' An empty list.
#'
#' @section Request syntax:
#' ```
#' svc$write_get_object_response(
#'   RequestRoute = "string",
#'   RequestToken = "string",
#'   Body = raw,
#'   StatusCode = 123,
#'   ErrorCode = "string",
#'   ErrorMessage = "string",
#'   AcceptRanges = "string",
#'   CacheControl = "string",
#'   ContentDisposition = "string",
#'   ContentEncoding = "string",
#'   ContentLanguage = "string",
#'   ContentLength = 123,
#'   ContentRange = "string",
#'   ContentType = "string",
#'   ChecksumCRC32 = "string",
#'   ChecksumCRC32C = "string",
#'   ChecksumCRC64NVME = "string",
#'   ChecksumSHA1 = "string",
#'   ChecksumSHA256 = "string",
#'   DeleteMarker = TRUE|FALSE,
#'   ETag = "string",
#'   Expires = "string",
#'   Expiration = "string",
#'   LastModified = as.POSIXct(
#'     "2015-01-01"
#'   ),
#'   MissingMeta = 123,
#'   Metadata = list(
#'     "string"
#'   ),
#'   ObjectLockMode = "GOVERNANCE"|"COMPLIANCE",
#'   ObjectLockLegalHoldStatus = "ON"|"OFF",
#'   ObjectLockRetainUntilDate = as.POSIXct(
#'     "2015-01-01"
#'   ),
#'   PartsCount = 123,
#'   ReplicationStatus = "COMPLETE"|"PENDING"|"FAILED"|"REPLICA"|"COMPLETED",
#'   RequestCharged = "requester",
#'   Restore = "string",
#'   ServerSideEncryption = "AES256"|"aws:fsx"|"aws:kms"|"aws:kms:dsse",
#'   SSECustomerAlgorithm = "string",
#'   SSEKMSKeyId = "string",
#'   SSECustomerKeyMD5 = "string",
#'   StorageClass = "STANDARD"|"REDUCED_REDUNDANCY"|"STANDARD_IA"|"ONEZONE_IA"|"INTELLIGENT_TIERING"|"GLACIER"|"DEEP_ARCHIVE"|"OUTPOSTS"|"GLACIER_IR"|"SNOW"|"EXPRESS_ONEZONE"|"FSX_OPENZFS"|"FSX_ONTAP",
#'   TagCount = 123,
#'   VersionId = "string",
#'   BucketKeyEnabled = TRUE|FALSE
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname s3_write_get_object_response
#'
#' @aliases s3_write_get_object_response
s3_write_get_object_response <- function(RequestRoute, RequestToken, Body = NULL, StatusCode = NULL, ErrorCode = NULL, ErrorMessage = NULL, AcceptRanges = NULL, CacheControl = NULL, ContentDisposition = NULL, ContentEncoding = NULL, ContentLanguage = NULL, ContentLength = NULL, ContentRange = NULL, ContentType = NULL, ChecksumCRC32 = NULL, ChecksumCRC32C = NULL, ChecksumCRC64NVME = NULL, ChecksumSHA1 = NULL, ChecksumSHA256 = NULL, DeleteMarker = NULL, ETag = NULL, Expires = NULL, Expiration = NULL, LastModified = NULL, MissingMeta = NULL, Metadata = NULL, ObjectLockMode = NULL, ObjectLockLegalHoldStatus = NULL, ObjectLockRetainUntilDate = NULL, PartsCount = NULL, ReplicationStatus = NULL, RequestCharged = NULL, Restore = NULL, ServerSideEncryption = NULL, SSECustomerAlgorithm = NULL, SSEKMSKeyId = NULL, SSECustomerKeyMD5 = NULL, StorageClass = NULL, TagCount = NULL, VersionId = NULL, BucketKeyEnabled = NULL) {
  op <- new_operation(
    name = "WriteGetObjectResponse",
    http_method = "POST",
    http_path = "/WriteGetObjectResponse",
    host_prefix = "{RequestRoute}.",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .s3$write_get_object_response_input(RequestRoute = RequestRoute, RequestToken = RequestToken, Body = Body, StatusCode = StatusCode, ErrorCode = ErrorCode, ErrorMessage = ErrorMessage, AcceptRanges = AcceptRanges, CacheControl = CacheControl, ContentDisposition = ContentDisposition, ContentEncoding = ContentEncoding, ContentLanguage = ContentLanguage, ContentLength = ContentLength, ContentRange = ContentRange, ContentType = ContentType, ChecksumCRC32 = ChecksumCRC32, ChecksumCRC32C = ChecksumCRC32C, ChecksumCRC64NVME = ChecksumCRC64NVME, ChecksumSHA1 = ChecksumSHA1, ChecksumSHA256 = ChecksumSHA256, DeleteMarker = DeleteMarker, ETag = ETag, Expires = Expires, Expiration = Expiration, LastModified = LastModified, MissingMeta = MissingMeta, Metadata = Metadata, ObjectLockMode = ObjectLockMode, ObjectLockLegalHoldStatus = ObjectLockLegalHoldStatus, ObjectLockRetainUntilDate = ObjectLockRetainUntilDate, PartsCount = PartsCount, ReplicationStatus = ReplicationStatus, RequestCharged = RequestCharged, Restore = Restore, ServerSideEncryption = ServerSideEncryption, SSECustomerAlgorithm = SSECustomerAlgorithm, SSEKMSKeyId = SSEKMSKeyId, SSECustomerKeyMD5 = SSECustomerKeyMD5, StorageClass = StorageClass, TagCount = TagCount, VersionId = VersionId, BucketKeyEnabled = BucketKeyEnabled)
  output <- .s3$write_get_object_response_output()
  config <- get_config()
  svc <- .s3$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.s3$operations$write_get_object_response <- s3_write_get_object_response
