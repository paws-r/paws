# This file is generated by make.paws. Please do not edit here.
#' @importFrom paws.common get_config new_operation new_request send_request
#' @include rekognition_service.R
NULL

#' Associates one or more faces with an existing UserID
#'
#' @description
#' Associates one or more faces with an existing UserID. Takes an array of
#' `FaceIds`. Each `FaceId` that are present in the `FaceIds` list is
#' associated with the provided UserID. The number of FaceIds that can be
#' used as input in a single request is limited to 100.
#' 
#' Note that the total number of faces that can be associated with a single
#' `UserID` is also limited to 100. Once a `UserID` has 100 faces
#' associated with it, no additional faces can be added. If more API calls
#' are made after the limit is reached, a `ServiceQuotaExceededException`
#' will result.
#' 
#' The `UserMatchThreshold` parameter specifies the minimum user match
#' confidence required for the face to be associated with a UserID that has
#' at least one `FaceID` already associated. This ensures that the
#' `FaceIds` are associated with the right UserID. The value ranges from
#' 0-100 and default value is 75.
#' 
#' If successful, an array of `AssociatedFace` objects containing the
#' associated `FaceIds` is returned. If a given face is already associated
#' with the given `UserID`, it will be ignored and will not be returned in
#' the response. If a given face is already associated to a different
#' `UserID`, isn't found in the collection, doesn’t meet the
#' `UserMatchThreshold`, or there are already 100 faces associated with the
#' `UserID`, it will be returned as part of an array of
#' `UnsuccessfulFaceAssociations.`
#' 
#' The `UserStatus` reflects the status of an operation which updates a
#' UserID representation with a list of given faces. The `UserStatus` can
#' be:
#' 
#' -   ACTIVE - All associations or disassociations of FaceID(s) for a
#'     UserID are complete.
#' 
#' -   CREATED - A UserID has been created, but has no FaceID(s) associated
#'     with it.
#' 
#' -   UPDATING - A UserID is being updated and there are current
#'     associations or disassociations of FaceID(s) taking place.
#'
#' @usage
#' rekognition_associate_faces(CollectionId, UserId, FaceIds,
#'   UserMatchThreshold, ClientRequestToken)
#'
#' @param CollectionId &#91;required&#93; The ID of an existing collection containing the UserID.
#' @param UserId &#91;required&#93; The ID for the existing UserID.
#' @param FaceIds &#91;required&#93; An array of FaceIDs to associate with the UserID.
#' @param UserMatchThreshold An optional value specifying the minimum confidence in the UserID match
#' to return. The default value is 75.
#' @param ClientRequestToken Idempotent token used to identify the request to
#' [`associate_faces`][rekognition_associate_faces]. If you use the same
#' token with multiple [`associate_faces`][rekognition_associate_faces]
#' requests, the same response is returned. Use ClientRequestToken to
#' prevent the same request from being processed more than once.
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   AssociatedFaces = list(
#'     list(
#'       FaceId = "string"
#'     )
#'   ),
#'   UnsuccessfulFaceAssociations = list(
#'     list(
#'       FaceId = "string",
#'       UserId = "string",
#'       Confidence = 123.0,
#'       Reasons = list(
#'         "FACE_NOT_FOUND"|"ASSOCIATED_TO_A_DIFFERENT_USER"|"LOW_MATCH_CONFIDENCE"
#'       )
#'     )
#'   ),
#'   UserStatus = "ACTIVE"|"UPDATING"|"CREATING"|"CREATED"
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$associate_faces(
#'   CollectionId = "string",
#'   UserId = "string",
#'   FaceIds = list(
#'     "string"
#'   ),
#'   UserMatchThreshold = 123.0,
#'   ClientRequestToken = "string"
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname rekognition_associate_faces
#'
#' @aliases rekognition_associate_faces
rekognition_associate_faces <- function(CollectionId, UserId, FaceIds, UserMatchThreshold = NULL, ClientRequestToken = NULL) {
  op <- new_operation(
    name = "AssociateFaces",
    http_method = "POST",
    http_path = "/",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .rekognition$associate_faces_input(CollectionId = CollectionId, UserId = UserId, FaceIds = FaceIds, UserMatchThreshold = UserMatchThreshold, ClientRequestToken = ClientRequestToken)
  output <- .rekognition$associate_faces_output()
  config <- get_config()
  svc <- .rekognition$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.rekognition$operations$associate_faces <- rekognition_associate_faces

#' Compares a face in the source input image with each of the 100 largest
#' faces detected in the target input image
#'
#' @description
#' Compares a face in the *source* input image with each of the 100 largest
#' faces detected in the *target* input image.
#' 
#' If the source image contains multiple faces, the service detects the
#' largest face and compares it with each face detected in the target
#' image.
#' 
#' CompareFaces uses machine learning algorithms, which are probabilistic.
#' A false negative is an incorrect prediction that a face in the target
#' image has a low similarity confidence score when compared to the face in
#' the source image. To reduce the probability of false negatives, we
#' recommend that you compare the target image against multiple source
#' images. If you plan to use [`compare_faces`][rekognition_compare_faces]
#' to make a decision that impacts an individual's rights, privacy, or
#' access to services, we recommend that you pass the result to a human for
#' review and further validation before taking action.
#' 
#' You pass the input and target images either as base64-encoded image
#' bytes or as references to images in an Amazon S3 bucket. If you use the
#' AWS CLI to call Amazon Rekognition operations, passing image bytes isn't
#' supported. The image must be formatted as a PNG or JPEG file.
#' 
#' In response, the operation returns an array of face matches ordered by
#' similarity score in descending order. For each face match, the response
#' provides a bounding box of the face, facial landmarks, pose details
#' (pitch, roll, and yaw), quality (brightness and sharpness), and
#' confidence value (indicating the level of confidence that the bounding
#' box contains a face). The response also provides a similarity score,
#' which indicates how closely the faces match.
#' 
#' By default, only faces with a similarity score of greater than or equal
#' to 80% are returned in the response. You can change this value by
#' specifying the `SimilarityThreshold` parameter.
#' 
#' [`compare_faces`][rekognition_compare_faces] also returns an array of
#' faces that don't match the source image. For each face, it returns a
#' bounding box, confidence value, landmarks, pose details, and quality.
#' The response also returns information about the face in the source
#' image, including the bounding box of the face and confidence value.
#' 
#' The `QualityFilter` input parameter allows you to filter out detected
#' faces that don’t meet a required quality bar. The quality bar is based
#' on a variety of common use cases. Use `QualityFilter` to set the quality
#' bar by specifying `LOW`, `MEDIUM`, or `HIGH`. If you do not want to
#' filter detected faces, specify `NONE`. The default value is `NONE`.
#' 
#' If the image doesn't contain Exif metadata,
#' [`compare_faces`][rekognition_compare_faces] returns orientation
#' information for the source and target images. Use these values to
#' display the images with the correct image orientation.
#' 
#' If no faces are detected in the source or target images,
#' [`compare_faces`][rekognition_compare_faces] returns an
#' `InvalidParameterException` error.
#' 
#' This is a stateless API operation. That is, data returned by this
#' operation doesn't persist.
#' 
#' For an example, see Comparing Faces in Images in the Amazon Rekognition
#' Developer Guide.
#' 
#' This operation requires permissions to perform the
#' `rekognition:CompareFaces` action.
#'
#' @usage
#' rekognition_compare_faces(SourceImage, TargetImage, SimilarityThreshold,
#'   QualityFilter)
#'
#' @param SourceImage &#91;required&#93; The input image as base64-encoded bytes or an S3 object. If you use the
#' AWS CLI to call Amazon Rekognition operations, passing base64-encoded
#' image bytes is not supported.
#' 
#' If you are using an AWS SDK to call Amazon Rekognition, you might not
#' need to base64-encode image bytes passed using the `Bytes` field. For
#' more information, see Images in the Amazon Rekognition developer guide.
#' @param TargetImage &#91;required&#93; The target image as base64-encoded bytes or an S3 object. If you use the
#' AWS CLI to call Amazon Rekognition operations, passing base64-encoded
#' image bytes is not supported.
#' 
#' If you are using an AWS SDK to call Amazon Rekognition, you might not
#' need to base64-encode image bytes passed using the `Bytes` field. For
#' more information, see Images in the Amazon Rekognition developer guide.
#' @param SimilarityThreshold The minimum level of confidence in the face matches that a match must
#' meet to be included in the `FaceMatches` array.
#' @param QualityFilter A filter that specifies a quality bar for how much filtering is done to
#' identify faces. Filtered faces aren't compared. If you specify `AUTO`,
#' Amazon Rekognition chooses the quality bar. If you specify `LOW`,
#' `MEDIUM`, or `HIGH`, filtering removes all faces that don’t meet the
#' chosen quality bar. The quality bar is based on a variety of common use
#' cases. Low-quality detections can occur for a number of reasons. Some
#' examples are an object that's misidentified as a face, a face that's too
#' blurry, or a face with a pose that's too extreme to use. If you specify
#' `NONE`, no filtering is performed. The default value is `NONE`.
#' 
#' To use quality filtering, the collection you are using must be
#' associated with version 3 of the face model or higher.
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   SourceImageFace = list(
#'     BoundingBox = list(
#'       Width = 123.0,
#'       Height = 123.0,
#'       Left = 123.0,
#'       Top = 123.0
#'     ),
#'     Confidence = 123.0
#'   ),
#'   FaceMatches = list(
#'     list(
#'       Similarity = 123.0,
#'       Face = list(
#'         BoundingBox = list(
#'           Width = 123.0,
#'           Height = 123.0,
#'           Left = 123.0,
#'           Top = 123.0
#'         ),
#'         Confidence = 123.0,
#'         Landmarks = list(
#'           list(
#'             Type = "eyeLeft"|"eyeRight"|"nose"|"mouthLeft"|"mouthRight"|"leftEyeBrowLeft"|"leftEyeBrowRight"|"leftEyeBrowUp"|"rightEyeBrowLeft"|"rightEyeBrowRight"|"rightEyeBrowUp"|"leftEyeLeft"|"leftEyeRight"|"leftEyeUp"|"leftEyeDown"|"rightEyeLeft"|"rightEyeRight"|"rightEyeUp"|"rightEyeDown"|"noseLeft"|"noseRight"|"mouthUp"|"mouthDown"|"leftPupil"|"rightPupil"|"upperJawlineLeft"|"midJawlineLeft"|"chinBottom"|"midJawlineRight"|"upperJawlineRight",
#'             X = 123.0,
#'             Y = 123.0
#'           )
#'         ),
#'         Pose = list(
#'           Roll = 123.0,
#'           Yaw = 123.0,
#'           Pitch = 123.0
#'         ),
#'         Quality = list(
#'           Brightness = 123.0,
#'           Sharpness = 123.0
#'         ),
#'         Emotions = list(
#'           list(
#'             Type = "HAPPY"|"SAD"|"ANGRY"|"CONFUSED"|"DISGUSTED"|"SURPRISED"|"CALM"|"UNKNOWN"|"FEAR",
#'             Confidence = 123.0
#'           )
#'         ),
#'         Smile = list(
#'           Value = TRUE|FALSE,
#'           Confidence = 123.0
#'         )
#'       )
#'     )
#'   ),
#'   UnmatchedFaces = list(
#'     list(
#'       BoundingBox = list(
#'         Width = 123.0,
#'         Height = 123.0,
#'         Left = 123.0,
#'         Top = 123.0
#'       ),
#'       Confidence = 123.0,
#'       Landmarks = list(
#'         list(
#'           Type = "eyeLeft"|"eyeRight"|"nose"|"mouthLeft"|"mouthRight"|"leftEyeBrowLeft"|"leftEyeBrowRight"|"leftEyeBrowUp"|"rightEyeBrowLeft"|"rightEyeBrowRight"|"rightEyeBrowUp"|"leftEyeLeft"|"leftEyeRight"|"leftEyeUp"|"leftEyeDown"|"rightEyeLeft"|"rightEyeRight"|"rightEyeUp"|"rightEyeDown"|"noseLeft"|"noseRight"|"mouthUp"|"mouthDown"|"leftPupil"|"rightPupil"|"upperJawlineLeft"|"midJawlineLeft"|"chinBottom"|"midJawlineRight"|"upperJawlineRight",
#'           X = 123.0,
#'           Y = 123.0
#'         )
#'       ),
#'       Pose = list(
#'         Roll = 123.0,
#'         Yaw = 123.0,
#'         Pitch = 123.0
#'       ),
#'       Quality = list(
#'         Brightness = 123.0,
#'         Sharpness = 123.0
#'       ),
#'       Emotions = list(
#'         list(
#'           Type = "HAPPY"|"SAD"|"ANGRY"|"CONFUSED"|"DISGUSTED"|"SURPRISED"|"CALM"|"UNKNOWN"|"FEAR",
#'           Confidence = 123.0
#'         )
#'       ),
#'       Smile = list(
#'         Value = TRUE|FALSE,
#'         Confidence = 123.0
#'       )
#'     )
#'   ),
#'   SourceImageOrientationCorrection = "ROTATE_0"|"ROTATE_90"|"ROTATE_180"|"ROTATE_270",
#'   TargetImageOrientationCorrection = "ROTATE_0"|"ROTATE_90"|"ROTATE_180"|"ROTATE_270"
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$compare_faces(
#'   SourceImage = list(
#'     Bytes = raw,
#'     S3Object = list(
#'       Bucket = "string",
#'       Name = "string",
#'       Version = "string"
#'     )
#'   ),
#'   TargetImage = list(
#'     Bytes = raw,
#'     S3Object = list(
#'       Bucket = "string",
#'       Name = "string",
#'       Version = "string"
#'     )
#'   ),
#'   SimilarityThreshold = 123.0,
#'   QualityFilter = "NONE"|"AUTO"|"LOW"|"MEDIUM"|"HIGH"
#' )
#' ```
#'
#' @examples
#' \dontrun{
#' # This operation compares the largest face detected in the source image
#' # with each face detected in the target image.
#' svc$compare_faces(
#'   SimilarityThreshold = 90L,
#'   SourceImage = list(
#'     S3Object = list(
#'       Bucket = "mybucket",
#'       Name = "mysourceimage"
#'     )
#'   ),
#'   TargetImage = list(
#'     S3Object = list(
#'       Bucket = "mybucket",
#'       Name = "mytargetimage"
#'     )
#'   )
#' )
#' }
#'
#' @keywords internal
#'
#' @rdname rekognition_compare_faces
#'
#' @aliases rekognition_compare_faces
rekognition_compare_faces <- function(SourceImage, TargetImage, SimilarityThreshold = NULL, QualityFilter = NULL) {
  op <- new_operation(
    name = "CompareFaces",
    http_method = "POST",
    http_path = "/",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .rekognition$compare_faces_input(SourceImage = SourceImage, TargetImage = TargetImage, SimilarityThreshold = SimilarityThreshold, QualityFilter = QualityFilter)
  output <- .rekognition$compare_faces_output()
  config <- get_config()
  svc <- .rekognition$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.rekognition$operations$compare_faces <- rekognition_compare_faces

#' This operation applies only to Amazon Rekognition Custom Labels
#'
#' @description
#' This operation applies only to Amazon Rekognition Custom Labels.
#' 
#' Copies a version of an Amazon Rekognition Custom Labels model from a
#' source project to a destination project. The source and destination
#' projects can be in different AWS accounts but must be in the same AWS
#' Region. You can't copy a model to another AWS service.
#' 
#' To copy a model version to a different AWS account, you need to create a
#' resource-based policy known as a *project policy*. You attach the
#' project policy to the source project by calling
#' [`put_project_policy`][rekognition_put_project_policy]. The project
#' policy gives permission to copy the model version from a trusting AWS
#' account to a trusted account.
#' 
#' For more information creating and attaching a project policy, see
#' Attaching a project policy (SDK) in the *Amazon Rekognition Custom
#' Labels Developer Guide*.
#' 
#' If you are copying a model version to a project in the same AWS account,
#' you don't need to create a project policy.
#' 
#' Copying project versions is supported only for Custom Labels models.
#' 
#' To copy a model, the destination project, source project, and source
#' model version must already exist.
#' 
#' Copying a model version takes a while to complete. To get the current
#' status, call
#' [`describe_project_versions`][rekognition_describe_project_versions] and
#' check the value of `Status` in the ProjectVersionDescription object. The
#' copy operation has finished when the value of `Status` is
#' `COPYING_COMPLETED`.
#' 
#' This operation requires permissions to perform the
#' `rekognition:CopyProjectVersion` action.
#'
#' @usage
#' rekognition_copy_project_version(SourceProjectArn,
#'   SourceProjectVersionArn, DestinationProjectArn, VersionName,
#'   OutputConfig, Tags, KmsKeyId)
#'
#' @param SourceProjectArn &#91;required&#93; The ARN of the source project in the trusting AWS account.
#' @param SourceProjectVersionArn &#91;required&#93; The ARN of the model version in the source project that you want to copy
#' to a destination project.
#' @param DestinationProjectArn &#91;required&#93; The ARN of the project in the trusted AWS account that you want to copy
#' the model version to.
#' @param VersionName &#91;required&#93; A name for the version of the model that's copied to the destination
#' project.
#' @param OutputConfig &#91;required&#93; The S3 bucket and folder location where the training output for the
#' source model version is placed.
#' @param Tags The key-value tags to assign to the model version.
#' @param KmsKeyId The identifier for your AWS Key Management Service key (AWS KMS key).
#' You can supply the Amazon Resource Name (ARN) of your KMS key, the ID of
#' your KMS key, an alias for your KMS key, or an alias ARN. The key is
#' used to encrypt training results and manifest files written to the
#' output Amazon S3 bucket (`OutputConfig`).
#' 
#' If you choose to use your own KMS key, you need the following
#' permissions on the KMS key.
#' 
#' -   kms:CreateGrant
#' 
#' -   kms:DescribeKey
#' 
#' -   kms:GenerateDataKey
#' 
#' -   kms:Decrypt
#' 
#' If you don't specify a value for `KmsKeyId`, images copied into the
#' service are encrypted using a key that AWS owns and manages.
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   ProjectVersionArn = "string"
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$copy_project_version(
#'   SourceProjectArn = "string",
#'   SourceProjectVersionArn = "string",
#'   DestinationProjectArn = "string",
#'   VersionName = "string",
#'   OutputConfig = list(
#'     S3Bucket = "string",
#'     S3KeyPrefix = "string"
#'   ),
#'   Tags = list(
#'     "string"
#'   ),
#'   KmsKeyId = "string"
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname rekognition_copy_project_version
#'
#' @aliases rekognition_copy_project_version
rekognition_copy_project_version <- function(SourceProjectArn, SourceProjectVersionArn, DestinationProjectArn, VersionName, OutputConfig, Tags = NULL, KmsKeyId = NULL) {
  op <- new_operation(
    name = "CopyProjectVersion",
    http_method = "POST",
    http_path = "/",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .rekognition$copy_project_version_input(SourceProjectArn = SourceProjectArn, SourceProjectVersionArn = SourceProjectVersionArn, DestinationProjectArn = DestinationProjectArn, VersionName = VersionName, OutputConfig = OutputConfig, Tags = Tags, KmsKeyId = KmsKeyId)
  output <- .rekognition$copy_project_version_output()
  config <- get_config()
  svc <- .rekognition$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.rekognition$operations$copy_project_version <- rekognition_copy_project_version

#' Creates a collection in an AWS Region
#'
#' @description
#' Creates a collection in an AWS Region. You can add faces to the
#' collection using the [`index_faces`][rekognition_index_faces] operation.
#' 
#' For example, you might create collections, one for each of your
#' application users. A user can then index faces using the
#' [`index_faces`][rekognition_index_faces] operation and persist results
#' in a specific collection. Then, a user can search the collection for
#' faces in the user-specific container.
#' 
#' When you create a collection, it is associated with the latest version
#' of the face model version.
#' 
#' Collection names are case-sensitive.
#' 
#' This operation requires permissions to perform the
#' `rekognition:CreateCollection` action. If you want to tag your
#' collection, you also require permission to perform the
#' `rekognition:TagResource` operation.
#'
#' @usage
#' rekognition_create_collection(CollectionId, Tags)
#'
#' @param CollectionId &#91;required&#93; ID for the collection that you are creating.
#' @param Tags A set of tags (key-value pairs) that you want to attach to the
#' collection.
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   StatusCode = 123,
#'   CollectionArn = "string",
#'   FaceModelVersion = "string"
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$create_collection(
#'   CollectionId = "string",
#'   Tags = list(
#'     "string"
#'   )
#' )
#' ```
#'
#' @examples
#' \dontrun{
#' # This operation creates a Rekognition collection for storing image data.
#' svc$create_collection(
#'   CollectionId = "myphotos"
#' )
#' }
#'
#' @keywords internal
#'
#' @rdname rekognition_create_collection
#'
#' @aliases rekognition_create_collection
rekognition_create_collection <- function(CollectionId, Tags = NULL) {
  op <- new_operation(
    name = "CreateCollection",
    http_method = "POST",
    http_path = "/",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .rekognition$create_collection_input(CollectionId = CollectionId, Tags = Tags)
  output <- .rekognition$create_collection_output()
  config <- get_config()
  svc <- .rekognition$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.rekognition$operations$create_collection <- rekognition_create_collection

#' This operation applies only to Amazon Rekognition Custom Labels
#'
#' @description
#' This operation applies only to Amazon Rekognition Custom Labels.
#' 
#' Creates a new Amazon Rekognition Custom Labels dataset. You can create a
#' dataset by using an Amazon Sagemaker format manifest file or by copying
#' an existing Amazon Rekognition Custom Labels dataset.
#' 
#' To create a training dataset for a project, specify `TRAIN` for the
#' value of `DatasetType`. To create the test dataset for a project,
#' specify `TEST` for the value of `DatasetType`.
#' 
#' The response from [`create_dataset`][rekognition_create_dataset] is the
#' Amazon Resource Name (ARN) for the dataset. Creating a dataset takes a
#' while to complete. Use
#' [`describe_dataset`][rekognition_describe_dataset] to check the current
#' status. The dataset created successfully if the value of `Status` is
#' `CREATE_COMPLETE`.
#' 
#' To check if any non-terminal errors occurred, call
#' [`list_dataset_entries`][rekognition_list_dataset_entries] and check for
#' the presence of `errors` lists in the JSON Lines.
#' 
#' Dataset creation fails if a terminal error occurs (`Status` =
#' `CREATE_FAILED`). Currently, you can't access the terminal error
#' information.
#' 
#' For more information, see Creating dataset in the *Amazon Rekognition
#' Custom Labels Developer Guide*.
#' 
#' This operation requires permissions to perform the
#' `rekognition:CreateDataset` action. If you want to copy an existing
#' dataset, you also require permission to perform the
#' `rekognition:ListDatasetEntries` action.
#'
#' @usage
#' rekognition_create_dataset(DatasetSource, DatasetType, ProjectArn, Tags)
#'
#' @param DatasetSource The source files for the dataset. You can specify the ARN of an existing
#' dataset or specify the Amazon S3 bucket location of an Amazon Sagemaker
#' format manifest file. If you don't specify `datasetSource`, an empty
#' dataset is created. To add labeled images to the dataset, You can use
#' the console or call
#' [`update_dataset_entries`][rekognition_update_dataset_entries].
#' @param DatasetType &#91;required&#93; The type of the dataset. Specify `TRAIN` to create a training dataset.
#' Specify `TEST` to create a test dataset.
#' @param ProjectArn &#91;required&#93; The ARN of the Amazon Rekognition Custom Labels project to which you
#' want to asssign the dataset.
#' @param Tags A set of tags (key-value pairs) that you want to attach to the dataset.
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   DatasetArn = "string"
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$create_dataset(
#'   DatasetSource = list(
#'     GroundTruthManifest = list(
#'       S3Object = list(
#'         Bucket = "string",
#'         Name = "string",
#'         Version = "string"
#'       )
#'     ),
#'     DatasetArn = "string"
#'   ),
#'   DatasetType = "TRAIN"|"TEST",
#'   ProjectArn = "string",
#'   Tags = list(
#'     "string"
#'   )
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname rekognition_create_dataset
#'
#' @aliases rekognition_create_dataset
rekognition_create_dataset <- function(DatasetSource = NULL, DatasetType, ProjectArn, Tags = NULL) {
  op <- new_operation(
    name = "CreateDataset",
    http_method = "POST",
    http_path = "/",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .rekognition$create_dataset_input(DatasetSource = DatasetSource, DatasetType = DatasetType, ProjectArn = ProjectArn, Tags = Tags)
  output <- .rekognition$create_dataset_output()
  config <- get_config()
  svc <- .rekognition$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.rekognition$operations$create_dataset <- rekognition_create_dataset

#' This API operation initiates a Face Liveness session
#'
#' @description
#' This API operation initiates a Face Liveness session. It returns a
#' `SessionId`, which you can use to start streaming Face Liveness video
#' and get the results for a Face Liveness session.
#' 
#' You can use the `OutputConfig` option in the Settings parameter to
#' provide an Amazon S3 bucket location. The Amazon S3 bucket stores
#' reference images and audit images. If no Amazon S3 bucket is defined,
#' raw bytes are sent instead.
#' 
#' You can use `AuditImagesLimit` to limit the number of audit images
#' returned when
#' [`get_face_liveness_session_results`][rekognition_get_face_liveness_session_results]
#' is called. This number is between 0 and 4. By default, it is set to 0.
#' The limit is best effort and based on the duration of the selfie-video.
#'
#' @usage
#' rekognition_create_face_liveness_session(KmsKeyId, Settings,
#'   ClientRequestToken)
#'
#' @param KmsKeyId The identifier for your AWS Key Management Service key (AWS KMS key).
#' Used to encrypt audit images and reference images.
#' @param Settings A session settings object. It contains settings for the operation to be
#' performed. For Face Liveness, it accepts `OutputConfig` and
#' `AuditImagesLimit`.
#' @param ClientRequestToken Idempotent token is used to recognize the Face Liveness request. If the
#' same token is used with multiple
#' [`create_face_liveness_session`][rekognition_create_face_liveness_session]
#' requests, the same session is returned. This token is employed to avoid
#' unintentionally creating the same session multiple times.
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   SessionId = "string"
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$create_face_liveness_session(
#'   KmsKeyId = "string",
#'   Settings = list(
#'     OutputConfig = list(
#'       S3Bucket = "string",
#'       S3KeyPrefix = "string"
#'     ),
#'     AuditImagesLimit = 123,
#'     ChallengePreferences = list(
#'       list(
#'         Type = "FaceMovementAndLightChallenge"|"FaceMovementChallenge",
#'         Versions = list(
#'           Minimum = "string",
#'           Maximum = "string"
#'         )
#'       )
#'     )
#'   ),
#'   ClientRequestToken = "string"
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname rekognition_create_face_liveness_session
#'
#' @aliases rekognition_create_face_liveness_session
rekognition_create_face_liveness_session <- function(KmsKeyId = NULL, Settings = NULL, ClientRequestToken = NULL) {
  op <- new_operation(
    name = "CreateFaceLivenessSession",
    http_method = "POST",
    http_path = "/",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .rekognition$create_face_liveness_session_input(KmsKeyId = KmsKeyId, Settings = Settings, ClientRequestToken = ClientRequestToken)
  output <- .rekognition$create_face_liveness_session_output()
  config <- get_config()
  svc <- .rekognition$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.rekognition$operations$create_face_liveness_session <- rekognition_create_face_liveness_session

#' Creates a new Amazon Rekognition project
#'
#' @description
#' Creates a new Amazon Rekognition project. A project is a group of
#' resources (datasets, model versions) that you use to create and manage a
#' Amazon Rekognition Custom Labels Model or custom adapter. You can
#' specify a feature to create the project with, if no feature is specified
#' then Custom Labels is used by default. For adapters, you can also choose
#' whether or not to have the project auto update by using the AutoUpdate
#' argument. This operation requires permissions to perform the
#' `rekognition:CreateProject` action.
#'
#' @usage
#' rekognition_create_project(ProjectName, Feature, AutoUpdate, Tags)
#'
#' @param ProjectName &#91;required&#93; The name of the project to create.
#' @param Feature Specifies feature that is being customized. If no value is provided
#' CUSTOM_LABELS is used as a default.
#' @param AutoUpdate Specifies whether automatic retraining should be attempted for the
#' versions of the project. Automatic retraining is done as a best effort.
#' Required argument for Content Moderation. Applicable only to adapters.
#' @param Tags A set of tags (key-value pairs) that you want to attach to the project.
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   ProjectArn = "string"
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$create_project(
#'   ProjectName = "string",
#'   Feature = "CONTENT_MODERATION"|"CUSTOM_LABELS",
#'   AutoUpdate = "ENABLED"|"DISABLED",
#'   Tags = list(
#'     "string"
#'   )
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname rekognition_create_project
#'
#' @aliases rekognition_create_project
rekognition_create_project <- function(ProjectName, Feature = NULL, AutoUpdate = NULL, Tags = NULL) {
  op <- new_operation(
    name = "CreateProject",
    http_method = "POST",
    http_path = "/",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .rekognition$create_project_input(ProjectName = ProjectName, Feature = Feature, AutoUpdate = AutoUpdate, Tags = Tags)
  output <- .rekognition$create_project_output()
  config <- get_config()
  svc <- .rekognition$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.rekognition$operations$create_project <- rekognition_create_project

#' Creates a new version of Amazon Rekognition project (like a Custom
#' Labels model or a custom adapter) and begins training
#'
#' @description
#' Creates a new version of Amazon Rekognition project (like a Custom
#' Labels model or a custom adapter) and begins training. Models and
#' adapters are managed as part of a Rekognition project. The response from
#' [`create_project_version`][rekognition_create_project_version] is an
#' Amazon Resource Name (ARN) for the project version.
#' 
#' The FeatureConfig operation argument allows you to configure specific
#' model or adapter settings. You can provide a description to the project
#' version by using the VersionDescription argment. Training can take a
#' while to complete. You can get the current status by calling
#' [`describe_project_versions`][rekognition_describe_project_versions].
#' Training completed successfully if the value of the `Status` field is
#' `TRAINING_COMPLETED`. Once training has successfully completed, call
#' [`describe_project_versions`][rekognition_describe_project_versions] to
#' get the training results and evaluate the model.
#' 
#' This operation requires permissions to perform the
#' `rekognition:CreateProjectVersion` action.
#' 
#' *The following applies only to projects with Amazon Rekognition Custom
#' Labels as the chosen feature:*
#' 
#' You can train a model in a project that doesn't have associated datasets
#' by specifying manifest files in the `TrainingData` and `TestingData`
#' fields.
#' 
#' If you open the console after training a model with manifest files,
#' Amazon Rekognition Custom Labels creates the datasets for you using the
#' most recent manifest files. You can no longer train a model version for
#' the project by specifying manifest files.
#' 
#' Instead of training with a project without associated datasets, we
#' recommend that you use the manifest files to create training and test
#' datasets for the project.
#'
#' @usage
#' rekognition_create_project_version(ProjectArn, VersionName,
#'   OutputConfig, TrainingData, TestingData, Tags, KmsKeyId,
#'   VersionDescription, FeatureConfig)
#'
#' @param ProjectArn &#91;required&#93; The ARN of the Amazon Rekognition project that will manage the project
#' version you want to train.
#' @param VersionName &#91;required&#93; A name for the version of the project version. This value must be
#' unique.
#' @param OutputConfig &#91;required&#93; The Amazon S3 bucket location to store the results of training. The
#' bucket can be any S3 bucket in your AWS account. You need `s3:PutObject`
#' permission on the bucket.
#' @param TrainingData Specifies an external manifest that the services uses to train the
#' project version. If you specify `TrainingData` you must also specify
#' `TestingData`. The project must not have any associated datasets.
#' @param TestingData Specifies an external manifest that the service uses to test the project
#' version. If you specify `TestingData` you must also specify
#' `TrainingData`. The project must not have any associated datasets.
#' @param Tags A set of tags (key-value pairs) that you want to attach to the project
#' version.
#' @param KmsKeyId The identifier for your AWS Key Management Service key (AWS KMS key).
#' You can supply the Amazon Resource Name (ARN) of your KMS key, the ID of
#' your KMS key, an alias for your KMS key, or an alias ARN. The key is
#' used to encrypt training images, test images, and manifest files copied
#' into the service for the project version. Your source images are
#' unaffected. The key is also used to encrypt training results and
#' manifest files written to the output Amazon S3 bucket (`OutputConfig`).
#' 
#' If you choose to use your own KMS key, you need the following
#' permissions on the KMS key.
#' 
#' -   kms:CreateGrant
#' 
#' -   kms:DescribeKey
#' 
#' -   kms:GenerateDataKey
#' 
#' -   kms:Decrypt
#' 
#' If you don't specify a value for `KmsKeyId`, images copied into the
#' service are encrypted using a key that AWS owns and manages.
#' @param VersionDescription A description applied to the project version being created.
#' @param FeatureConfig Feature-specific configuration of the training job. If the job
#' configuration does not match the feature type associated with the
#' project, an InvalidParameterException is returned.
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   ProjectVersionArn = "string"
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$create_project_version(
#'   ProjectArn = "string",
#'   VersionName = "string",
#'   OutputConfig = list(
#'     S3Bucket = "string",
#'     S3KeyPrefix = "string"
#'   ),
#'   TrainingData = list(
#'     Assets = list(
#'       list(
#'         GroundTruthManifest = list(
#'           S3Object = list(
#'             Bucket = "string",
#'             Name = "string",
#'             Version = "string"
#'           )
#'         )
#'       )
#'     )
#'   ),
#'   TestingData = list(
#'     Assets = list(
#'       list(
#'         GroundTruthManifest = list(
#'           S3Object = list(
#'             Bucket = "string",
#'             Name = "string",
#'             Version = "string"
#'           )
#'         )
#'       )
#'     ),
#'     AutoCreate = TRUE|FALSE
#'   ),
#'   Tags = list(
#'     "string"
#'   ),
#'   KmsKeyId = "string",
#'   VersionDescription = "string",
#'   FeatureConfig = list(
#'     ContentModeration = list(
#'       ConfidenceThreshold = 123.0
#'     )
#'   )
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname rekognition_create_project_version
#'
#' @aliases rekognition_create_project_version
rekognition_create_project_version <- function(ProjectArn, VersionName, OutputConfig, TrainingData = NULL, TestingData = NULL, Tags = NULL, KmsKeyId = NULL, VersionDescription = NULL, FeatureConfig = NULL) {
  op <- new_operation(
    name = "CreateProjectVersion",
    http_method = "POST",
    http_path = "/",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .rekognition$create_project_version_input(ProjectArn = ProjectArn, VersionName = VersionName, OutputConfig = OutputConfig, TrainingData = TrainingData, TestingData = TestingData, Tags = Tags, KmsKeyId = KmsKeyId, VersionDescription = VersionDescription, FeatureConfig = FeatureConfig)
  output <- .rekognition$create_project_version_output()
  config <- get_config()
  svc <- .rekognition$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.rekognition$operations$create_project_version <- rekognition_create_project_version

#' Creates an Amazon Rekognition stream processor that you can use to
#' detect and recognize faces or to detect labels in a streaming video
#'
#' @description
#' Creates an Amazon Rekognition stream processor that you can use to
#' detect and recognize faces or to detect labels in a streaming video.
#' 
#' Amazon Rekognition Video is a consumer of live video from Amazon Kinesis
#' Video Streams. There are two different settings for stream processors in
#' Amazon Rekognition: detecting faces and detecting labels.
#' 
#' -   If you are creating a stream processor for detecting faces, you
#'     provide as input a Kinesis video stream (`Input`) and a Kinesis data
#'     stream (`Output`) stream for receiving the output. You must use the
#'     `FaceSearch` option in `Settings`, specifying the collection that
#'     contains the faces you want to recognize. After you have finished
#'     analyzing a streaming video, use
#'     [`stop_stream_processor`][rekognition_stop_stream_processor] to stop
#'     processing.
#' 
#' -   If you are creating a stream processor to detect labels, you provide
#'     as input a Kinesis video stream (`Input`), Amazon S3 bucket
#'     information (`Output`), and an Amazon SNS topic ARN
#'     (`NotificationChannel`). You can also provide a KMS key ID to
#'     encrypt the data sent to your Amazon S3 bucket. You specify what you
#'     want to detect by using the `ConnectedHome` option in settings, and
#'     selecting one of the following: `PERSON`, `PET`, `PACKAGE`, `ALL`
#'     You can also specify where in the frame you want Amazon Rekognition
#'     to monitor with `RegionsOfInterest`. When you run the
#'     [`start_stream_processor`][rekognition_start_stream_processor]
#'     operation on a label detection stream processor, you input start and
#'     stop information to determine the length of the processing time.
#' 
#' Use `Name` to assign an identifier for the stream processor. You use
#' `Name` to manage the stream processor. For example, you can start
#' processing the source video by calling
#' [`start_stream_processor`][rekognition_start_stream_processor] with the
#' `Name` field.
#' 
#' This operation requires permissions to perform the
#' `rekognition:CreateStreamProcessor` action. If you want to tag your
#' stream processor, you also require permission to perform the
#' `rekognition:TagResource` operation.
#'
#' @usage
#' rekognition_create_stream_processor(Input, Output, Name, Settings,
#'   RoleArn, Tags, NotificationChannel, KmsKeyId, RegionsOfInterest,
#'   DataSharingPreference)
#'
#' @param Input &#91;required&#93; Kinesis video stream stream that provides the source streaming video. If
#' you are using the AWS CLI, the parameter name is `StreamProcessorInput`.
#' This is required for both face search and label detection stream
#' processors.
#' @param Output &#91;required&#93; Kinesis data stream stream or Amazon S3 bucket location to which Amazon
#' Rekognition Video puts the analysis results. If you are using the AWS
#' CLI, the parameter name is `StreamProcessorOutput`. This must be a
#' S3Destination of an Amazon S3 bucket that you own for a label detection
#' stream processor or a Kinesis data stream ARN for a face search stream
#' processor.
#' @param Name &#91;required&#93; An identifier you assign to the stream processor. You can use `Name` to
#' manage the stream processor. For example, you can get the current status
#' of the stream processor by calling
#' [`describe_stream_processor`][rekognition_describe_stream_processor].
#' `Name` is idempotent. This is required for both face search and label
#' detection stream processors.
#' @param Settings &#91;required&#93; Input parameters used in a streaming video analyzed by a stream
#' processor. You can use `FaceSearch` to recognize faces in a streaming
#' video, or you can use `ConnectedHome` to detect labels.
#' @param RoleArn &#91;required&#93; The Amazon Resource Number (ARN) of the IAM role that allows access to
#' the stream processor. The IAM role provides Rekognition read permissions
#' for a Kinesis stream. It also provides write permissions to an Amazon S3
#' bucket and Amazon Simple Notification Service topic for a label
#' detection stream processor. This is required for both face search and
#' label detection stream processors.
#' @param Tags A set of tags (key-value pairs) that you want to attach to the stream
#' processor.
#' @param NotificationChannel 
#' @param KmsKeyId The identifier for your AWS Key Management Service key (AWS KMS key).
#' This is an optional parameter for label detection stream processors and
#' should not be used to create a face search stream processor. You can
#' supply the Amazon Resource Name (ARN) of your KMS key, the ID of your
#' KMS key, an alias for your KMS key, or an alias ARN. The key is used to
#' encrypt results and data published to your Amazon S3 bucket, which
#' includes image frames and hero images. Your source images are
#' unaffected.
#' @param RegionsOfInterest Specifies locations in the frames where Amazon Rekognition checks for
#' objects or people. You can specify up to 10 regions of interest, and
#' each region has either a polygon or a bounding box. This is an optional
#' parameter for label detection stream processors and should not be used
#' to create a face search stream processor.
#' @param DataSharingPreference Shows whether you are sharing data with Rekognition to improve model
#' performance. You can choose this option at the account level or on a
#' per-stream basis. Note that if you opt out at the account level this
#' setting is ignored on individual streams.
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   StreamProcessorArn = "string"
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$create_stream_processor(
#'   Input = list(
#'     KinesisVideoStream = list(
#'       Arn = "string"
#'     )
#'   ),
#'   Output = list(
#'     KinesisDataStream = list(
#'       Arn = "string"
#'     ),
#'     S3Destination = list(
#'       Bucket = "string",
#'       KeyPrefix = "string"
#'     )
#'   ),
#'   Name = "string",
#'   Settings = list(
#'     FaceSearch = list(
#'       CollectionId = "string",
#'       FaceMatchThreshold = 123.0
#'     ),
#'     ConnectedHome = list(
#'       Labels = list(
#'         "string"
#'       ),
#'       MinConfidence = 123.0
#'     )
#'   ),
#'   RoleArn = "string",
#'   Tags = list(
#'     "string"
#'   ),
#'   NotificationChannel = list(
#'     SNSTopicArn = "string"
#'   ),
#'   KmsKeyId = "string",
#'   RegionsOfInterest = list(
#'     list(
#'       BoundingBox = list(
#'         Width = 123.0,
#'         Height = 123.0,
#'         Left = 123.0,
#'         Top = 123.0
#'       ),
#'       Polygon = list(
#'         list(
#'           X = 123.0,
#'           Y = 123.0
#'         )
#'       )
#'     )
#'   ),
#'   DataSharingPreference = list(
#'     OptIn = TRUE|FALSE
#'   )
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname rekognition_create_stream_processor
#'
#' @aliases rekognition_create_stream_processor
rekognition_create_stream_processor <- function(Input, Output, Name, Settings, RoleArn, Tags = NULL, NotificationChannel = NULL, KmsKeyId = NULL, RegionsOfInterest = NULL, DataSharingPreference = NULL) {
  op <- new_operation(
    name = "CreateStreamProcessor",
    http_method = "POST",
    http_path = "/",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .rekognition$create_stream_processor_input(Input = Input, Output = Output, Name = Name, Settings = Settings, RoleArn = RoleArn, Tags = Tags, NotificationChannel = NotificationChannel, KmsKeyId = KmsKeyId, RegionsOfInterest = RegionsOfInterest, DataSharingPreference = DataSharingPreference)
  output <- .rekognition$create_stream_processor_output()
  config <- get_config()
  svc <- .rekognition$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.rekognition$operations$create_stream_processor <- rekognition_create_stream_processor

#' Creates a new User within a collection specified by CollectionId
#'
#' @description
#' Creates a new User within a collection specified by `CollectionId`.
#' Takes `UserId` as a parameter, which is a user provided ID which should
#' be unique within the collection. The provided `UserId` will alias the
#' system generated UUID to make the `UserId` more user friendly.
#' 
#' Uses a `ClientToken`, an idempotency token that ensures a call to
#' [`create_user`][rekognition_create_user] completes only once. If the
#' value is not supplied, the AWS SDK generates an idempotency token for
#' the requests. This prevents retries after a network error results from
#' making multiple [`create_user`][rekognition_create_user] calls.
#'
#' @usage
#' rekognition_create_user(CollectionId, UserId, ClientRequestToken)
#'
#' @param CollectionId &#91;required&#93; The ID of an existing collection to which the new UserID needs to be
#' created.
#' @param UserId &#91;required&#93; ID for the UserID to be created. This ID needs to be unique within the
#' collection.
#' @param ClientRequestToken Idempotent token used to identify the request to
#' [`create_user`][rekognition_create_user]. If you use the same token with
#' multiple [`create_user`][rekognition_create_user] requests, the same
#' response is returned. Use ClientRequestToken to prevent the same request
#' from being processed more than once.
#'
#' @return
#' An empty list.
#'
#' @section Request syntax:
#' ```
#' svc$create_user(
#'   CollectionId = "string",
#'   UserId = "string",
#'   ClientRequestToken = "string"
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname rekognition_create_user
#'
#' @aliases rekognition_create_user
rekognition_create_user <- function(CollectionId, UserId, ClientRequestToken = NULL) {
  op <- new_operation(
    name = "CreateUser",
    http_method = "POST",
    http_path = "/",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .rekognition$create_user_input(CollectionId = CollectionId, UserId = UserId, ClientRequestToken = ClientRequestToken)
  output <- .rekognition$create_user_output()
  config <- get_config()
  svc <- .rekognition$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.rekognition$operations$create_user <- rekognition_create_user

#' Deletes the specified collection
#'
#' @description
#' Deletes the specified collection. Note that this operation removes all
#' faces in the collection. For an example, see [Deleting a
#' collection](https://docs.aws.amazon.com/rekognition/latest/dg/delete-collection-procedure.html).
#' 
#' This operation requires permissions to perform the
#' `rekognition:DeleteCollection` action.
#'
#' @usage
#' rekognition_delete_collection(CollectionId)
#'
#' @param CollectionId &#91;required&#93; ID of the collection to delete.
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   StatusCode = 123
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$delete_collection(
#'   CollectionId = "string"
#' )
#' ```
#'
#' @examples
#' \dontrun{
#' # This operation deletes a Rekognition collection.
#' svc$delete_collection(
#'   CollectionId = "myphotos"
#' )
#' }
#'
#' @keywords internal
#'
#' @rdname rekognition_delete_collection
#'
#' @aliases rekognition_delete_collection
rekognition_delete_collection <- function(CollectionId) {
  op <- new_operation(
    name = "DeleteCollection",
    http_method = "POST",
    http_path = "/",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .rekognition$delete_collection_input(CollectionId = CollectionId)
  output <- .rekognition$delete_collection_output()
  config <- get_config()
  svc <- .rekognition$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.rekognition$operations$delete_collection <- rekognition_delete_collection

#' This operation applies only to Amazon Rekognition Custom Labels
#'
#' @description
#' This operation applies only to Amazon Rekognition Custom Labels.
#' 
#' Deletes an existing Amazon Rekognition Custom Labels dataset. Deleting a
#' dataset might take while. Use
#' [`describe_dataset`][rekognition_describe_dataset] to check the current
#' status. The dataset is still deleting if the value of `Status` is
#' `DELETE_IN_PROGRESS`. If you try to access the dataset after it is
#' deleted, you get a `ResourceNotFoundException` exception.
#' 
#' You can't delete a dataset while it is creating (`Status` =
#' `CREATE_IN_PROGRESS`) or if the dataset is updating (`Status` =
#' `UPDATE_IN_PROGRESS`).
#' 
#' This operation requires permissions to perform the
#' `rekognition:DeleteDataset` action.
#'
#' @usage
#' rekognition_delete_dataset(DatasetArn)
#'
#' @param DatasetArn &#91;required&#93; The ARN of the Amazon Rekognition Custom Labels dataset that you want to
#' delete.
#'
#' @return
#' An empty list.
#'
#' @section Request syntax:
#' ```
#' svc$delete_dataset(
#'   DatasetArn = "string"
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname rekognition_delete_dataset
#'
#' @aliases rekognition_delete_dataset
rekognition_delete_dataset <- function(DatasetArn) {
  op <- new_operation(
    name = "DeleteDataset",
    http_method = "POST",
    http_path = "/",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .rekognition$delete_dataset_input(DatasetArn = DatasetArn)
  output <- .rekognition$delete_dataset_output()
  config <- get_config()
  svc <- .rekognition$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.rekognition$operations$delete_dataset <- rekognition_delete_dataset

#' Deletes faces from a collection
#'
#' @description
#' Deletes faces from a collection. You specify a collection ID and an
#' array of face IDs to remove from the collection.
#' 
#' This operation requires permissions to perform the
#' `rekognition:DeleteFaces` action.
#'
#' @usage
#' rekognition_delete_faces(CollectionId, FaceIds)
#'
#' @param CollectionId &#91;required&#93; Collection from which to remove the specific faces.
#' @param FaceIds &#91;required&#93; An array of face IDs to delete.
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   DeletedFaces = list(
#'     "string"
#'   ),
#'   UnsuccessfulFaceDeletions = list(
#'     list(
#'       FaceId = "string",
#'       UserId = "string",
#'       Reasons = list(
#'         "ASSOCIATED_TO_AN_EXISTING_USER"|"FACE_NOT_FOUND"
#'       )
#'     )
#'   )
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$delete_faces(
#'   CollectionId = "string",
#'   FaceIds = list(
#'     "string"
#'   )
#' )
#' ```
#'
#' @examples
#' \dontrun{
#' # This operation deletes one or more faces from a Rekognition collection.
#' svc$delete_faces(
#'   CollectionId = "myphotos",
#'   FaceIds = list(
#'     "ff43d742-0c13-5d16-a3e8-03d3f58e980b"
#'   )
#' )
#' }
#'
#' @keywords internal
#'
#' @rdname rekognition_delete_faces
#'
#' @aliases rekognition_delete_faces
rekognition_delete_faces <- function(CollectionId, FaceIds) {
  op <- new_operation(
    name = "DeleteFaces",
    http_method = "POST",
    http_path = "/",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .rekognition$delete_faces_input(CollectionId = CollectionId, FaceIds = FaceIds)
  output <- .rekognition$delete_faces_output()
  config <- get_config()
  svc <- .rekognition$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.rekognition$operations$delete_faces <- rekognition_delete_faces

#' Deletes a Amazon Rekognition project
#'
#' @description
#' Deletes a Amazon Rekognition project. To delete a project you must first
#' delete all models or adapters associated with the project. To delete a
#' model or adapter, see
#' [`delete_project_version`][rekognition_delete_project_version].
#' 
#' [`delete_project`][rekognition_delete_project] is an asynchronous
#' operation. To check if the project is deleted, call
#' [`describe_projects`][rekognition_describe_projects]. The project is
#' deleted when the project no longer appears in the response. Be aware
#' that deleting a given project will also delete any `ProjectPolicies`
#' associated with that project.
#' 
#' This operation requires permissions to perform the
#' `rekognition:DeleteProject` action.
#'
#' @usage
#' rekognition_delete_project(ProjectArn)
#'
#' @param ProjectArn &#91;required&#93; The Amazon Resource Name (ARN) of the project that you want to delete.
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   Status = "CREATING"|"CREATED"|"DELETING"
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$delete_project(
#'   ProjectArn = "string"
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname rekognition_delete_project
#'
#' @aliases rekognition_delete_project
rekognition_delete_project <- function(ProjectArn) {
  op <- new_operation(
    name = "DeleteProject",
    http_method = "POST",
    http_path = "/",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .rekognition$delete_project_input(ProjectArn = ProjectArn)
  output <- .rekognition$delete_project_output()
  config <- get_config()
  svc <- .rekognition$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.rekognition$operations$delete_project <- rekognition_delete_project

#' This operation applies only to Amazon Rekognition Custom Labels
#'
#' @description
#' This operation applies only to Amazon Rekognition Custom Labels.
#' 
#' Deletes an existing project policy.
#' 
#' To get a list of project policies attached to a project, call
#' [`list_project_policies`][rekognition_list_project_policies]. To attach
#' a project policy to a project, call
#' [`put_project_policy`][rekognition_put_project_policy].
#' 
#' This operation requires permissions to perform the
#' `rekognition:DeleteProjectPolicy` action.
#'
#' @usage
#' rekognition_delete_project_policy(ProjectArn, PolicyName,
#'   PolicyRevisionId)
#'
#' @param ProjectArn &#91;required&#93; The Amazon Resource Name (ARN) of the project that the project policy
#' you want to delete is attached to.
#' @param PolicyName &#91;required&#93; The name of the policy that you want to delete.
#' @param PolicyRevisionId The ID of the project policy revision that you want to delete.
#'
#' @return
#' An empty list.
#'
#' @section Request syntax:
#' ```
#' svc$delete_project_policy(
#'   ProjectArn = "string",
#'   PolicyName = "string",
#'   PolicyRevisionId = "string"
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname rekognition_delete_project_policy
#'
#' @aliases rekognition_delete_project_policy
rekognition_delete_project_policy <- function(ProjectArn, PolicyName, PolicyRevisionId = NULL) {
  op <- new_operation(
    name = "DeleteProjectPolicy",
    http_method = "POST",
    http_path = "/",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .rekognition$delete_project_policy_input(ProjectArn = ProjectArn, PolicyName = PolicyName, PolicyRevisionId = PolicyRevisionId)
  output <- .rekognition$delete_project_policy_output()
  config <- get_config()
  svc <- .rekognition$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.rekognition$operations$delete_project_policy <- rekognition_delete_project_policy

#' Deletes a Rekognition project model or project version, like a Amazon
#' Rekognition Custom Labels model or a custom adapter
#'
#' @description
#' Deletes a Rekognition project model or project version, like a Amazon
#' Rekognition Custom Labels model or a custom adapter.
#' 
#' You can't delete a project version if it is running or if it is
#' training. To check the status of a project version, use the Status field
#' returned from
#' [`describe_project_versions`][rekognition_describe_project_versions]. To
#' stop a project version call
#' [`stop_project_version`][rekognition_stop_project_version]. If the
#' project version is training, wait until it finishes.
#' 
#' This operation requires permissions to perform the
#' `rekognition:DeleteProjectVersion` action.
#'
#' @usage
#' rekognition_delete_project_version(ProjectVersionArn)
#'
#' @param ProjectVersionArn &#91;required&#93; The Amazon Resource Name (ARN) of the project version that you want to
#' delete.
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   Status = "TRAINING_IN_PROGRESS"|"TRAINING_COMPLETED"|"TRAINING_FAILED"|"STARTING"|"RUNNING"|"FAILED"|"STOPPING"|"STOPPED"|"DELETING"|"COPYING_IN_PROGRESS"|"COPYING_COMPLETED"|"COPYING_FAILED"|"DEPRECATED"|"EXPIRED"
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$delete_project_version(
#'   ProjectVersionArn = "string"
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname rekognition_delete_project_version
#'
#' @aliases rekognition_delete_project_version
rekognition_delete_project_version <- function(ProjectVersionArn) {
  op <- new_operation(
    name = "DeleteProjectVersion",
    http_method = "POST",
    http_path = "/",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .rekognition$delete_project_version_input(ProjectVersionArn = ProjectVersionArn)
  output <- .rekognition$delete_project_version_output()
  config <- get_config()
  svc <- .rekognition$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.rekognition$operations$delete_project_version <- rekognition_delete_project_version

#' Deletes the stream processor identified by Name
#'
#' @description
#' Deletes the stream processor identified by `Name`. You assign the value
#' for `Name` when you create the stream processor with
#' [`create_stream_processor`][rekognition_create_stream_processor]. You
#' might not be able to use the same name for a stream processor for a few
#' seconds after calling
#' [`delete_stream_processor`][rekognition_delete_stream_processor].
#'
#' @usage
#' rekognition_delete_stream_processor(Name)
#'
#' @param Name &#91;required&#93; The name of the stream processor you want to delete.
#'
#' @return
#' An empty list.
#'
#' @section Request syntax:
#' ```
#' svc$delete_stream_processor(
#'   Name = "string"
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname rekognition_delete_stream_processor
#'
#' @aliases rekognition_delete_stream_processor
rekognition_delete_stream_processor <- function(Name) {
  op <- new_operation(
    name = "DeleteStreamProcessor",
    http_method = "POST",
    http_path = "/",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .rekognition$delete_stream_processor_input(Name = Name)
  output <- .rekognition$delete_stream_processor_output()
  config <- get_config()
  svc <- .rekognition$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.rekognition$operations$delete_stream_processor <- rekognition_delete_stream_processor

#' Deletes the specified UserID within the collection
#'
#' @description
#' Deletes the specified UserID within the collection. Faces that are
#' associated with the UserID are disassociated from the UserID before
#' deleting the specified UserID. If the specified `Collection` or `UserID`
#' is already deleted or not found, a `ResourceNotFoundException` will be
#' thrown. If the action is successful with a 200 response, an empty HTTP
#' body is returned.
#'
#' @usage
#' rekognition_delete_user(CollectionId, UserId, ClientRequestToken)
#'
#' @param CollectionId &#91;required&#93; The ID of an existing collection from which the UserID needs to be
#' deleted.
#' @param UserId &#91;required&#93; ID for the UserID to be deleted.
#' @param ClientRequestToken Idempotent token used to identify the request to
#' [`delete_user`][rekognition_delete_user]. If you use the same token with
#' multiple [`delete_user`][rekognition_delete_user]requests, the same
#' response is returned. Use ClientRequestToken to prevent the same request
#' from being processed more than once.
#'
#' @return
#' An empty list.
#'
#' @section Request syntax:
#' ```
#' svc$delete_user(
#'   CollectionId = "string",
#'   UserId = "string",
#'   ClientRequestToken = "string"
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname rekognition_delete_user
#'
#' @aliases rekognition_delete_user
rekognition_delete_user <- function(CollectionId, UserId, ClientRequestToken = NULL) {
  op <- new_operation(
    name = "DeleteUser",
    http_method = "POST",
    http_path = "/",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .rekognition$delete_user_input(CollectionId = CollectionId, UserId = UserId, ClientRequestToken = ClientRequestToken)
  output <- .rekognition$delete_user_output()
  config <- get_config()
  svc <- .rekognition$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.rekognition$operations$delete_user <- rekognition_delete_user

#' Describes the specified collection
#'
#' @description
#' Describes the specified collection. You can use
#' [`describe_collection`][rekognition_describe_collection] to get
#' information, such as the number of faces indexed into a collection and
#' the version of the model used by the collection for face detection.
#' 
#' For more information, see Describing a Collection in the Amazon
#' Rekognition Developer Guide.
#'
#' @usage
#' rekognition_describe_collection(CollectionId)
#'
#' @param CollectionId &#91;required&#93; The ID of the collection to describe.
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   FaceCount = 123,
#'   FaceModelVersion = "string",
#'   CollectionARN = "string",
#'   CreationTimestamp = as.POSIXct(
#'     "2015-01-01"
#'   ),
#'   UserCount = 123
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$describe_collection(
#'   CollectionId = "string"
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname rekognition_describe_collection
#'
#' @aliases rekognition_describe_collection
rekognition_describe_collection <- function(CollectionId) {
  op <- new_operation(
    name = "DescribeCollection",
    http_method = "POST",
    http_path = "/",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .rekognition$describe_collection_input(CollectionId = CollectionId)
  output <- .rekognition$describe_collection_output()
  config <- get_config()
  svc <- .rekognition$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.rekognition$operations$describe_collection <- rekognition_describe_collection

#' This operation applies only to Amazon Rekognition Custom Labels
#'
#' @description
#' This operation applies only to Amazon Rekognition Custom Labels.
#' 
#' Describes an Amazon Rekognition Custom Labels dataset. You can get
#' information such as the current status of a dataset and statistics about
#' the images and labels in a dataset.
#' 
#' This operation requires permissions to perform the
#' `rekognition:DescribeDataset` action.
#'
#' @usage
#' rekognition_describe_dataset(DatasetArn)
#'
#' @param DatasetArn &#91;required&#93; The Amazon Resource Name (ARN) of the dataset that you want to describe.
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   DatasetDescription = list(
#'     CreationTimestamp = as.POSIXct(
#'       "2015-01-01"
#'     ),
#'     LastUpdatedTimestamp = as.POSIXct(
#'       "2015-01-01"
#'     ),
#'     Status = "CREATE_IN_PROGRESS"|"CREATE_COMPLETE"|"CREATE_FAILED"|"UPDATE_IN_PROGRESS"|"UPDATE_COMPLETE"|"UPDATE_FAILED"|"DELETE_IN_PROGRESS",
#'     StatusMessage = "string",
#'     StatusMessageCode = "SUCCESS"|"SERVICE_ERROR"|"CLIENT_ERROR",
#'     DatasetStats = list(
#'       LabeledEntries = 123,
#'       TotalEntries = 123,
#'       TotalLabels = 123,
#'       ErrorEntries = 123
#'     )
#'   )
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$describe_dataset(
#'   DatasetArn = "string"
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname rekognition_describe_dataset
#'
#' @aliases rekognition_describe_dataset
rekognition_describe_dataset <- function(DatasetArn) {
  op <- new_operation(
    name = "DescribeDataset",
    http_method = "POST",
    http_path = "/",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .rekognition$describe_dataset_input(DatasetArn = DatasetArn)
  output <- .rekognition$describe_dataset_output()
  config <- get_config()
  svc <- .rekognition$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.rekognition$operations$describe_dataset <- rekognition_describe_dataset

#' Lists and describes the versions of an Amazon Rekognition project
#'
#' @description
#' Lists and describes the versions of an Amazon Rekognition project. You
#' can specify up to 10 model or adapter versions in `ProjectVersionArns`.
#' If you don't specify a value, descriptions for all model/adapter
#' versions in the project are returned.
#' 
#' This operation requires permissions to perform the
#' `rekognition:DescribeProjectVersions` action.
#'
#' @usage
#' rekognition_describe_project_versions(ProjectArn, VersionNames,
#'   NextToken, MaxResults)
#'
#' @param ProjectArn &#91;required&#93; The Amazon Resource Name (ARN) of the project that contains the
#' model/adapter you want to describe.
#' @param VersionNames A list of model or project version names that you want to describe. You
#' can add up to 10 model or project version names to the list. If you
#' don't specify a value, all project version descriptions are returned. A
#' version name is part of a project version ARN. For example,
#' `my-model.2020-01-21T09.10.15` is the version name in the following ARN.
#' `arn:aws:rekognition:us-east-1:123456789012:project/getting-started/version/my-model.2020-01-21T09.10.15/1234567890123`.
#' @param NextToken If the previous response was incomplete (because there is more results
#' to retrieve), Amazon Rekognition returns a pagination token in the
#' response. You can use this pagination token to retrieve the next set of
#' results.
#' @param MaxResults The maximum number of results to return per paginated call. The largest
#' value you can specify is 100. If you specify a value greater than 100, a
#' ValidationException error occurs. The default value is 100.
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   ProjectVersionDescriptions = list(
#'     list(
#'       ProjectVersionArn = "string",
#'       CreationTimestamp = as.POSIXct(
#'         "2015-01-01"
#'       ),
#'       MinInferenceUnits = 123,
#'       Status = "TRAINING_IN_PROGRESS"|"TRAINING_COMPLETED"|"TRAINING_FAILED"|"STARTING"|"RUNNING"|"FAILED"|"STOPPING"|"STOPPED"|"DELETING"|"COPYING_IN_PROGRESS"|"COPYING_COMPLETED"|"COPYING_FAILED"|"DEPRECATED"|"EXPIRED",
#'       StatusMessage = "string",
#'       BillableTrainingTimeInSeconds = 123,
#'       TrainingEndTimestamp = as.POSIXct(
#'         "2015-01-01"
#'       ),
#'       OutputConfig = list(
#'         S3Bucket = "string",
#'         S3KeyPrefix = "string"
#'       ),
#'       TrainingDataResult = list(
#'         Input = list(
#'           Assets = list(
#'             list(
#'               GroundTruthManifest = list(
#'                 S3Object = list(
#'                   Bucket = "string",
#'                   Name = "string",
#'                   Version = "string"
#'                 )
#'               )
#'             )
#'           )
#'         ),
#'         Output = list(
#'           Assets = list(
#'             list(
#'               GroundTruthManifest = list(
#'                 S3Object = list(
#'                   Bucket = "string",
#'                   Name = "string",
#'                   Version = "string"
#'                 )
#'               )
#'             )
#'           )
#'         ),
#'         Validation = list(
#'           Assets = list(
#'             list(
#'               GroundTruthManifest = list(
#'                 S3Object = list(
#'                   Bucket = "string",
#'                   Name = "string",
#'                   Version = "string"
#'                 )
#'               )
#'             )
#'           )
#'         )
#'       ),
#'       TestingDataResult = list(
#'         Input = list(
#'           Assets = list(
#'             list(
#'               GroundTruthManifest = list(
#'                 S3Object = list(
#'                   Bucket = "string",
#'                   Name = "string",
#'                   Version = "string"
#'                 )
#'               )
#'             )
#'           ),
#'           AutoCreate = TRUE|FALSE
#'         ),
#'         Output = list(
#'           Assets = list(
#'             list(
#'               GroundTruthManifest = list(
#'                 S3Object = list(
#'                   Bucket = "string",
#'                   Name = "string",
#'                   Version = "string"
#'                 )
#'               )
#'             )
#'           ),
#'           AutoCreate = TRUE|FALSE
#'         ),
#'         Validation = list(
#'           Assets = list(
#'             list(
#'               GroundTruthManifest = list(
#'                 S3Object = list(
#'                   Bucket = "string",
#'                   Name = "string",
#'                   Version = "string"
#'                 )
#'               )
#'             )
#'           )
#'         )
#'       ),
#'       EvaluationResult = list(
#'         F1Score = 123.0,
#'         Summary = list(
#'           S3Object = list(
#'             Bucket = "string",
#'             Name = "string",
#'             Version = "string"
#'           )
#'         )
#'       ),
#'       ManifestSummary = list(
#'         S3Object = list(
#'           Bucket = "string",
#'           Name = "string",
#'           Version = "string"
#'         )
#'       ),
#'       KmsKeyId = "string",
#'       MaxInferenceUnits = 123,
#'       SourceProjectVersionArn = "string",
#'       VersionDescription = "string",
#'       Feature = "CONTENT_MODERATION"|"CUSTOM_LABELS",
#'       BaseModelVersion = "string",
#'       FeatureConfig = list(
#'         ContentModeration = list(
#'           ConfidenceThreshold = 123.0
#'         )
#'       )
#'     )
#'   ),
#'   NextToken = "string"
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$describe_project_versions(
#'   ProjectArn = "string",
#'   VersionNames = list(
#'     "string"
#'   ),
#'   NextToken = "string",
#'   MaxResults = 123
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname rekognition_describe_project_versions
#'
#' @aliases rekognition_describe_project_versions
rekognition_describe_project_versions <- function(ProjectArn, VersionNames = NULL, NextToken = NULL, MaxResults = NULL) {
  op <- new_operation(
    name = "DescribeProjectVersions",
    http_method = "POST",
    http_path = "/",
    host_prefix = "",
    paginator = list(input_token = "NextToken", limit_key = "MaxResults", output_token = "NextToken", result_key = "ProjectVersionDescriptions"),
    stream_api = FALSE
  )
  input <- .rekognition$describe_project_versions_input(ProjectArn = ProjectArn, VersionNames = VersionNames, NextToken = NextToken, MaxResults = MaxResults)
  output <- .rekognition$describe_project_versions_output()
  config <- get_config()
  svc <- .rekognition$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.rekognition$operations$describe_project_versions <- rekognition_describe_project_versions

#' Gets information about your Rekognition projects
#'
#' @description
#' Gets information about your Rekognition projects.
#' 
#' This operation requires permissions to perform the
#' `rekognition:DescribeProjects` action.
#'
#' @usage
#' rekognition_describe_projects(NextToken, MaxResults, ProjectNames,
#'   Features)
#'
#' @param NextToken If the previous response was incomplete (because there is more results
#' to retrieve), Rekognition returns a pagination token in the response.
#' You can use this pagination token to retrieve the next set of results.
#' @param MaxResults The maximum number of results to return per paginated call. The largest
#' value you can specify is 100. If you specify a value greater than 100, a
#' ValidationException error occurs. The default value is 100.
#' @param ProjectNames A list of the projects that you want Rekognition to describe. If you
#' don't specify a value, the response includes descriptions for all the
#' projects in your AWS account.
#' @param Features Specifies the type of customization to filter projects by. If no value
#' is specified, CUSTOM_LABELS is used as a default.
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   ProjectDescriptions = list(
#'     list(
#'       ProjectArn = "string",
#'       CreationTimestamp = as.POSIXct(
#'         "2015-01-01"
#'       ),
#'       Status = "CREATING"|"CREATED"|"DELETING",
#'       Datasets = list(
#'         list(
#'           CreationTimestamp = as.POSIXct(
#'             "2015-01-01"
#'           ),
#'           DatasetType = "TRAIN"|"TEST",
#'           DatasetArn = "string",
#'           Status = "CREATE_IN_PROGRESS"|"CREATE_COMPLETE"|"CREATE_FAILED"|"UPDATE_IN_PROGRESS"|"UPDATE_COMPLETE"|"UPDATE_FAILED"|"DELETE_IN_PROGRESS",
#'           StatusMessage = "string",
#'           StatusMessageCode = "SUCCESS"|"SERVICE_ERROR"|"CLIENT_ERROR"
#'         )
#'       ),
#'       Feature = "CONTENT_MODERATION"|"CUSTOM_LABELS",
#'       AutoUpdate = "ENABLED"|"DISABLED"
#'     )
#'   ),
#'   NextToken = "string"
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$describe_projects(
#'   NextToken = "string",
#'   MaxResults = 123,
#'   ProjectNames = list(
#'     "string"
#'   ),
#'   Features = list(
#'     "CONTENT_MODERATION"|"CUSTOM_LABELS"
#'   )
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname rekognition_describe_projects
#'
#' @aliases rekognition_describe_projects
rekognition_describe_projects <- function(NextToken = NULL, MaxResults = NULL, ProjectNames = NULL, Features = NULL) {
  op <- new_operation(
    name = "DescribeProjects",
    http_method = "POST",
    http_path = "/",
    host_prefix = "",
    paginator = list(input_token = "NextToken", limit_key = "MaxResults", output_token = "NextToken", result_key = "ProjectDescriptions"),
    stream_api = FALSE
  )
  input <- .rekognition$describe_projects_input(NextToken = NextToken, MaxResults = MaxResults, ProjectNames = ProjectNames, Features = Features)
  output <- .rekognition$describe_projects_output()
  config <- get_config()
  svc <- .rekognition$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.rekognition$operations$describe_projects <- rekognition_describe_projects

#' Provides information about a stream processor created by
#' CreateStreamProcessor
#'
#' @description
#' Provides information about a stream processor created by
#' [`create_stream_processor`][rekognition_create_stream_processor]. You
#' can get information about the input and output streams, the input
#' parameters for the face recognition being performed, and the current
#' status of the stream processor.
#'
#' @usage
#' rekognition_describe_stream_processor(Name)
#'
#' @param Name &#91;required&#93; Name of the stream processor for which you want information.
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   Name = "string",
#'   StreamProcessorArn = "string",
#'   Status = "STOPPED"|"STARTING"|"RUNNING"|"FAILED"|"STOPPING"|"UPDATING",
#'   StatusMessage = "string",
#'   CreationTimestamp = as.POSIXct(
#'     "2015-01-01"
#'   ),
#'   LastUpdateTimestamp = as.POSIXct(
#'     "2015-01-01"
#'   ),
#'   Input = list(
#'     KinesisVideoStream = list(
#'       Arn = "string"
#'     )
#'   ),
#'   Output = list(
#'     KinesisDataStream = list(
#'       Arn = "string"
#'     ),
#'     S3Destination = list(
#'       Bucket = "string",
#'       KeyPrefix = "string"
#'     )
#'   ),
#'   RoleArn = "string",
#'   Settings = list(
#'     FaceSearch = list(
#'       CollectionId = "string",
#'       FaceMatchThreshold = 123.0
#'     ),
#'     ConnectedHome = list(
#'       Labels = list(
#'         "string"
#'       ),
#'       MinConfidence = 123.0
#'     )
#'   ),
#'   NotificationChannel = list(
#'     SNSTopicArn = "string"
#'   ),
#'   KmsKeyId = "string",
#'   RegionsOfInterest = list(
#'     list(
#'       BoundingBox = list(
#'         Width = 123.0,
#'         Height = 123.0,
#'         Left = 123.0,
#'         Top = 123.0
#'       ),
#'       Polygon = list(
#'         list(
#'           X = 123.0,
#'           Y = 123.0
#'         )
#'       )
#'     )
#'   ),
#'   DataSharingPreference = list(
#'     OptIn = TRUE|FALSE
#'   )
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$describe_stream_processor(
#'   Name = "string"
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname rekognition_describe_stream_processor
#'
#' @aliases rekognition_describe_stream_processor
rekognition_describe_stream_processor <- function(Name) {
  op <- new_operation(
    name = "DescribeStreamProcessor",
    http_method = "POST",
    http_path = "/",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .rekognition$describe_stream_processor_input(Name = Name)
  output <- .rekognition$describe_stream_processor_output()
  config <- get_config()
  svc <- .rekognition$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.rekognition$operations$describe_stream_processor <- rekognition_describe_stream_processor

#' This operation applies only to Amazon Rekognition Custom Labels
#'
#' @description
#' This operation applies only to Amazon Rekognition Custom Labels.
#' 
#' Detects custom labels in a supplied image by using an Amazon Rekognition
#' Custom Labels model.
#' 
#' You specify which version of a model version to use by using the
#' `ProjectVersionArn` input parameter.
#' 
#' You pass the input image as base64-encoded image bytes or as a reference
#' to an image in an Amazon S3 bucket. If you use the AWS CLI to call
#' Amazon Rekognition operations, passing image bytes is not supported. The
#' image must be either a PNG or JPEG formatted file.
#' 
#' For each object that the model version detects on an image, the API
#' returns a (`CustomLabel`) object in an array (`CustomLabels`). Each
#' `CustomLabel` object provides the label name (`Name`), the level of
#' confidence that the image contains the object (`Confidence`), and object
#' location information, if it exists, for the label on the image
#' (`Geometry`).
#' 
#' To filter labels that are returned, specify a value for `MinConfidence`.
#' `DetectCustomLabelsLabels` only returns labels with a confidence that's
#' higher than the specified value. The value of `MinConfidence` maps to
#' the assumed threshold values created during training. For more
#' information, see *Assumed threshold* in the Amazon Rekognition Custom
#' Labels Developer Guide. Amazon Rekognition Custom Labels metrics
#' expresses an assumed threshold as a floating point value between 0-1.
#' The range of `MinConfidence` normalizes the threshold value to a
#' percentage value (0-100). Confidence responses from
#' [`detect_custom_labels`][rekognition_detect_custom_labels] are also
#' returned as a percentage. You can use `MinConfidence` to change the
#' precision and recall or your model. For more information, see *Analyzing
#' an image* in the Amazon Rekognition Custom Labels Developer Guide.
#' 
#' If you don't specify a value for `MinConfidence`,
#' [`detect_custom_labels`][rekognition_detect_custom_labels] returns
#' labels based on the assumed threshold of each label.
#' 
#' This is a stateless API operation. That is, the operation does not
#' persist any data.
#' 
#' This operation requires permissions to perform the
#' `rekognition:DetectCustomLabels` action.
#' 
#' For more information, see *Analyzing an image* in the Amazon Rekognition
#' Custom Labels Developer Guide.
#'
#' @usage
#' rekognition_detect_custom_labels(ProjectVersionArn, Image, MaxResults,
#'   MinConfidence)
#'
#' @param ProjectVersionArn &#91;required&#93; The ARN of the model version that you want to use. Only models
#' associated with Custom Labels projects accepted by the operation. If a
#' provided ARN refers to a model version associated with a project for a
#' different feature type, then an InvalidParameterException is returned.
#' @param Image &#91;required&#93; 
#' @param MaxResults Maximum number of results you want the service to return in the
#' response. The service returns the specified number of highest confidence
#' labels ranked from highest confidence to lowest.
#' @param MinConfidence Specifies the minimum confidence level for the labels to return.
#' [`detect_custom_labels`][rekognition_detect_custom_labels] doesn't
#' return any labels with a confidence value that's lower than this
#' specified value. If you specify a value of 0,
#' [`detect_custom_labels`][rekognition_detect_custom_labels] returns all
#' labels, regardless of the assumed threshold applied to each label. If
#' you don't specify a value for `MinConfidence`,
#' [`detect_custom_labels`][rekognition_detect_custom_labels] returns
#' labels based on the assumed threshold of each label.
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   CustomLabels = list(
#'     list(
#'       Name = "string",
#'       Confidence = 123.0,
#'       Geometry = list(
#'         BoundingBox = list(
#'           Width = 123.0,
#'           Height = 123.0,
#'           Left = 123.0,
#'           Top = 123.0
#'         ),
#'         Polygon = list(
#'           list(
#'             X = 123.0,
#'             Y = 123.0
#'           )
#'         )
#'       )
#'     )
#'   )
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$detect_custom_labels(
#'   ProjectVersionArn = "string",
#'   Image = list(
#'     Bytes = raw,
#'     S3Object = list(
#'       Bucket = "string",
#'       Name = "string",
#'       Version = "string"
#'     )
#'   ),
#'   MaxResults = 123,
#'   MinConfidence = 123.0
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname rekognition_detect_custom_labels
#'
#' @aliases rekognition_detect_custom_labels
rekognition_detect_custom_labels <- function(ProjectVersionArn, Image, MaxResults = NULL, MinConfidence = NULL) {
  op <- new_operation(
    name = "DetectCustomLabels",
    http_method = "POST",
    http_path = "/",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .rekognition$detect_custom_labels_input(ProjectVersionArn = ProjectVersionArn, Image = Image, MaxResults = MaxResults, MinConfidence = MinConfidence)
  output <- .rekognition$detect_custom_labels_output()
  config <- get_config()
  svc <- .rekognition$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.rekognition$operations$detect_custom_labels <- rekognition_detect_custom_labels

#' Detects faces within an image that is provided as input
#'
#' @description
#' Detects faces within an image that is provided as input.
#' 
#' [`detect_faces`][rekognition_detect_faces] detects the 100 largest faces
#' in the image. For each face detected, the operation returns face
#' details. These details include a bounding box of the face, a confidence
#' value (that the bounding box contains a face), and a fixed set of
#' attributes such as facial landmarks (for example, coordinates of eye and
#' mouth), pose, presence of facial occlusion, and so on.
#' 
#' The face-detection algorithm is most effective on frontal faces. For
#' non-frontal or obscured faces, the algorithm might not detect the faces
#' or might detect faces with lower confidence.
#' 
#' You pass the input image either as base64-encoded image bytes or as a
#' reference to an image in an Amazon S3 bucket. If you use the AWS CLI to
#' call Amazon Rekognition operations, passing image bytes is not
#' supported. The image must be either a PNG or JPEG formatted file.
#' 
#' This is a stateless API operation. That is, the operation does not
#' persist any data.
#' 
#' This operation requires permissions to perform the
#' `rekognition:DetectFaces` action.
#'
#' @usage
#' rekognition_detect_faces(Image, Attributes)
#'
#' @param Image &#91;required&#93; The input image as base64-encoded bytes or an S3 object. If you use the
#' AWS CLI to call Amazon Rekognition operations, passing base64-encoded
#' image bytes is not supported.
#' 
#' If you are using an AWS SDK to call Amazon Rekognition, you might not
#' need to base64-encode image bytes passed using the `Bytes` field. For
#' more information, see Images in the Amazon Rekognition developer guide.
#' @param Attributes An array of facial attributes you want to be returned. A `DEFAULT`
#' subset of facial attributes - `BoundingBox`, `Confidence`, `Pose`,
#' `Quality`, and `Landmarks` - will always be returned. You can request
#' for specific facial attributes (in addition to the default list) - by
#' using \[`"DEFAULT", "FACE_OCCLUDED"`\] or just \[`"FACE_OCCLUDED"`\].
#' You can request for all facial attributes by using \[`"ALL"]`.
#' Requesting more attributes may increase response time.
#' 
#' If you provide both, `["ALL", "DEFAULT"]`, the service uses a logical
#' "AND" operator to determine which attributes to return (in this case,
#' all attributes).
#' 
#' Note that while the FaceOccluded and EyeDirection attributes are
#' supported when using [`detect_faces`][rekognition_detect_faces], they
#' aren't supported when analyzing videos with
#' [`start_face_detection`][rekognition_start_face_detection] and
#' [`get_face_detection`][rekognition_get_face_detection].
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   FaceDetails = list(
#'     list(
#'       BoundingBox = list(
#'         Width = 123.0,
#'         Height = 123.0,
#'         Left = 123.0,
#'         Top = 123.0
#'       ),
#'       AgeRange = list(
#'         Low = 123,
#'         High = 123
#'       ),
#'       Smile = list(
#'         Value = TRUE|FALSE,
#'         Confidence = 123.0
#'       ),
#'       Eyeglasses = list(
#'         Value = TRUE|FALSE,
#'         Confidence = 123.0
#'       ),
#'       Sunglasses = list(
#'         Value = TRUE|FALSE,
#'         Confidence = 123.0
#'       ),
#'       Gender = list(
#'         Value = "Male"|"Female",
#'         Confidence = 123.0
#'       ),
#'       Beard = list(
#'         Value = TRUE|FALSE,
#'         Confidence = 123.0
#'       ),
#'       Mustache = list(
#'         Value = TRUE|FALSE,
#'         Confidence = 123.0
#'       ),
#'       EyesOpen = list(
#'         Value = TRUE|FALSE,
#'         Confidence = 123.0
#'       ),
#'       MouthOpen = list(
#'         Value = TRUE|FALSE,
#'         Confidence = 123.0
#'       ),
#'       Emotions = list(
#'         list(
#'           Type = "HAPPY"|"SAD"|"ANGRY"|"CONFUSED"|"DISGUSTED"|"SURPRISED"|"CALM"|"UNKNOWN"|"FEAR",
#'           Confidence = 123.0
#'         )
#'       ),
#'       Landmarks = list(
#'         list(
#'           Type = "eyeLeft"|"eyeRight"|"nose"|"mouthLeft"|"mouthRight"|"leftEyeBrowLeft"|"leftEyeBrowRight"|"leftEyeBrowUp"|"rightEyeBrowLeft"|"rightEyeBrowRight"|"rightEyeBrowUp"|"leftEyeLeft"|"leftEyeRight"|"leftEyeUp"|"leftEyeDown"|"rightEyeLeft"|"rightEyeRight"|"rightEyeUp"|"rightEyeDown"|"noseLeft"|"noseRight"|"mouthUp"|"mouthDown"|"leftPupil"|"rightPupil"|"upperJawlineLeft"|"midJawlineLeft"|"chinBottom"|"midJawlineRight"|"upperJawlineRight",
#'           X = 123.0,
#'           Y = 123.0
#'         )
#'       ),
#'       Pose = list(
#'         Roll = 123.0,
#'         Yaw = 123.0,
#'         Pitch = 123.0
#'       ),
#'       Quality = list(
#'         Brightness = 123.0,
#'         Sharpness = 123.0
#'       ),
#'       Confidence = 123.0,
#'       FaceOccluded = list(
#'         Value = TRUE|FALSE,
#'         Confidence = 123.0
#'       ),
#'       EyeDirection = list(
#'         Yaw = 123.0,
#'         Pitch = 123.0,
#'         Confidence = 123.0
#'       )
#'     )
#'   ),
#'   OrientationCorrection = "ROTATE_0"|"ROTATE_90"|"ROTATE_180"|"ROTATE_270"
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$detect_faces(
#'   Image = list(
#'     Bytes = raw,
#'     S3Object = list(
#'       Bucket = "string",
#'       Name = "string",
#'       Version = "string"
#'     )
#'   ),
#'   Attributes = list(
#'     "DEFAULT"|"ALL"|"AGE_RANGE"|"BEARD"|"EMOTIONS"|"EYE_DIRECTION"|"EYEGLASSES"|"EYES_OPEN"|"GENDER"|"MOUTH_OPEN"|"MUSTACHE"|"FACE_OCCLUDED"|"SMILE"|"SUNGLASSES"
#'   )
#' )
#' ```
#'
#' @examples
#' \dontrun{
#' # This operation detects faces in an image stored in an AWS S3 bucket.
#' svc$detect_faces(
#'   Image = list(
#'     S3Object = list(
#'       Bucket = "mybucket",
#'       Name = "myphoto"
#'     )
#'   )
#' )
#' }
#'
#' @keywords internal
#'
#' @rdname rekognition_detect_faces
#'
#' @aliases rekognition_detect_faces
rekognition_detect_faces <- function(Image, Attributes = NULL) {
  op <- new_operation(
    name = "DetectFaces",
    http_method = "POST",
    http_path = "/",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .rekognition$detect_faces_input(Image = Image, Attributes = Attributes)
  output <- .rekognition$detect_faces_output()
  config <- get_config()
  svc <- .rekognition$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.rekognition$operations$detect_faces <- rekognition_detect_faces

#' Detects instances of real-world entities within an image (JPEG or PNG)
#' provided as input
#'
#' @description
#' Detects instances of real-world entities within an image (JPEG or PNG)
#' provided as input. This includes objects like flower, tree, and table;
#' events like wedding, graduation, and birthday party; and concepts like
#' landscape, evening, and nature.
#' 
#' For an example, see Analyzing images stored in an Amazon S3 bucket in
#' the Amazon Rekognition Developer Guide.
#' 
#' You pass the input image as base64-encoded image bytes or as a reference
#' to an image in an Amazon S3 bucket. If you use the AWS CLI to call
#' Amazon Rekognition operations, passing image bytes is not supported. The
#' image must be either a PNG or JPEG formatted file.
#' 
#' **Optional Parameters**
#' 
#' You can specify one or both of the `GENERAL_LABELS` and
#' `IMAGE_PROPERTIES` feature types when calling the DetectLabels API.
#' Including `GENERAL_LABELS` will ensure the response includes the labels
#' detected in the input image, while including `IMAGE_PROPERTIES `will
#' ensure the response includes information about the image quality and
#' color.
#' 
#' When using `GENERAL_LABELS` and/or `IMAGE_PROPERTIES` you can provide
#' filtering criteria to the Settings parameter. You can filter with sets
#' of individual labels or with label categories. You can specify inclusive
#' filters, exclusive filters, or a combination of inclusive and exclusive
#' filters. For more information on filtering see [Detecting Labels in an
#' Image](https://docs.aws.amazon.com/rekognition/latest/dg/labels-detect-labels-image.html).
#' 
#' When getting labels, you can specify `MinConfidence` to control the
#' confidence threshold for the labels returned. The default is 55%. You
#' can also add the `MaxLabels` parameter to limit the number of labels
#' returned. The default and upper limit is 1000 labels. These arguments
#' are only valid when supplying GENERAL_LABELS as a feature type.
#' 
#' **Response Elements**
#' 
#' For each object, scene, and concept the API returns one or more labels.
#' The API returns the following types of information about labels:
#' 
#' -   Name - The name of the detected label.
#' 
#' -   Confidence - The level of confidence in the label assigned to a
#'     detected object.
#' 
#' -   Parents - The ancestor labels for a detected label. DetectLabels
#'     returns a hierarchical taxonomy of detected labels. For example, a
#'     detected car might be assigned the label car. The label car has two
#'     parent labels: Vehicle (its parent) and Transportation (its
#'     grandparent). The response includes the all ancestors for a label,
#'     where every ancestor is a unique label. In the previous example,
#'     Car, Vehicle, and Transportation are returned as unique labels in
#'     the response.
#' 
#' -   Aliases - Possible Aliases for the label.
#' 
#' -   Categories - The label categories that the detected label belongs
#'     to.
#' 
#' -   BoundingBox — Bounding boxes are described for all instances of
#'     detected common object labels, returned in an array of Instance
#'     objects. An Instance object contains a BoundingBox object,
#'     describing the location of the label on the input image. It also
#'     includes the confidence for the accuracy of the detected bounding
#'     box.
#' 
#' The API returns the following information regarding the image, as part
#' of the ImageProperties structure:
#' 
#' -   Quality - Information about the Sharpness, Brightness, and Contrast
#'     of the input image, scored between 0 to 100. Image quality is
#'     returned for the entire image, as well as the background and the
#'     foreground.
#' 
#' -   Dominant Color - An array of the dominant colors in the image.
#' 
#' -   Foreground - Information about the sharpness, brightness, and
#'     dominant colors of the input image’s foreground.
#' 
#' -   Background - Information about the sharpness, brightness, and
#'     dominant colors of the input image’s background.
#' 
#' The list of returned labels will include at least one label for every
#' detected object, along with information about that label. In the
#' following example, suppose the input image has a lighthouse, the sea,
#' and a rock. The response includes all three labels, one for each object,
#' as well as the confidence in the label:
#' 
#' `{Name: lighthouse, Confidence: 98.4629}`
#' 
#' `{Name: rock,Confidence: 79.2097}`
#' 
#' ` {Name: sea,Confidence: 75.061}`
#' 
#' The list of labels can include multiple labels for the same object. For
#' example, if the input image shows a flower (for example, a tulip), the
#' operation might return the following three labels.
#' 
#' `{Name: flower,Confidence: 99.0562}`
#' 
#' `{Name: plant,Confidence: 99.0562}`
#' 
#' `{Name: tulip,Confidence: 99.0562}`
#' 
#' In this example, the detection algorithm more precisely identifies the
#' flower as a tulip.
#' 
#' If the object detected is a person, the operation doesn't provide the
#' same facial details that the [`detect_faces`][rekognition_detect_faces]
#' operation provides.
#' 
#' This is a stateless API operation that doesn't return any data.
#' 
#' This operation requires permissions to perform the
#' `rekognition:DetectLabels` action.
#'
#' @usage
#' rekognition_detect_labels(Image, MaxLabels, MinConfidence, Features,
#'   Settings)
#'
#' @param Image &#91;required&#93; The input image as base64-encoded bytes or an S3 object. If you use the
#' AWS CLI to call Amazon Rekognition operations, passing image bytes is
#' not supported. Images stored in an S3 Bucket do not need to be
#' base64-encoded.
#' 
#' If you are using an AWS SDK to call Amazon Rekognition, you might not
#' need to base64-encode image bytes passed using the `Bytes` field. For
#' more information, see Images in the Amazon Rekognition developer guide.
#' @param MaxLabels Maximum number of labels you want the service to return in the response.
#' The service returns the specified number of highest confidence labels.
#' Only valid when GENERAL_LABELS is specified as a feature type in the
#' Feature input parameter.
#' @param MinConfidence Specifies the minimum confidence level for the labels to return. Amazon
#' Rekognition doesn't return any labels with confidence lower than this
#' specified value.
#' 
#' If `MinConfidence` is not specified, the operation returns labels with a
#' confidence values greater than or equal to 55 percent. Only valid when
#' GENERAL_LABELS is specified as a feature type in the Feature input
#' parameter.
#' @param Features A list of the types of analysis to perform. Specifying GENERAL_LABELS
#' uses the label detection feature, while specifying IMAGE_PROPERTIES
#' returns information regarding image color and quality. If no option is
#' specified GENERAL_LABELS is used by default.
#' @param Settings A list of the filters to be applied to returned detected labels and
#' image properties. Specified filters can be inclusive, exclusive, or a
#' combination of both. Filters can be used for individual labels or label
#' categories. The exact label names or label categories must be supplied.
#' For a full list of labels and label categories, see [Detecting
#' labels](https://docs.aws.amazon.com/rekognition/latest/dg/labels.html).
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   Labels = list(
#'     list(
#'       Name = "string",
#'       Confidence = 123.0,
#'       Instances = list(
#'         list(
#'           BoundingBox = list(
#'             Width = 123.0,
#'             Height = 123.0,
#'             Left = 123.0,
#'             Top = 123.0
#'           ),
#'           Confidence = 123.0,
#'           DominantColors = list(
#'             list(
#'               Red = 123,
#'               Blue = 123,
#'               Green = 123,
#'               HexCode = "string",
#'               CSSColor = "string",
#'               SimplifiedColor = "string",
#'               PixelPercent = 123.0
#'             )
#'           )
#'         )
#'       ),
#'       Parents = list(
#'         list(
#'           Name = "string"
#'         )
#'       ),
#'       Aliases = list(
#'         list(
#'           Name = "string"
#'         )
#'       ),
#'       Categories = list(
#'         list(
#'           Name = "string"
#'         )
#'       )
#'     )
#'   ),
#'   OrientationCorrection = "ROTATE_0"|"ROTATE_90"|"ROTATE_180"|"ROTATE_270",
#'   LabelModelVersion = "string",
#'   ImageProperties = list(
#'     Quality = list(
#'       Brightness = 123.0,
#'       Sharpness = 123.0,
#'       Contrast = 123.0
#'     ),
#'     DominantColors = list(
#'       list(
#'         Red = 123,
#'         Blue = 123,
#'         Green = 123,
#'         HexCode = "string",
#'         CSSColor = "string",
#'         SimplifiedColor = "string",
#'         PixelPercent = 123.0
#'       )
#'     ),
#'     Foreground = list(
#'       Quality = list(
#'         Brightness = 123.0,
#'         Sharpness = 123.0,
#'         Contrast = 123.0
#'       ),
#'       DominantColors = list(
#'         list(
#'           Red = 123,
#'           Blue = 123,
#'           Green = 123,
#'           HexCode = "string",
#'           CSSColor = "string",
#'           SimplifiedColor = "string",
#'           PixelPercent = 123.0
#'         )
#'       )
#'     ),
#'     Background = list(
#'       Quality = list(
#'         Brightness = 123.0,
#'         Sharpness = 123.0,
#'         Contrast = 123.0
#'       ),
#'       DominantColors = list(
#'         list(
#'           Red = 123,
#'           Blue = 123,
#'           Green = 123,
#'           HexCode = "string",
#'           CSSColor = "string",
#'           SimplifiedColor = "string",
#'           PixelPercent = 123.0
#'         )
#'       )
#'     )
#'   )
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$detect_labels(
#'   Image = list(
#'     Bytes = raw,
#'     S3Object = list(
#'       Bucket = "string",
#'       Name = "string",
#'       Version = "string"
#'     )
#'   ),
#'   MaxLabels = 123,
#'   MinConfidence = 123.0,
#'   Features = list(
#'     "GENERAL_LABELS"|"IMAGE_PROPERTIES"
#'   ),
#'   Settings = list(
#'     GeneralLabels = list(
#'       LabelInclusionFilters = list(
#'         "string"
#'       ),
#'       LabelExclusionFilters = list(
#'         "string"
#'       ),
#'       LabelCategoryInclusionFilters = list(
#'         "string"
#'       ),
#'       LabelCategoryExclusionFilters = list(
#'         "string"
#'       )
#'     ),
#'     ImageProperties = list(
#'       MaxDominantColors = 123
#'     )
#'   )
#' )
#' ```
#'
#' @examples
#' \dontrun{
#' # This operation detects labels in the supplied image
#' svc$detect_labels(
#'   Image = list(
#'     S3Object = list(
#'       Bucket = "mybucket",
#'       Name = "myphoto"
#'     )
#'   ),
#'   MaxLabels = 123L,
#'   MinConfidence = 70L
#' )
#' }
#'
#' @keywords internal
#'
#' @rdname rekognition_detect_labels
#'
#' @aliases rekognition_detect_labels
rekognition_detect_labels <- function(Image, MaxLabels = NULL, MinConfidence = NULL, Features = NULL, Settings = NULL) {
  op <- new_operation(
    name = "DetectLabels",
    http_method = "POST",
    http_path = "/",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .rekognition$detect_labels_input(Image = Image, MaxLabels = MaxLabels, MinConfidence = MinConfidence, Features = Features, Settings = Settings)
  output <- .rekognition$detect_labels_output()
  config <- get_config()
  svc <- .rekognition$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.rekognition$operations$detect_labels <- rekognition_detect_labels

#' Detects unsafe content in a specified JPEG or PNG format image
#'
#' @description
#' Detects unsafe content in a specified JPEG or PNG format image. Use
#' [`detect_moderation_labels`][rekognition_detect_moderation_labels] to
#' moderate images depending on your requirements. For example, you might
#' want to filter images that contain nudity, but not images containing
#' suggestive content.
#' 
#' To filter images, use the labels returned by
#' [`detect_moderation_labels`][rekognition_detect_moderation_labels] to
#' determine which types of content are appropriate.
#' 
#' For information about moderation labels, see Detecting Unsafe Content in
#' the Amazon Rekognition Developer Guide.
#' 
#' You pass the input image either as base64-encoded image bytes or as a
#' reference to an image in an Amazon S3 bucket. If you use the AWS CLI to
#' call Amazon Rekognition operations, passing image bytes is not
#' supported. The image must be either a PNG or JPEG formatted file.
#' 
#' You can specify an adapter to use when retrieving label predictions by
#' providing a `ProjectVersionArn` to the `ProjectVersion` argument.
#'
#' @usage
#' rekognition_detect_moderation_labels(Image, MinConfidence,
#'   HumanLoopConfig, ProjectVersion)
#'
#' @param Image &#91;required&#93; The input image as base64-encoded bytes or an S3 object. If you use the
#' AWS CLI to call Amazon Rekognition operations, passing base64-encoded
#' image bytes is not supported.
#' 
#' If you are using an AWS SDK to call Amazon Rekognition, you might not
#' need to base64-encode image bytes passed using the `Bytes` field. For
#' more information, see Images in the Amazon Rekognition developer guide.
#' @param MinConfidence Specifies the minimum confidence level for the labels to return. Amazon
#' Rekognition doesn't return any labels with a confidence level lower than
#' this specified value.
#' 
#' If you don't specify `MinConfidence`, the operation returns labels with
#' confidence values greater than or equal to 50 percent.
#' @param HumanLoopConfig Sets up the configuration for human evaluation, including the
#' FlowDefinition the image will be sent to.
#' @param ProjectVersion Identifier for the custom adapter. Expects the ProjectVersionArn as a
#' value. Use the CreateProject or CreateProjectVersion APIs to create a
#' custom adapter.
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   ModerationLabels = list(
#'     list(
#'       Confidence = 123.0,
#'       Name = "string",
#'       ParentName = "string",
#'       TaxonomyLevel = 123
#'     )
#'   ),
#'   ModerationModelVersion = "string",
#'   HumanLoopActivationOutput = list(
#'     HumanLoopArn = "string",
#'     HumanLoopActivationReasons = list(
#'       "string"
#'     ),
#'     HumanLoopActivationConditionsEvaluationResults = "string"
#'   ),
#'   ProjectVersion = "string",
#'   ContentTypes = list(
#'     list(
#'       Confidence = 123.0,
#'       Name = "string"
#'     )
#'   )
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$detect_moderation_labels(
#'   Image = list(
#'     Bytes = raw,
#'     S3Object = list(
#'       Bucket = "string",
#'       Name = "string",
#'       Version = "string"
#'     )
#'   ),
#'   MinConfidence = 123.0,
#'   HumanLoopConfig = list(
#'     HumanLoopName = "string",
#'     FlowDefinitionArn = "string",
#'     DataAttributes = list(
#'       ContentClassifiers = list(
#'         "FreeOfPersonallyIdentifiableInformation"|"FreeOfAdultContent"
#'       )
#'     )
#'   ),
#'   ProjectVersion = "string"
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname rekognition_detect_moderation_labels
#'
#' @aliases rekognition_detect_moderation_labels
rekognition_detect_moderation_labels <- function(Image, MinConfidence = NULL, HumanLoopConfig = NULL, ProjectVersion = NULL) {
  op <- new_operation(
    name = "DetectModerationLabels",
    http_method = "POST",
    http_path = "/",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .rekognition$detect_moderation_labels_input(Image = Image, MinConfidence = MinConfidence, HumanLoopConfig = HumanLoopConfig, ProjectVersion = ProjectVersion)
  output <- .rekognition$detect_moderation_labels_output()
  config <- get_config()
  svc <- .rekognition$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.rekognition$operations$detect_moderation_labels <- rekognition_detect_moderation_labels

#' Detects Personal Protective Equipment (PPE) worn by people detected in
#' an image
#'
#' @description
#' Detects Personal Protective Equipment (PPE) worn by people detected in
#' an image. Amazon Rekognition can detect the following types of PPE.
#' 
#' -   Face cover
#' 
#' -   Hand cover
#' 
#' -   Head cover
#' 
#' You pass the input image as base64-encoded image bytes or as a reference
#' to an image in an Amazon S3 bucket. The image must be either a PNG or
#' JPG formatted file.
#' 
#' [`detect_protective_equipment`][rekognition_detect_protective_equipment]
#' detects PPE worn by up to 15 persons detected in an image.
#' 
#' For each person detected in the image the API returns an array of body
#' parts (face, head, left-hand, right-hand). For each body part, an array
#' of detected items of PPE is returned, including an indicator of whether
#' or not the PPE covers the body part. The API returns the confidence it
#' has in each detection (person, PPE, body part and body part coverage).
#' It also returns a bounding box (BoundingBox) for each detected person
#' and each detected item of PPE.
#' 
#' You can optionally request a summary of detected PPE items with the
#' `SummarizationAttributes` input parameter. The summary provides the
#' following information.
#' 
#' -   The persons detected as wearing all of the types of PPE that you
#'     specify.
#' 
#' -   The persons detected as not wearing all of the types PPE that you
#'     specify.
#' 
#' -   The persons detected where PPE adornment could not be determined.
#' 
#' This is a stateless API operation. That is, the operation does not
#' persist any data.
#' 
#' This operation requires permissions to perform the
#' `rekognition:DetectProtectiveEquipment` action.
#'
#' @usage
#' rekognition_detect_protective_equipment(Image, SummarizationAttributes)
#'
#' @param Image &#91;required&#93; The image in which you want to detect PPE on detected persons. The image
#' can be passed as image bytes or you can reference an image stored in an
#' Amazon S3 bucket.
#' @param SummarizationAttributes An array of PPE types that you want to summarize.
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   ProtectiveEquipmentModelVersion = "string",
#'   Persons = list(
#'     list(
#'       BodyParts = list(
#'         list(
#'           Name = "FACE"|"HEAD"|"LEFT_HAND"|"RIGHT_HAND",
#'           Confidence = 123.0,
#'           EquipmentDetections = list(
#'             list(
#'               BoundingBox = list(
#'                 Width = 123.0,
#'                 Height = 123.0,
#'                 Left = 123.0,
#'                 Top = 123.0
#'               ),
#'               Confidence = 123.0,
#'               Type = "FACE_COVER"|"HAND_COVER"|"HEAD_COVER",
#'               CoversBodyPart = list(
#'                 Confidence = 123.0,
#'                 Value = TRUE|FALSE
#'               )
#'             )
#'           )
#'         )
#'       ),
#'       BoundingBox = list(
#'         Width = 123.0,
#'         Height = 123.0,
#'         Left = 123.0,
#'         Top = 123.0
#'       ),
#'       Confidence = 123.0,
#'       Id = 123
#'     )
#'   ),
#'   Summary = list(
#'     PersonsWithRequiredEquipment = list(
#'       123
#'     ),
#'     PersonsWithoutRequiredEquipment = list(
#'       123
#'     ),
#'     PersonsIndeterminate = list(
#'       123
#'     )
#'   )
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$detect_protective_equipment(
#'   Image = list(
#'     Bytes = raw,
#'     S3Object = list(
#'       Bucket = "string",
#'       Name = "string",
#'       Version = "string"
#'     )
#'   ),
#'   SummarizationAttributes = list(
#'     MinConfidence = 123.0,
#'     RequiredEquipmentTypes = list(
#'       "FACE_COVER"|"HAND_COVER"|"HEAD_COVER"
#'     )
#'   )
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname rekognition_detect_protective_equipment
#'
#' @aliases rekognition_detect_protective_equipment
rekognition_detect_protective_equipment <- function(Image, SummarizationAttributes = NULL) {
  op <- new_operation(
    name = "DetectProtectiveEquipment",
    http_method = "POST",
    http_path = "/",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .rekognition$detect_protective_equipment_input(Image = Image, SummarizationAttributes = SummarizationAttributes)
  output <- .rekognition$detect_protective_equipment_output()
  config <- get_config()
  svc <- .rekognition$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.rekognition$operations$detect_protective_equipment <- rekognition_detect_protective_equipment

#' Detects text in the input image and converts it into machine-readable
#' text
#'
#' @description
#' Detects text in the input image and converts it into machine-readable
#' text.
#' 
#' Pass the input image as base64-encoded image bytes or as a reference to
#' an image in an Amazon S3 bucket. If you use the AWS CLI to call Amazon
#' Rekognition operations, you must pass it as a reference to an image in
#' an Amazon S3 bucket. For the AWS CLI, passing image bytes is not
#' supported. The image must be either a .png or .jpeg formatted file.
#' 
#' The [`detect_text`][rekognition_detect_text] operation returns text in
#' an array of TextDetection elements, `TextDetections`. Each
#' `TextDetection` element provides information about a single word or line
#' of text that was detected in the image.
#' 
#' A word is one or more script characters that are not separated by
#' spaces. [`detect_text`][rekognition_detect_text] can detect up to 100
#' words in an image.
#' 
#' A line is a string of equally spaced words. A line isn't necessarily a
#' complete sentence. For example, a driver's license number is detected as
#' a line. A line ends when there is no aligned text after it. Also, a line
#' ends when there is a large gap between words, relative to the length of
#' the words. This means, depending on the gap between words, Amazon
#' Rekognition may detect multiple lines in text aligned in the same
#' direction. Periods don't represent the end of a line. If a sentence
#' spans multiple lines, the [`detect_text`][rekognition_detect_text]
#' operation returns multiple lines.
#' 
#' To determine whether a `TextDetection` element is a line of text or a
#' word, use the `TextDetection` object `Type` field.
#' 
#' To be detected, text must be within +/- 90 degrees orientation of the
#' horizontal axis.
#' 
#' For more information, see Detecting text in the Amazon Rekognition
#' Developer Guide.
#'
#' @usage
#' rekognition_detect_text(Image, Filters)
#'
#' @param Image &#91;required&#93; The input image as base64-encoded bytes or an Amazon S3 object. If you
#' use the AWS CLI to call Amazon Rekognition operations, you can't pass
#' image bytes.
#' 
#' If you are using an AWS SDK to call Amazon Rekognition, you might not
#' need to base64-encode image bytes passed using the `Bytes` field. For
#' more information, see Images in the Amazon Rekognition developer guide.
#' @param Filters Optional parameters that let you set the criteria that the text must
#' meet to be included in your response.
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   TextDetections = list(
#'     list(
#'       DetectedText = "string",
#'       Type = "LINE"|"WORD",
#'       Id = 123,
#'       ParentId = 123,
#'       Confidence = 123.0,
#'       Geometry = list(
#'         BoundingBox = list(
#'           Width = 123.0,
#'           Height = 123.0,
#'           Left = 123.0,
#'           Top = 123.0
#'         ),
#'         Polygon = list(
#'           list(
#'             X = 123.0,
#'             Y = 123.0
#'           )
#'         )
#'       )
#'     )
#'   ),
#'   TextModelVersion = "string"
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$detect_text(
#'   Image = list(
#'     Bytes = raw,
#'     S3Object = list(
#'       Bucket = "string",
#'       Name = "string",
#'       Version = "string"
#'     )
#'   ),
#'   Filters = list(
#'     WordFilter = list(
#'       MinConfidence = 123.0,
#'       MinBoundingBoxHeight = 123.0,
#'       MinBoundingBoxWidth = 123.0
#'     ),
#'     RegionsOfInterest = list(
#'       list(
#'         BoundingBox = list(
#'           Width = 123.0,
#'           Height = 123.0,
#'           Left = 123.0,
#'           Top = 123.0
#'         ),
#'         Polygon = list(
#'           list(
#'             X = 123.0,
#'             Y = 123.0
#'           )
#'         )
#'       )
#'     )
#'   )
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname rekognition_detect_text
#'
#' @aliases rekognition_detect_text
rekognition_detect_text <- function(Image, Filters = NULL) {
  op <- new_operation(
    name = "DetectText",
    http_method = "POST",
    http_path = "/",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .rekognition$detect_text_input(Image = Image, Filters = Filters)
  output <- .rekognition$detect_text_output()
  config <- get_config()
  svc <- .rekognition$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.rekognition$operations$detect_text <- rekognition_detect_text

#' Removes the association between a Face supplied in an array of FaceIds
#' and the User
#'
#' @description
#' Removes the association between a `Face` supplied in an array of
#' `FaceIds` and the User. If the User is not present already, then a
#' `ResourceNotFound` exception is thrown. If successful, an array of faces
#' that are disassociated from the User is returned. If a given face is
#' already disassociated from the given UserID, it will be ignored and not
#' be returned in the response. If a given face is already associated with
#' a different User or not found in the collection it will be returned as
#' part of `UnsuccessfulDisassociations`. You can remove 1 - 100 face IDs
#' from a user at one time.
#'
#' @usage
#' rekognition_disassociate_faces(CollectionId, UserId, ClientRequestToken,
#'   FaceIds)
#'
#' @param CollectionId &#91;required&#93; The ID of an existing collection containing the UserID.
#' @param UserId &#91;required&#93; ID for the existing UserID.
#' @param ClientRequestToken Idempotent token used to identify the request to
#' [`disassociate_faces`][rekognition_disassociate_faces]. If you use the
#' same token with multiple
#' [`disassociate_faces`][rekognition_disassociate_faces] requests, the
#' same response is returned. Use ClientRequestToken to prevent the same
#' request from being processed more than once.
#' @param FaceIds &#91;required&#93; An array of face IDs to disassociate from the UserID.
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   DisassociatedFaces = list(
#'     list(
#'       FaceId = "string"
#'     )
#'   ),
#'   UnsuccessfulFaceDisassociations = list(
#'     list(
#'       FaceId = "string",
#'       UserId = "string",
#'       Reasons = list(
#'         "FACE_NOT_FOUND"|"ASSOCIATED_TO_A_DIFFERENT_USER"
#'       )
#'     )
#'   ),
#'   UserStatus = "ACTIVE"|"UPDATING"|"CREATING"|"CREATED"
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$disassociate_faces(
#'   CollectionId = "string",
#'   UserId = "string",
#'   ClientRequestToken = "string",
#'   FaceIds = list(
#'     "string"
#'   )
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname rekognition_disassociate_faces
#'
#' @aliases rekognition_disassociate_faces
rekognition_disassociate_faces <- function(CollectionId, UserId, ClientRequestToken = NULL, FaceIds) {
  op <- new_operation(
    name = "DisassociateFaces",
    http_method = "POST",
    http_path = "/",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .rekognition$disassociate_faces_input(CollectionId = CollectionId, UserId = UserId, ClientRequestToken = ClientRequestToken, FaceIds = FaceIds)
  output <- .rekognition$disassociate_faces_output()
  config <- get_config()
  svc <- .rekognition$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.rekognition$operations$disassociate_faces <- rekognition_disassociate_faces

#' This operation applies only to Amazon Rekognition Custom Labels
#'
#' @description
#' This operation applies only to Amazon Rekognition Custom Labels.
#' 
#' Distributes the entries (images) in a training dataset across the
#' training dataset and the test dataset for a project.
#' [`distribute_dataset_entries`][rekognition_distribute_dataset_entries]
#' moves 20% of the training dataset images to the test dataset. An entry
#' is a JSON Line that describes an image.
#' 
#' You supply the Amazon Resource Names (ARN) of a project's training
#' dataset and test dataset. The training dataset must contain the images
#' that you want to split. The test dataset must be empty. The datasets
#' must belong to the same project. To create training and test datasets
#' for a project, call [`create_dataset`][rekognition_create_dataset].
#' 
#' Distributing a dataset takes a while to complete. To check the status
#' call [`describe_dataset`][rekognition_describe_dataset]. The operation
#' is complete when the `Status` field for the training dataset and the
#' test dataset is `UPDATE_COMPLETE`. If the dataset split fails, the value
#' of `Status` is `UPDATE_FAILED`.
#' 
#' This operation requires permissions to perform the
#' `rekognition:DistributeDatasetEntries` action.
#'
#' @usage
#' rekognition_distribute_dataset_entries(Datasets)
#'
#' @param Datasets &#91;required&#93; The ARNS for the training dataset and test dataset that you want to use.
#' The datasets must belong to the same project. The test dataset must be
#' empty.
#'
#' @return
#' An empty list.
#'
#' @section Request syntax:
#' ```
#' svc$distribute_dataset_entries(
#'   Datasets = list(
#'     list(
#'       Arn = "string"
#'     )
#'   )
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname rekognition_distribute_dataset_entries
#'
#' @aliases rekognition_distribute_dataset_entries
rekognition_distribute_dataset_entries <- function(Datasets) {
  op <- new_operation(
    name = "DistributeDatasetEntries",
    http_method = "POST",
    http_path = "/",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .rekognition$distribute_dataset_entries_input(Datasets = Datasets)
  output <- .rekognition$distribute_dataset_entries_output()
  config <- get_config()
  svc <- .rekognition$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.rekognition$operations$distribute_dataset_entries <- rekognition_distribute_dataset_entries

#' Gets the name and additional information about a celebrity based on
#' their Amazon Rekognition ID
#'
#' @description
#' Gets the name and additional information about a celebrity based on
#' their Amazon Rekognition ID. The additional information is returned as
#' an array of URLs. If there is no additional information about the
#' celebrity, this list is empty.
#' 
#' For more information, see Getting information about a celebrity in the
#' Amazon Rekognition Developer Guide.
#' 
#' This operation requires permissions to perform the
#' `rekognition:GetCelebrityInfo` action.
#'
#' @usage
#' rekognition_get_celebrity_info(Id)
#'
#' @param Id &#91;required&#93; The ID for the celebrity. You get the celebrity ID from a call to the
#' [`recognize_celebrities`][rekognition_recognize_celebrities] operation,
#' which recognizes celebrities in an image.
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   Urls = list(
#'     "string"
#'   ),
#'   Name = "string",
#'   KnownGender = list(
#'     Type = "Male"|"Female"|"Nonbinary"|"Unlisted"
#'   )
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$get_celebrity_info(
#'   Id = "string"
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname rekognition_get_celebrity_info
#'
#' @aliases rekognition_get_celebrity_info
rekognition_get_celebrity_info <- function(Id) {
  op <- new_operation(
    name = "GetCelebrityInfo",
    http_method = "POST",
    http_path = "/",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .rekognition$get_celebrity_info_input(Id = Id)
  output <- .rekognition$get_celebrity_info_output()
  config <- get_config()
  svc <- .rekognition$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.rekognition$operations$get_celebrity_info <- rekognition_get_celebrity_info

#' Gets the celebrity recognition results for a Amazon Rekognition Video
#' analysis started by StartCelebrityRecognition
#'
#' @description
#' Gets the celebrity recognition results for a Amazon Rekognition Video
#' analysis started by
#' [`start_celebrity_recognition`][rekognition_start_celebrity_recognition].
#' 
#' Celebrity recognition in a video is an asynchronous operation. Analysis
#' is started by a call to
#' [`start_celebrity_recognition`][rekognition_start_celebrity_recognition]
#' which returns a job identifier (`JobId`).
#' 
#' When the celebrity recognition operation finishes, Amazon Rekognition
#' Video publishes a completion status to the Amazon Simple Notification
#' Service topic registered in the initial call to
#' [`start_celebrity_recognition`][rekognition_start_celebrity_recognition].
#' To get the results of the celebrity recognition analysis, first check
#' that the status value published to the Amazon SNS topic is `SUCCEEDED`.
#' If so, call `GetCelebrityDetection` and pass the job identifier
#' (`JobId`) from the initial call to `StartCelebrityDetection`.
#' 
#' For more information, see Working With Stored Videos in the Amazon
#' Rekognition Developer Guide.
#' 
#' [`get_celebrity_recognition`][rekognition_get_celebrity_recognition]
#' returns detected celebrities and the time(s) they are detected in an
#' array (`Celebrities`) of CelebrityRecognition objects. Each
#' `CelebrityRecognition` contains information about the celebrity in a
#' CelebrityDetail object and the time, `Timestamp`, the celebrity was
#' detected. This CelebrityDetail object stores information about the
#' detected celebrity's face attributes, a face bounding box, known gender,
#' the celebrity's name, and a confidence estimate.
#' 
#' [`get_celebrity_recognition`][rekognition_get_celebrity_recognition]
#' only returns the default facial attributes (`BoundingBox`, `Confidence`,
#' `Landmarks`, `Pose`, and `Quality`). The `BoundingBox` field only
#' applies to the detected face instance. The other facial attributes
#' listed in the `Face` object of the following response syntax are not
#' returned. For more information, see FaceDetail in the Amazon Rekognition
#' Developer Guide.
#' 
#' By default, the `Celebrities` array is sorted by time (milliseconds from
#' the start of the video). You can also sort the array by celebrity by
#' specifying the value `ID` in the `SortBy` input parameter.
#' 
#' The `CelebrityDetail` object includes the celebrity identifer and
#' additional information urls. If you don't store the additional
#' information urls, you can get them later by calling
#' [`get_celebrity_info`][rekognition_get_celebrity_info] with the
#' celebrity identifer.
#' 
#' No information is returned for faces not recognized as celebrities.
#' 
#' Use MaxResults parameter to limit the number of labels returned. If
#' there are more results than specified in `MaxResults`, the value of
#' `NextToken` in the operation response contains a pagination token for
#' getting the next set of results. To get the next page of results, call
#' `GetCelebrityDetection` and populate the `NextToken` request parameter
#' with the token value returned from the previous call to
#' [`get_celebrity_recognition`][rekognition_get_celebrity_recognition].
#'
#' @usage
#' rekognition_get_celebrity_recognition(JobId, MaxResults, NextToken,
#'   SortBy)
#'
#' @param JobId &#91;required&#93; Job identifier for the required celebrity recognition analysis. You can
#' get the job identifer from a call to
#' [`start_celebrity_recognition`][rekognition_start_celebrity_recognition].
#' @param MaxResults Maximum number of results to return per paginated call. The largest
#' value you can specify is 1000. If you specify a value greater than 1000,
#' a maximum of 1000 results is returned. The default value is 1000.
#' @param NextToken If the previous response was incomplete (because there is more
#' recognized celebrities to retrieve), Amazon Rekognition Video returns a
#' pagination token in the response. You can use this pagination token to
#' retrieve the next set of celebrities.
#' @param SortBy Sort to use for celebrities returned in `Celebrities` field. Specify
#' `ID` to sort by the celebrity identifier, specify `TIMESTAMP` to sort by
#' the time the celebrity was recognized.
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   JobStatus = "IN_PROGRESS"|"SUCCEEDED"|"FAILED",
#'   StatusMessage = "string",
#'   VideoMetadata = list(
#'     Codec = "string",
#'     DurationMillis = 123,
#'     Format = "string",
#'     FrameRate = 123.0,
#'     FrameHeight = 123,
#'     FrameWidth = 123,
#'     ColorRange = "FULL"|"LIMITED"
#'   ),
#'   NextToken = "string",
#'   Celebrities = list(
#'     list(
#'       Timestamp = 123,
#'       Celebrity = list(
#'         Urls = list(
#'           "string"
#'         ),
#'         Name = "string",
#'         Id = "string",
#'         Confidence = 123.0,
#'         BoundingBox = list(
#'           Width = 123.0,
#'           Height = 123.0,
#'           Left = 123.0,
#'           Top = 123.0
#'         ),
#'         Face = list(
#'           BoundingBox = list(
#'             Width = 123.0,
#'             Height = 123.0,
#'             Left = 123.0,
#'             Top = 123.0
#'           ),
#'           AgeRange = list(
#'             Low = 123,
#'             High = 123
#'           ),
#'           Smile = list(
#'             Value = TRUE|FALSE,
#'             Confidence = 123.0
#'           ),
#'           Eyeglasses = list(
#'             Value = TRUE|FALSE,
#'             Confidence = 123.0
#'           ),
#'           Sunglasses = list(
#'             Value = TRUE|FALSE,
#'             Confidence = 123.0
#'           ),
#'           Gender = list(
#'             Value = "Male"|"Female",
#'             Confidence = 123.0
#'           ),
#'           Beard = list(
#'             Value = TRUE|FALSE,
#'             Confidence = 123.0
#'           ),
#'           Mustache = list(
#'             Value = TRUE|FALSE,
#'             Confidence = 123.0
#'           ),
#'           EyesOpen = list(
#'             Value = TRUE|FALSE,
#'             Confidence = 123.0
#'           ),
#'           MouthOpen = list(
#'             Value = TRUE|FALSE,
#'             Confidence = 123.0
#'           ),
#'           Emotions = list(
#'             list(
#'               Type = "HAPPY"|"SAD"|"ANGRY"|"CONFUSED"|"DISGUSTED"|"SURPRISED"|"CALM"|"UNKNOWN"|"FEAR",
#'               Confidence = 123.0
#'             )
#'           ),
#'           Landmarks = list(
#'             list(
#'               Type = "eyeLeft"|"eyeRight"|"nose"|"mouthLeft"|"mouthRight"|"leftEyeBrowLeft"|"leftEyeBrowRight"|"leftEyeBrowUp"|"rightEyeBrowLeft"|"rightEyeBrowRight"|"rightEyeBrowUp"|"leftEyeLeft"|"leftEyeRight"|"leftEyeUp"|"leftEyeDown"|"rightEyeLeft"|"rightEyeRight"|"rightEyeUp"|"rightEyeDown"|"noseLeft"|"noseRight"|"mouthUp"|"mouthDown"|"leftPupil"|"rightPupil"|"upperJawlineLeft"|"midJawlineLeft"|"chinBottom"|"midJawlineRight"|"upperJawlineRight",
#'               X = 123.0,
#'               Y = 123.0
#'             )
#'           ),
#'           Pose = list(
#'             Roll = 123.0,
#'             Yaw = 123.0,
#'             Pitch = 123.0
#'           ),
#'           Quality = list(
#'             Brightness = 123.0,
#'             Sharpness = 123.0
#'           ),
#'           Confidence = 123.0,
#'           FaceOccluded = list(
#'             Value = TRUE|FALSE,
#'             Confidence = 123.0
#'           ),
#'           EyeDirection = list(
#'             Yaw = 123.0,
#'             Pitch = 123.0,
#'             Confidence = 123.0
#'           )
#'         ),
#'         KnownGender = list(
#'           Type = "Male"|"Female"|"Nonbinary"|"Unlisted"
#'         )
#'       )
#'     )
#'   ),
#'   JobId = "string",
#'   Video = list(
#'     S3Object = list(
#'       Bucket = "string",
#'       Name = "string",
#'       Version = "string"
#'     )
#'   ),
#'   JobTag = "string"
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$get_celebrity_recognition(
#'   JobId = "string",
#'   MaxResults = 123,
#'   NextToken = "string",
#'   SortBy = "ID"|"TIMESTAMP"
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname rekognition_get_celebrity_recognition
#'
#' @aliases rekognition_get_celebrity_recognition
rekognition_get_celebrity_recognition <- function(JobId, MaxResults = NULL, NextToken = NULL, SortBy = NULL) {
  op <- new_operation(
    name = "GetCelebrityRecognition",
    http_method = "POST",
    http_path = "/",
    host_prefix = "",
    paginator = list(input_token = "NextToken", limit_key = "MaxResults", output_token = "NextToken"),
    stream_api = FALSE
  )
  input <- .rekognition$get_celebrity_recognition_input(JobId = JobId, MaxResults = MaxResults, NextToken = NextToken, SortBy = SortBy)
  output <- .rekognition$get_celebrity_recognition_output()
  config <- get_config()
  svc <- .rekognition$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.rekognition$operations$get_celebrity_recognition <- rekognition_get_celebrity_recognition

#' Gets the inappropriate, unwanted, or offensive content analysis results
#' for a Amazon Rekognition Video analysis started by
#' StartContentModeration
#'
#' @description
#' Gets the inappropriate, unwanted, or offensive content analysis results
#' for a Amazon Rekognition Video analysis started by
#' [`start_content_moderation`][rekognition_start_content_moderation]. For
#' a list of moderation labels in Amazon Rekognition, see [Using the image
#' and video moderation
#' APIs](https://docs.aws.amazon.com/rekognition/latest/dg/moderation.html#moderation-api).
#' 
#' Amazon Rekognition Video inappropriate or offensive content detection in
#' a stored video is an asynchronous operation. You start analysis by
#' calling
#' [`start_content_moderation`][rekognition_start_content_moderation] which
#' returns a job identifier (`JobId`). When analysis finishes, Amazon
#' Rekognition Video publishes a completion status to the Amazon Simple
#' Notification Service topic registered in the initial call to
#' [`start_content_moderation`][rekognition_start_content_moderation]. To
#' get the results of the content analysis, first check that the status
#' value published to the Amazon SNS topic is `SUCCEEDED`. If so, call
#' [`get_content_moderation`][rekognition_get_content_moderation] and pass
#' the job identifier (`JobId`) from the initial call to
#' [`start_content_moderation`][rekognition_start_content_moderation].
#' 
#' For more information, see Working with Stored Videos in the Amazon
#' Rekognition Devlopers Guide.
#' 
#' [`get_content_moderation`][rekognition_get_content_moderation] returns
#' detected inappropriate, unwanted, or offensive content moderation
#' labels, and the time they are detected, in an array, `ModerationLabels`,
#' of ContentModerationDetection objects.
#' 
#' By default, the moderated labels are returned sorted by time, in
#' milliseconds from the start of the video. You can also sort them by
#' moderated label by specifying `NAME` for the `SortBy` input parameter.
#' 
#' Since video analysis can return a large number of results, use the
#' `MaxResults` parameter to limit the number of labels returned in a
#' single call to
#' [`get_content_moderation`][rekognition_get_content_moderation]. If there
#' are more results than specified in `MaxResults`, the value of
#' `NextToken` in the operation response contains a pagination token for
#' getting the next set of results. To get the next page of results, call
#' [`get_content_moderation`][rekognition_get_content_moderation] and
#' populate the `NextToken` request parameter with the value of `NextToken`
#' returned from the previous call to
#' [`get_content_moderation`][rekognition_get_content_moderation].
#' 
#' For more information, see moderating content in the Amazon Rekognition
#' Developer Guide.
#'
#' @usage
#' rekognition_get_content_moderation(JobId, MaxResults, NextToken, SortBy,
#'   AggregateBy)
#'
#' @param JobId &#91;required&#93; The identifier for the inappropriate, unwanted, or offensive content
#' moderation job. Use `JobId` to identify the job in a subsequent call to
#' [`get_content_moderation`][rekognition_get_content_moderation].
#' @param MaxResults Maximum number of results to return per paginated call. The largest
#' value you can specify is 1000. If you specify a value greater than 1000,
#' a maximum of 1000 results is returned. The default value is 1000.
#' @param NextToken If the previous response was incomplete (because there is more data to
#' retrieve), Amazon Rekognition returns a pagination token in the
#' response. You can use this pagination token to retrieve the next set of
#' content moderation labels.
#' @param SortBy Sort to use for elements in the `ModerationLabelDetections` array. Use
#' `TIMESTAMP` to sort array elements by the time labels are detected. Use
#' `NAME` to alphabetically group elements for a label together. Within
#' each label group, the array element are sorted by detection confidence.
#' The default sort is by `TIMESTAMP`.
#' @param AggregateBy Defines how to aggregate results of the StartContentModeration request.
#' Default aggregation option is TIMESTAMPS. SEGMENTS mode aggregates
#' moderation labels over time.
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   JobStatus = "IN_PROGRESS"|"SUCCEEDED"|"FAILED",
#'   StatusMessage = "string",
#'   VideoMetadata = list(
#'     Codec = "string",
#'     DurationMillis = 123,
#'     Format = "string",
#'     FrameRate = 123.0,
#'     FrameHeight = 123,
#'     FrameWidth = 123,
#'     ColorRange = "FULL"|"LIMITED"
#'   ),
#'   ModerationLabels = list(
#'     list(
#'       Timestamp = 123,
#'       ModerationLabel = list(
#'         Confidence = 123.0,
#'         Name = "string",
#'         ParentName = "string",
#'         TaxonomyLevel = 123
#'       ),
#'       StartTimestampMillis = 123,
#'       EndTimestampMillis = 123,
#'       DurationMillis = 123,
#'       ContentTypes = list(
#'         list(
#'           Confidence = 123.0,
#'           Name = "string"
#'         )
#'       )
#'     )
#'   ),
#'   NextToken = "string",
#'   ModerationModelVersion = "string",
#'   JobId = "string",
#'   Video = list(
#'     S3Object = list(
#'       Bucket = "string",
#'       Name = "string",
#'       Version = "string"
#'     )
#'   ),
#'   JobTag = "string",
#'   GetRequestMetadata = list(
#'     SortBy = "NAME"|"TIMESTAMP",
#'     AggregateBy = "TIMESTAMPS"|"SEGMENTS"
#'   )
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$get_content_moderation(
#'   JobId = "string",
#'   MaxResults = 123,
#'   NextToken = "string",
#'   SortBy = "NAME"|"TIMESTAMP",
#'   AggregateBy = "TIMESTAMPS"|"SEGMENTS"
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname rekognition_get_content_moderation
#'
#' @aliases rekognition_get_content_moderation
rekognition_get_content_moderation <- function(JobId, MaxResults = NULL, NextToken = NULL, SortBy = NULL, AggregateBy = NULL) {
  op <- new_operation(
    name = "GetContentModeration",
    http_method = "POST",
    http_path = "/",
    host_prefix = "",
    paginator = list(input_token = "NextToken", limit_key = "MaxResults", output_token = "NextToken"),
    stream_api = FALSE
  )
  input <- .rekognition$get_content_moderation_input(JobId = JobId, MaxResults = MaxResults, NextToken = NextToken, SortBy = SortBy, AggregateBy = AggregateBy)
  output <- .rekognition$get_content_moderation_output()
  config <- get_config()
  svc <- .rekognition$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.rekognition$operations$get_content_moderation <- rekognition_get_content_moderation

#' Gets face detection results for a Amazon Rekognition Video analysis
#' started by StartFaceDetection
#'
#' @description
#' Gets face detection results for a Amazon Rekognition Video analysis
#' started by [`start_face_detection`][rekognition_start_face_detection].
#' 
#' Face detection with Amazon Rekognition Video is an asynchronous
#' operation. You start face detection by calling
#' [`start_face_detection`][rekognition_start_face_detection] which returns
#' a job identifier (`JobId`). When the face detection operation finishes,
#' Amazon Rekognition Video publishes a completion status to the Amazon
#' Simple Notification Service topic registered in the initial call to
#' [`start_face_detection`][rekognition_start_face_detection]. To get the
#' results of the face detection operation, first check that the status
#' value published to the Amazon SNS topic is `SUCCEEDED`. If so, call
#' [`get_face_detection`][rekognition_get_face_detection] and pass the job
#' identifier (`JobId`) from the initial call to
#' [`start_face_detection`][rekognition_start_face_detection].
#' 
#' [`get_face_detection`][rekognition_get_face_detection] returns an array
#' of detected faces (`Faces`) sorted by the time the faces were detected.
#' 
#' Use MaxResults parameter to limit the number of labels returned. If
#' there are more results than specified in `MaxResults`, the value of
#' `NextToken` in the operation response contains a pagination token for
#' getting the next set of results. To get the next page of results, call
#' [`get_face_detection`][rekognition_get_face_detection] and populate the
#' `NextToken` request parameter with the token value returned from the
#' previous call to [`get_face_detection`][rekognition_get_face_detection].
#' 
#' Note that for the [`get_face_detection`][rekognition_get_face_detection]
#' operation, the returned values for `FaceOccluded` and `EyeDirection`
#' will always be "null".
#'
#' @usage
#' rekognition_get_face_detection(JobId, MaxResults, NextToken)
#'
#' @param JobId &#91;required&#93; Unique identifier for the face detection job. The `JobId` is returned
#' from [`start_face_detection`][rekognition_start_face_detection].
#' @param MaxResults Maximum number of results to return per paginated call. The largest
#' value you can specify is 1000. If you specify a value greater than 1000,
#' a maximum of 1000 results is returned. The default value is 1000.
#' @param NextToken If the previous response was incomplete (because there are more faces to
#' retrieve), Amazon Rekognition Video returns a pagination token in the
#' response. You can use this pagination token to retrieve the next set of
#' faces.
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   JobStatus = "IN_PROGRESS"|"SUCCEEDED"|"FAILED",
#'   StatusMessage = "string",
#'   VideoMetadata = list(
#'     Codec = "string",
#'     DurationMillis = 123,
#'     Format = "string",
#'     FrameRate = 123.0,
#'     FrameHeight = 123,
#'     FrameWidth = 123,
#'     ColorRange = "FULL"|"LIMITED"
#'   ),
#'   NextToken = "string",
#'   Faces = list(
#'     list(
#'       Timestamp = 123,
#'       Face = list(
#'         BoundingBox = list(
#'           Width = 123.0,
#'           Height = 123.0,
#'           Left = 123.0,
#'           Top = 123.0
#'         ),
#'         AgeRange = list(
#'           Low = 123,
#'           High = 123
#'         ),
#'         Smile = list(
#'           Value = TRUE|FALSE,
#'           Confidence = 123.0
#'         ),
#'         Eyeglasses = list(
#'           Value = TRUE|FALSE,
#'           Confidence = 123.0
#'         ),
#'         Sunglasses = list(
#'           Value = TRUE|FALSE,
#'           Confidence = 123.0
#'         ),
#'         Gender = list(
#'           Value = "Male"|"Female",
#'           Confidence = 123.0
#'         ),
#'         Beard = list(
#'           Value = TRUE|FALSE,
#'           Confidence = 123.0
#'         ),
#'         Mustache = list(
#'           Value = TRUE|FALSE,
#'           Confidence = 123.0
#'         ),
#'         EyesOpen = list(
#'           Value = TRUE|FALSE,
#'           Confidence = 123.0
#'         ),
#'         MouthOpen = list(
#'           Value = TRUE|FALSE,
#'           Confidence = 123.0
#'         ),
#'         Emotions = list(
#'           list(
#'             Type = "HAPPY"|"SAD"|"ANGRY"|"CONFUSED"|"DISGUSTED"|"SURPRISED"|"CALM"|"UNKNOWN"|"FEAR",
#'             Confidence = 123.0
#'           )
#'         ),
#'         Landmarks = list(
#'           list(
#'             Type = "eyeLeft"|"eyeRight"|"nose"|"mouthLeft"|"mouthRight"|"leftEyeBrowLeft"|"leftEyeBrowRight"|"leftEyeBrowUp"|"rightEyeBrowLeft"|"rightEyeBrowRight"|"rightEyeBrowUp"|"leftEyeLeft"|"leftEyeRight"|"leftEyeUp"|"leftEyeDown"|"rightEyeLeft"|"rightEyeRight"|"rightEyeUp"|"rightEyeDown"|"noseLeft"|"noseRight"|"mouthUp"|"mouthDown"|"leftPupil"|"rightPupil"|"upperJawlineLeft"|"midJawlineLeft"|"chinBottom"|"midJawlineRight"|"upperJawlineRight",
#'             X = 123.0,
#'             Y = 123.0
#'           )
#'         ),
#'         Pose = list(
#'           Roll = 123.0,
#'           Yaw = 123.0,
#'           Pitch = 123.0
#'         ),
#'         Quality = list(
#'           Brightness = 123.0,
#'           Sharpness = 123.0
#'         ),
#'         Confidence = 123.0,
#'         FaceOccluded = list(
#'           Value = TRUE|FALSE,
#'           Confidence = 123.0
#'         ),
#'         EyeDirection = list(
#'           Yaw = 123.0,
#'           Pitch = 123.0,
#'           Confidence = 123.0
#'         )
#'       )
#'     )
#'   ),
#'   JobId = "string",
#'   Video = list(
#'     S3Object = list(
#'       Bucket = "string",
#'       Name = "string",
#'       Version = "string"
#'     )
#'   ),
#'   JobTag = "string"
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$get_face_detection(
#'   JobId = "string",
#'   MaxResults = 123,
#'   NextToken = "string"
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname rekognition_get_face_detection
#'
#' @aliases rekognition_get_face_detection
rekognition_get_face_detection <- function(JobId, MaxResults = NULL, NextToken = NULL) {
  op <- new_operation(
    name = "GetFaceDetection",
    http_method = "POST",
    http_path = "/",
    host_prefix = "",
    paginator = list(input_token = "NextToken", limit_key = "MaxResults", output_token = "NextToken"),
    stream_api = FALSE
  )
  input <- .rekognition$get_face_detection_input(JobId = JobId, MaxResults = MaxResults, NextToken = NextToken)
  output <- .rekognition$get_face_detection_output()
  config <- get_config()
  svc <- .rekognition$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.rekognition$operations$get_face_detection <- rekognition_get_face_detection

#' Retrieves the results of a specific Face Liveness session
#'
#' @description
#' Retrieves the results of a specific Face Liveness session. It requires
#' the `sessionId` as input, which was created using
#' [`create_face_liveness_session`][rekognition_create_face_liveness_session].
#' Returns the corresponding Face Liveness confidence score, a reference
#' image that includes a face bounding box, and audit images that also
#' contain face bounding boxes. The Face Liveness confidence score ranges
#' from 0 to 100.
#' 
#' The number of audit images returned by
#' [`get_face_liveness_session_results`][rekognition_get_face_liveness_session_results]
#' is defined by the `AuditImagesLimit` paramater when calling
#' [`create_face_liveness_session`][rekognition_create_face_liveness_session].
#' Reference images are always returned when possible.
#'
#' @usage
#' rekognition_get_face_liveness_session_results(SessionId)
#'
#' @param SessionId &#91;required&#93; A unique 128-bit UUID. This is used to uniquely identify the session and
#' also acts as an idempotency token for all operations associated with the
#' session.
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   SessionId = "string",
#'   Status = "CREATED"|"IN_PROGRESS"|"SUCCEEDED"|"FAILED"|"EXPIRED",
#'   Confidence = 123.0,
#'   ReferenceImage = list(
#'     Bytes = raw,
#'     S3Object = list(
#'       Bucket = "string",
#'       Name = "string",
#'       Version = "string"
#'     ),
#'     BoundingBox = list(
#'       Width = 123.0,
#'       Height = 123.0,
#'       Left = 123.0,
#'       Top = 123.0
#'     )
#'   ),
#'   AuditImages = list(
#'     list(
#'       Bytes = raw,
#'       S3Object = list(
#'         Bucket = "string",
#'         Name = "string",
#'         Version = "string"
#'       ),
#'       BoundingBox = list(
#'         Width = 123.0,
#'         Height = 123.0,
#'         Left = 123.0,
#'         Top = 123.0
#'       )
#'     )
#'   ),
#'   Challenge = list(
#'     Type = "FaceMovementAndLightChallenge"|"FaceMovementChallenge",
#'     Version = "string"
#'   )
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$get_face_liveness_session_results(
#'   SessionId = "string"
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname rekognition_get_face_liveness_session_results
#'
#' @aliases rekognition_get_face_liveness_session_results
rekognition_get_face_liveness_session_results <- function(SessionId) {
  op <- new_operation(
    name = "GetFaceLivenessSessionResults",
    http_method = "POST",
    http_path = "/",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .rekognition$get_face_liveness_session_results_input(SessionId = SessionId)
  output <- .rekognition$get_face_liveness_session_results_output()
  config <- get_config()
  svc <- .rekognition$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.rekognition$operations$get_face_liveness_session_results <- rekognition_get_face_liveness_session_results

#' Gets the face search results for Amazon Rekognition Video face search
#' started by StartFaceSearch
#'
#' @description
#' Gets the face search results for Amazon Rekognition Video face search
#' started by [`start_face_search`][rekognition_start_face_search]. The
#' search returns faces in a collection that match the faces of persons
#' detected in a video. It also includes the time(s) that faces are matched
#' in the video.
#' 
#' Face search in a video is an asynchronous operation. You start face
#' search by calling to
#' [`start_face_search`][rekognition_start_face_search] which returns a job
#' identifier (`JobId`). When the search operation finishes, Amazon
#' Rekognition Video publishes a completion status to the Amazon Simple
#' Notification Service topic registered in the initial call to
#' [`start_face_search`][rekognition_start_face_search]. To get the search
#' results, first check that the status value published to the Amazon SNS
#' topic is `SUCCEEDED`. If so, call
#' [`get_face_search`][rekognition_get_face_search] and pass the job
#' identifier (`JobId`) from the initial call to
#' [`start_face_search`][rekognition_start_face_search].
#' 
#' For more information, see Searching Faces in a Collection in the Amazon
#' Rekognition Developer Guide.
#' 
#' The search results are retured in an array, `Persons`, of PersonMatch
#' objects. Each`PersonMatch` element contains details about the matching
#' faces in the input collection, person information (facial attributes,
#' bounding boxes, and person identifer) for the matched person, and the
#' time the person was matched in the video.
#' 
#' [`get_face_search`][rekognition_get_face_search] only returns the
#' default facial attributes (`BoundingBox`, `Confidence`, `Landmarks`,
#' `Pose`, and `Quality`). The other facial attributes listed in the `Face`
#' object of the following response syntax are not returned. For more
#' information, see FaceDetail in the Amazon Rekognition Developer Guide.
#' 
#' By default, the `Persons` array is sorted by the time, in milliseconds
#' from the start of the video, persons are matched. You can also sort by
#' persons by specifying `INDEX` for the `SORTBY` input parameter.
#'
#' @usage
#' rekognition_get_face_search(JobId, MaxResults, NextToken, SortBy)
#'
#' @param JobId &#91;required&#93; The job identifer for the search request. You get the job identifier
#' from an initial call to
#' [`start_face_search`][rekognition_start_face_search].
#' @param MaxResults Maximum number of results to return per paginated call. The largest
#' value you can specify is 1000. If you specify a value greater than 1000,
#' a maximum of 1000 results is returned. The default value is 1000.
#' @param NextToken If the previous response was incomplete (because there is more search
#' results to retrieve), Amazon Rekognition Video returns a pagination
#' token in the response. You can use this pagination token to retrieve the
#' next set of search results.
#' @param SortBy Sort to use for grouping faces in the response. Use `TIMESTAMP` to group
#' faces by the time that they are recognized. Use `INDEX` to sort by
#' recognized faces.
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   JobStatus = "IN_PROGRESS"|"SUCCEEDED"|"FAILED",
#'   StatusMessage = "string",
#'   NextToken = "string",
#'   VideoMetadata = list(
#'     Codec = "string",
#'     DurationMillis = 123,
#'     Format = "string",
#'     FrameRate = 123.0,
#'     FrameHeight = 123,
#'     FrameWidth = 123,
#'     ColorRange = "FULL"|"LIMITED"
#'   ),
#'   Persons = list(
#'     list(
#'       Timestamp = 123,
#'       Person = list(
#'         Index = 123,
#'         BoundingBox = list(
#'           Width = 123.0,
#'           Height = 123.0,
#'           Left = 123.0,
#'           Top = 123.0
#'         ),
#'         Face = list(
#'           BoundingBox = list(
#'             Width = 123.0,
#'             Height = 123.0,
#'             Left = 123.0,
#'             Top = 123.0
#'           ),
#'           AgeRange = list(
#'             Low = 123,
#'             High = 123
#'           ),
#'           Smile = list(
#'             Value = TRUE|FALSE,
#'             Confidence = 123.0
#'           ),
#'           Eyeglasses = list(
#'             Value = TRUE|FALSE,
#'             Confidence = 123.0
#'           ),
#'           Sunglasses = list(
#'             Value = TRUE|FALSE,
#'             Confidence = 123.0
#'           ),
#'           Gender = list(
#'             Value = "Male"|"Female",
#'             Confidence = 123.0
#'           ),
#'           Beard = list(
#'             Value = TRUE|FALSE,
#'             Confidence = 123.0
#'           ),
#'           Mustache = list(
#'             Value = TRUE|FALSE,
#'             Confidence = 123.0
#'           ),
#'           EyesOpen = list(
#'             Value = TRUE|FALSE,
#'             Confidence = 123.0
#'           ),
#'           MouthOpen = list(
#'             Value = TRUE|FALSE,
#'             Confidence = 123.0
#'           ),
#'           Emotions = list(
#'             list(
#'               Type = "HAPPY"|"SAD"|"ANGRY"|"CONFUSED"|"DISGUSTED"|"SURPRISED"|"CALM"|"UNKNOWN"|"FEAR",
#'               Confidence = 123.0
#'             )
#'           ),
#'           Landmarks = list(
#'             list(
#'               Type = "eyeLeft"|"eyeRight"|"nose"|"mouthLeft"|"mouthRight"|"leftEyeBrowLeft"|"leftEyeBrowRight"|"leftEyeBrowUp"|"rightEyeBrowLeft"|"rightEyeBrowRight"|"rightEyeBrowUp"|"leftEyeLeft"|"leftEyeRight"|"leftEyeUp"|"leftEyeDown"|"rightEyeLeft"|"rightEyeRight"|"rightEyeUp"|"rightEyeDown"|"noseLeft"|"noseRight"|"mouthUp"|"mouthDown"|"leftPupil"|"rightPupil"|"upperJawlineLeft"|"midJawlineLeft"|"chinBottom"|"midJawlineRight"|"upperJawlineRight",
#'               X = 123.0,
#'               Y = 123.0
#'             )
#'           ),
#'           Pose = list(
#'             Roll = 123.0,
#'             Yaw = 123.0,
#'             Pitch = 123.0
#'           ),
#'           Quality = list(
#'             Brightness = 123.0,
#'             Sharpness = 123.0
#'           ),
#'           Confidence = 123.0,
#'           FaceOccluded = list(
#'             Value = TRUE|FALSE,
#'             Confidence = 123.0
#'           ),
#'           EyeDirection = list(
#'             Yaw = 123.0,
#'             Pitch = 123.0,
#'             Confidence = 123.0
#'           )
#'         )
#'       ),
#'       FaceMatches = list(
#'         list(
#'           Similarity = 123.0,
#'           Face = list(
#'             FaceId = "string",
#'             BoundingBox = list(
#'               Width = 123.0,
#'               Height = 123.0,
#'               Left = 123.0,
#'               Top = 123.0
#'             ),
#'             ImageId = "string",
#'             ExternalImageId = "string",
#'             Confidence = 123.0,
#'             IndexFacesModelVersion = "string",
#'             UserId = "string"
#'           )
#'         )
#'       )
#'     )
#'   ),
#'   JobId = "string",
#'   Video = list(
#'     S3Object = list(
#'       Bucket = "string",
#'       Name = "string",
#'       Version = "string"
#'     )
#'   ),
#'   JobTag = "string"
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$get_face_search(
#'   JobId = "string",
#'   MaxResults = 123,
#'   NextToken = "string",
#'   SortBy = "INDEX"|"TIMESTAMP"
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname rekognition_get_face_search
#'
#' @aliases rekognition_get_face_search
rekognition_get_face_search <- function(JobId, MaxResults = NULL, NextToken = NULL, SortBy = NULL) {
  op <- new_operation(
    name = "GetFaceSearch",
    http_method = "POST",
    http_path = "/",
    host_prefix = "",
    paginator = list(input_token = "NextToken", limit_key = "MaxResults", output_token = "NextToken"),
    stream_api = FALSE
  )
  input <- .rekognition$get_face_search_input(JobId = JobId, MaxResults = MaxResults, NextToken = NextToken, SortBy = SortBy)
  output <- .rekognition$get_face_search_output()
  config <- get_config()
  svc <- .rekognition$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.rekognition$operations$get_face_search <- rekognition_get_face_search

#' Gets the label detection results of a Amazon Rekognition Video analysis
#' started by StartLabelDetection
#'
#' @description
#' Gets the label detection results of a Amazon Rekognition Video analysis
#' started by [`start_label_detection`][rekognition_start_label_detection].
#' 
#' The label detection operation is started by a call to
#' [`start_label_detection`][rekognition_start_label_detection] which
#' returns a job identifier (`JobId`). When the label detection operation
#' finishes, Amazon Rekognition publishes a completion status to the Amazon
#' Simple Notification Service topic registered in the initial call to
#' `StartlabelDetection`.
#' 
#' To get the results of the label detection operation, first check that
#' the status value published to the Amazon SNS topic is `SUCCEEDED`. If
#' so, call [`get_label_detection`][rekognition_get_label_detection] and
#' pass the job identifier (`JobId`) from the initial call to
#' [`start_label_detection`][rekognition_start_label_detection].
#' 
#' [`get_label_detection`][rekognition_get_label_detection] returns an
#' array of detected labels (`Labels`) sorted by the time the labels were
#' detected. You can also sort by the label name by specifying `NAME` for
#' the `SortBy` input parameter. If there is no `NAME` specified, the
#' default sort is by timestamp.
#' 
#' You can select how results are aggregated by using the `AggregateBy`
#' input parameter. The default aggregation method is `TIMESTAMPS`. You can
#' also aggregate by `SEGMENTS`, which aggregates all instances of labels
#' detected in a given segment.
#' 
#' The returned Labels array may include the following attributes:
#' 
#' -   Name - The name of the detected label.
#' 
#' -   Confidence - The level of confidence in the label assigned to a
#'     detected object.
#' 
#' -   Parents - The ancestor labels for a detected label.
#'     GetLabelDetection returns a hierarchical taxonomy of detected
#'     labels. For example, a detected car might be assigned the label car.
#'     The label car has two parent labels: Vehicle (its parent) and
#'     Transportation (its grandparent). The response includes the all
#'     ancestors for a label, where every ancestor is a unique label. In
#'     the previous example, Car, Vehicle, and Transportation are returned
#'     as unique labels in the response.
#' 
#' -   Aliases - Possible Aliases for the label.
#' 
#' -   Categories - The label categories that the detected label belongs
#'     to.
#' 
#' -   BoundingBox — Bounding boxes are described for all instances of
#'     detected common object labels, returned in an array of Instance
#'     objects. An Instance object contains a BoundingBox object,
#'     describing the location of the label on the input image. It also
#'     includes the confidence for the accuracy of the detected bounding
#'     box.
#' 
#' -   Timestamp - Time, in milliseconds from the start of the video, that
#'     the label was detected. For aggregation by `SEGMENTS`, the
#'     `StartTimestampMillis`, `EndTimestampMillis`, and `DurationMillis`
#'     structures are what define a segment. Although the “Timestamp”
#'     structure is still returned with each label, its value is set to be
#'     the same as `StartTimestampMillis`.
#' 
#' Timestamp and Bounding box information are returned for detected
#' Instances, only if aggregation is done by `TIMESTAMPS`. If aggregating
#' by `SEGMENTS`, information about detected instances isn’t returned.
#' 
#' The version of the label model used for the detection is also returned.
#' 
#' **Note `DominantColors` isn't returned for `Instances`, although it is
#' shown as part of the response in the sample seen below.**
#' 
#' Use `MaxResults` parameter to limit the number of labels returned. If
#' there are more results than specified in `MaxResults`, the value of
#' `NextToken` in the operation response contains a pagination token for
#' getting the next set of results. To get the next page of results, call
#' `GetlabelDetection` and populate the `NextToken` request parameter with
#' the token value returned from the previous call to
#' [`get_label_detection`][rekognition_get_label_detection].
#' 
#' If you are retrieving results while using the Amazon Simple Notification
#' Service, note that you will receive an "ERROR" notification if the job
#' encounters an issue.
#'
#' @usage
#' rekognition_get_label_detection(JobId, MaxResults, NextToken, SortBy,
#'   AggregateBy)
#'
#' @param JobId &#91;required&#93; Job identifier for the label detection operation for which you want
#' results returned. You get the job identifer from an initial call to
#' `StartlabelDetection`.
#' @param MaxResults Maximum number of results to return per paginated call. The largest
#' value you can specify is 1000. If you specify a value greater than 1000,
#' a maximum of 1000 results is returned. The default value is 1000.
#' @param NextToken If the previous response was incomplete (because there are more labels
#' to retrieve), Amazon Rekognition Video returns a pagination token in the
#' response. You can use this pagination token to retrieve the next set of
#' labels.
#' @param SortBy Sort to use for elements in the `Labels` array. Use `TIMESTAMP` to sort
#' array elements by the time labels are detected. Use `NAME` to
#' alphabetically group elements for a label together. Within each label
#' group, the array element are sorted by detection confidence. The default
#' sort is by `TIMESTAMP`.
#' @param AggregateBy Defines how to aggregate the returned results. Results can be aggregated
#' by timestamps or segments.
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   JobStatus = "IN_PROGRESS"|"SUCCEEDED"|"FAILED",
#'   StatusMessage = "string",
#'   VideoMetadata = list(
#'     Codec = "string",
#'     DurationMillis = 123,
#'     Format = "string",
#'     FrameRate = 123.0,
#'     FrameHeight = 123,
#'     FrameWidth = 123,
#'     ColorRange = "FULL"|"LIMITED"
#'   ),
#'   NextToken = "string",
#'   Labels = list(
#'     list(
#'       Timestamp = 123,
#'       Label = list(
#'         Name = "string",
#'         Confidence = 123.0,
#'         Instances = list(
#'           list(
#'             BoundingBox = list(
#'               Width = 123.0,
#'               Height = 123.0,
#'               Left = 123.0,
#'               Top = 123.0
#'             ),
#'             Confidence = 123.0,
#'             DominantColors = list(
#'               list(
#'                 Red = 123,
#'                 Blue = 123,
#'                 Green = 123,
#'                 HexCode = "string",
#'                 CSSColor = "string",
#'                 SimplifiedColor = "string",
#'                 PixelPercent = 123.0
#'               )
#'             )
#'           )
#'         ),
#'         Parents = list(
#'           list(
#'             Name = "string"
#'           )
#'         ),
#'         Aliases = list(
#'           list(
#'             Name = "string"
#'           )
#'         ),
#'         Categories = list(
#'           list(
#'             Name = "string"
#'           )
#'         )
#'       ),
#'       StartTimestampMillis = 123,
#'       EndTimestampMillis = 123,
#'       DurationMillis = 123
#'     )
#'   ),
#'   LabelModelVersion = "string",
#'   JobId = "string",
#'   Video = list(
#'     S3Object = list(
#'       Bucket = "string",
#'       Name = "string",
#'       Version = "string"
#'     )
#'   ),
#'   JobTag = "string",
#'   GetRequestMetadata = list(
#'     SortBy = "NAME"|"TIMESTAMP",
#'     AggregateBy = "TIMESTAMPS"|"SEGMENTS"
#'   )
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$get_label_detection(
#'   JobId = "string",
#'   MaxResults = 123,
#'   NextToken = "string",
#'   SortBy = "NAME"|"TIMESTAMP",
#'   AggregateBy = "TIMESTAMPS"|"SEGMENTS"
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname rekognition_get_label_detection
#'
#' @aliases rekognition_get_label_detection
rekognition_get_label_detection <- function(JobId, MaxResults = NULL, NextToken = NULL, SortBy = NULL, AggregateBy = NULL) {
  op <- new_operation(
    name = "GetLabelDetection",
    http_method = "POST",
    http_path = "/",
    host_prefix = "",
    paginator = list(input_token = "NextToken", limit_key = "MaxResults", output_token = "NextToken"),
    stream_api = FALSE
  )
  input <- .rekognition$get_label_detection_input(JobId = JobId, MaxResults = MaxResults, NextToken = NextToken, SortBy = SortBy, AggregateBy = AggregateBy)
  output <- .rekognition$get_label_detection_output()
  config <- get_config()
  svc <- .rekognition$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.rekognition$operations$get_label_detection <- rekognition_get_label_detection

#' Retrieves the results for a given media analysis job
#'
#' @description
#' Retrieves the results for a given media analysis job. Takes a `JobId`
#' returned by StartMediaAnalysisJob.
#'
#' @usage
#' rekognition_get_media_analysis_job(JobId)
#'
#' @param JobId &#91;required&#93; Unique identifier for the media analysis job for which you want to
#' retrieve results.
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   JobId = "string",
#'   JobName = "string",
#'   OperationsConfig = list(
#'     DetectModerationLabels = list(
#'       MinConfidence = 123.0,
#'       ProjectVersion = "string"
#'     )
#'   ),
#'   Status = "CREATED"|"QUEUED"|"IN_PROGRESS"|"SUCCEEDED"|"FAILED",
#'   FailureDetails = list(
#'     Code = "INTERNAL_ERROR"|"INVALID_S3_OBJECT"|"INVALID_MANIFEST"|"INVALID_OUTPUT_CONFIG"|"INVALID_KMS_KEY"|"ACCESS_DENIED"|"RESOURCE_NOT_FOUND"|"RESOURCE_NOT_READY"|"THROTTLED",
#'     Message = "string"
#'   ),
#'   CreationTimestamp = as.POSIXct(
#'     "2015-01-01"
#'   ),
#'   CompletionTimestamp = as.POSIXct(
#'     "2015-01-01"
#'   ),
#'   Input = list(
#'     S3Object = list(
#'       Bucket = "string",
#'       Name = "string",
#'       Version = "string"
#'     )
#'   ),
#'   OutputConfig = list(
#'     S3Bucket = "string",
#'     S3KeyPrefix = "string"
#'   ),
#'   KmsKeyId = "string",
#'   Results = list(
#'     S3Object = list(
#'       Bucket = "string",
#'       Name = "string",
#'       Version = "string"
#'     ),
#'     ModelVersions = list(
#'       Moderation = "string"
#'     )
#'   ),
#'   ManifestSummary = list(
#'     S3Object = list(
#'       Bucket = "string",
#'       Name = "string",
#'       Version = "string"
#'     )
#'   )
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$get_media_analysis_job(
#'   JobId = "string"
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname rekognition_get_media_analysis_job
#'
#' @aliases rekognition_get_media_analysis_job
rekognition_get_media_analysis_job <- function(JobId) {
  op <- new_operation(
    name = "GetMediaAnalysisJob",
    http_method = "POST",
    http_path = "/",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .rekognition$get_media_analysis_job_input(JobId = JobId)
  output <- .rekognition$get_media_analysis_job_output()
  config <- get_config()
  svc <- .rekognition$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.rekognition$operations$get_media_analysis_job <- rekognition_get_media_analysis_job

#' End of support notice: On October 31, 2025, AWS will discontinue support
#' for Amazon Rekognition People Pathing
#'
#' @description
#' *End of support notice:* On October 31, 2025, AWS will discontinue
#' support for Amazon Rekognition People Pathing. After October 31, 2025,
#' you will no longer be able to use the Rekognition People Pathing
#' capability. For more information, visit this [blog
#' post](https://aws.amazon.com/blogs/machine-learning/transitioning-from-amazon-rekognition-people-pathing-exploring-other-alternatives/).
#' 
#' Gets the path tracking results of a Amazon Rekognition Video analysis
#' started by [`start_person_tracking`][rekognition_start_person_tracking].
#' 
#' The person path tracking operation is started by a call to
#' [`start_person_tracking`][rekognition_start_person_tracking] which
#' returns a job identifier (`JobId`). When the operation finishes, Amazon
#' Rekognition Video publishes a completion status to the Amazon Simple
#' Notification Service topic registered in the initial call to
#' [`start_person_tracking`][rekognition_start_person_tracking].
#' 
#' To get the results of the person path tracking operation, first check
#' that the status value published to the Amazon SNS topic is `SUCCEEDED`.
#' If so, call [`get_person_tracking`][rekognition_get_person_tracking] and
#' pass the job identifier (`JobId`) from the initial call to
#' [`start_person_tracking`][rekognition_start_person_tracking].
#' 
#' [`get_person_tracking`][rekognition_get_person_tracking] returns an
#' array, `Persons`, of tracked persons and the time(s) their paths were
#' tracked in the video.
#' 
#' [`get_person_tracking`][rekognition_get_person_tracking] only returns
#' the default facial attributes (`BoundingBox`, `Confidence`, `Landmarks`,
#' `Pose`, and `Quality`). The other facial attributes listed in the `Face`
#' object of the following response syntax are not returned.
#' 
#' For more information, see FaceDetail in the Amazon Rekognition Developer
#' Guide.
#' 
#' By default, the array is sorted by the time(s) a person's path is
#' tracked in the video. You can sort by tracked persons by specifying
#' `INDEX` for the `SortBy` input parameter.
#' 
#' Use the `MaxResults` parameter to limit the number of items returned. If
#' there are more results than specified in `MaxResults`, the value of
#' `NextToken` in the operation response contains a pagination token for
#' getting the next set of results. To get the next page of results, call
#' [`get_person_tracking`][rekognition_get_person_tracking] and populate
#' the `NextToken` request parameter with the token value returned from the
#' previous call to
#' [`get_person_tracking`][rekognition_get_person_tracking].
#'
#' @usage
#' rekognition_get_person_tracking(JobId, MaxResults, NextToken, SortBy)
#'
#' @param JobId &#91;required&#93; The identifier for a job that tracks persons in a video. You get the
#' `JobId` from a call to
#' [`start_person_tracking`][rekognition_start_person_tracking].
#' @param MaxResults Maximum number of results to return per paginated call. The largest
#' value you can specify is 1000. If you specify a value greater than 1000,
#' a maximum of 1000 results is returned. The default value is 1000.
#' @param NextToken If the previous response was incomplete (because there are more persons
#' to retrieve), Amazon Rekognition Video returns a pagination token in the
#' response. You can use this pagination token to retrieve the next set of
#' persons.
#' @param SortBy Sort to use for elements in the `Persons` array. Use `TIMESTAMP` to sort
#' array elements by the time persons are detected. Use `INDEX` to sort by
#' the tracked persons. If you sort by `INDEX`, the array elements for each
#' person are sorted by detection confidence. The default sort is by
#' `TIMESTAMP`.
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   JobStatus = "IN_PROGRESS"|"SUCCEEDED"|"FAILED",
#'   StatusMessage = "string",
#'   VideoMetadata = list(
#'     Codec = "string",
#'     DurationMillis = 123,
#'     Format = "string",
#'     FrameRate = 123.0,
#'     FrameHeight = 123,
#'     FrameWidth = 123,
#'     ColorRange = "FULL"|"LIMITED"
#'   ),
#'   NextToken = "string",
#'   Persons = list(
#'     list(
#'       Timestamp = 123,
#'       Person = list(
#'         Index = 123,
#'         BoundingBox = list(
#'           Width = 123.0,
#'           Height = 123.0,
#'           Left = 123.0,
#'           Top = 123.0
#'         ),
#'         Face = list(
#'           BoundingBox = list(
#'             Width = 123.0,
#'             Height = 123.0,
#'             Left = 123.0,
#'             Top = 123.0
#'           ),
#'           AgeRange = list(
#'             Low = 123,
#'             High = 123
#'           ),
#'           Smile = list(
#'             Value = TRUE|FALSE,
#'             Confidence = 123.0
#'           ),
#'           Eyeglasses = list(
#'             Value = TRUE|FALSE,
#'             Confidence = 123.0
#'           ),
#'           Sunglasses = list(
#'             Value = TRUE|FALSE,
#'             Confidence = 123.0
#'           ),
#'           Gender = list(
#'             Value = "Male"|"Female",
#'             Confidence = 123.0
#'           ),
#'           Beard = list(
#'             Value = TRUE|FALSE,
#'             Confidence = 123.0
#'           ),
#'           Mustache = list(
#'             Value = TRUE|FALSE,
#'             Confidence = 123.0
#'           ),
#'           EyesOpen = list(
#'             Value = TRUE|FALSE,
#'             Confidence = 123.0
#'           ),
#'           MouthOpen = list(
#'             Value = TRUE|FALSE,
#'             Confidence = 123.0
#'           ),
#'           Emotions = list(
#'             list(
#'               Type = "HAPPY"|"SAD"|"ANGRY"|"CONFUSED"|"DISGUSTED"|"SURPRISED"|"CALM"|"UNKNOWN"|"FEAR",
#'               Confidence = 123.0
#'             )
#'           ),
#'           Landmarks = list(
#'             list(
#'               Type = "eyeLeft"|"eyeRight"|"nose"|"mouthLeft"|"mouthRight"|"leftEyeBrowLeft"|"leftEyeBrowRight"|"leftEyeBrowUp"|"rightEyeBrowLeft"|"rightEyeBrowRight"|"rightEyeBrowUp"|"leftEyeLeft"|"leftEyeRight"|"leftEyeUp"|"leftEyeDown"|"rightEyeLeft"|"rightEyeRight"|"rightEyeUp"|"rightEyeDown"|"noseLeft"|"noseRight"|"mouthUp"|"mouthDown"|"leftPupil"|"rightPupil"|"upperJawlineLeft"|"midJawlineLeft"|"chinBottom"|"midJawlineRight"|"upperJawlineRight",
#'               X = 123.0,
#'               Y = 123.0
#'             )
#'           ),
#'           Pose = list(
#'             Roll = 123.0,
#'             Yaw = 123.0,
#'             Pitch = 123.0
#'           ),
#'           Quality = list(
#'             Brightness = 123.0,
#'             Sharpness = 123.0
#'           ),
#'           Confidence = 123.0,
#'           FaceOccluded = list(
#'             Value = TRUE|FALSE,
#'             Confidence = 123.0
#'           ),
#'           EyeDirection = list(
#'             Yaw = 123.0,
#'             Pitch = 123.0,
#'             Confidence = 123.0
#'           )
#'         )
#'       )
#'     )
#'   ),
#'   JobId = "string",
#'   Video = list(
#'     S3Object = list(
#'       Bucket = "string",
#'       Name = "string",
#'       Version = "string"
#'     )
#'   ),
#'   JobTag = "string"
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$get_person_tracking(
#'   JobId = "string",
#'   MaxResults = 123,
#'   NextToken = "string",
#'   SortBy = "INDEX"|"TIMESTAMP"
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname rekognition_get_person_tracking
#'
#' @aliases rekognition_get_person_tracking
rekognition_get_person_tracking <- function(JobId, MaxResults = NULL, NextToken = NULL, SortBy = NULL) {
  op <- new_operation(
    name = "GetPersonTracking",
    http_method = "POST",
    http_path = "/",
    host_prefix = "",
    paginator = list(input_token = "NextToken", limit_key = "MaxResults", output_token = "NextToken"),
    stream_api = FALSE
  )
  input <- .rekognition$get_person_tracking_input(JobId = JobId, MaxResults = MaxResults, NextToken = NextToken, SortBy = SortBy)
  output <- .rekognition$get_person_tracking_output()
  config <- get_config()
  svc <- .rekognition$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.rekognition$operations$get_person_tracking <- rekognition_get_person_tracking

#' Gets the segment detection results of a Amazon Rekognition Video
#' analysis started by StartSegmentDetection
#'
#' @description
#' Gets the segment detection results of a Amazon Rekognition Video
#' analysis started by
#' [`start_segment_detection`][rekognition_start_segment_detection].
#' 
#' Segment detection with Amazon Rekognition Video is an asynchronous
#' operation. You start segment detection by calling
#' [`start_segment_detection`][rekognition_start_segment_detection] which
#' returns a job identifier (`JobId`). When the segment detection operation
#' finishes, Amazon Rekognition publishes a completion status to the Amazon
#' Simple Notification Service topic registered in the initial call to
#' [`start_segment_detection`][rekognition_start_segment_detection]. To get
#' the results of the segment detection operation, first check that the
#' status value published to the Amazon SNS topic is `SUCCEEDED`. if so,
#' call [`get_segment_detection`][rekognition_get_segment_detection] and
#' pass the job identifier (`JobId`) from the initial call of
#' [`start_segment_detection`][rekognition_start_segment_detection].
#' 
#' [`get_segment_detection`][rekognition_get_segment_detection] returns
#' detected segments in an array (`Segments`) of SegmentDetection objects.
#' `Segments` is sorted by the segment types specified in the
#' `SegmentTypes` input parameter of
#' [`start_segment_detection`][rekognition_start_segment_detection]. Each
#' element of the array includes the detected segment, the precentage
#' confidence in the acuracy of the detected segment, the type of the
#' segment, and the frame in which the segment was detected.
#' 
#' Use `SelectedSegmentTypes` to find out the type of segment detection
#' requested in the call to
#' [`start_segment_detection`][rekognition_start_segment_detection].
#' 
#' Use the `MaxResults` parameter to limit the number of segment detections
#' returned. If there are more results than specified in `MaxResults`, the
#' value of `NextToken` in the operation response contains a pagination
#' token for getting the next set of results. To get the next page of
#' results, call
#' [`get_segment_detection`][rekognition_get_segment_detection] and
#' populate the `NextToken` request parameter with the token value returned
#' from the previous call to
#' [`get_segment_detection`][rekognition_get_segment_detection].
#' 
#' For more information, see Detecting video segments in stored video in
#' the Amazon Rekognition Developer Guide.
#'
#' @usage
#' rekognition_get_segment_detection(JobId, MaxResults, NextToken)
#'
#' @param JobId &#91;required&#93; Job identifier for the text detection operation for which you want
#' results returned. You get the job identifer from an initial call to
#' [`start_segment_detection`][rekognition_start_segment_detection].
#' @param MaxResults Maximum number of results to return per paginated call. The largest
#' value you can specify is 1000.
#' @param NextToken If the response is truncated, Amazon Rekognition Video returns this
#' token that you can use in the subsequent request to retrieve the next
#' set of text.
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   JobStatus = "IN_PROGRESS"|"SUCCEEDED"|"FAILED",
#'   StatusMessage = "string",
#'   VideoMetadata = list(
#'     list(
#'       Codec = "string",
#'       DurationMillis = 123,
#'       Format = "string",
#'       FrameRate = 123.0,
#'       FrameHeight = 123,
#'       FrameWidth = 123,
#'       ColorRange = "FULL"|"LIMITED"
#'     )
#'   ),
#'   AudioMetadata = list(
#'     list(
#'       Codec = "string",
#'       DurationMillis = 123,
#'       SampleRate = 123,
#'       NumberOfChannels = 123
#'     )
#'   ),
#'   NextToken = "string",
#'   Segments = list(
#'     list(
#'       Type = "TECHNICAL_CUE"|"SHOT",
#'       StartTimestampMillis = 123,
#'       EndTimestampMillis = 123,
#'       DurationMillis = 123,
#'       StartTimecodeSMPTE = "string",
#'       EndTimecodeSMPTE = "string",
#'       DurationSMPTE = "string",
#'       TechnicalCueSegment = list(
#'         Type = "ColorBars"|"EndCredits"|"BlackFrames"|"OpeningCredits"|"StudioLogo"|"Slate"|"Content",
#'         Confidence = 123.0
#'       ),
#'       ShotSegment = list(
#'         Index = 123,
#'         Confidence = 123.0
#'       ),
#'       StartFrameNumber = 123,
#'       EndFrameNumber = 123,
#'       DurationFrames = 123
#'     )
#'   ),
#'   SelectedSegmentTypes = list(
#'     list(
#'       Type = "TECHNICAL_CUE"|"SHOT",
#'       ModelVersion = "string"
#'     )
#'   ),
#'   JobId = "string",
#'   Video = list(
#'     S3Object = list(
#'       Bucket = "string",
#'       Name = "string",
#'       Version = "string"
#'     )
#'   ),
#'   JobTag = "string"
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$get_segment_detection(
#'   JobId = "string",
#'   MaxResults = 123,
#'   NextToken = "string"
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname rekognition_get_segment_detection
#'
#' @aliases rekognition_get_segment_detection
rekognition_get_segment_detection <- function(JobId, MaxResults = NULL, NextToken = NULL) {
  op <- new_operation(
    name = "GetSegmentDetection",
    http_method = "POST",
    http_path = "/",
    host_prefix = "",
    paginator = list(input_token = "NextToken", limit_key = "MaxResults", output_token = "NextToken"),
    stream_api = FALSE
  )
  input <- .rekognition$get_segment_detection_input(JobId = JobId, MaxResults = MaxResults, NextToken = NextToken)
  output <- .rekognition$get_segment_detection_output()
  config <- get_config()
  svc <- .rekognition$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.rekognition$operations$get_segment_detection <- rekognition_get_segment_detection

#' Gets the text detection results of a Amazon Rekognition Video analysis
#' started by StartTextDetection
#'
#' @description
#' Gets the text detection results of a Amazon Rekognition Video analysis
#' started by [`start_text_detection`][rekognition_start_text_detection].
#' 
#' Text detection with Amazon Rekognition Video is an asynchronous
#' operation. You start text detection by calling
#' [`start_text_detection`][rekognition_start_text_detection] which returns
#' a job identifier (`JobId`) When the text detection operation finishes,
#' Amazon Rekognition publishes a completion status to the Amazon Simple
#' Notification Service topic registered in the initial call to
#' [`start_text_detection`][rekognition_start_text_detection]. To get the
#' results of the text detection operation, first check that the status
#' value published to the Amazon SNS topic is `SUCCEEDED`. if so, call
#' [`get_text_detection`][rekognition_get_text_detection] and pass the job
#' identifier (`JobId`) from the initial call of
#' [`start_label_detection`][rekognition_start_label_detection].
#' 
#' [`get_text_detection`][rekognition_get_text_detection] returns an array
#' of detected text (`TextDetections`) sorted by the time the text was
#' detected, up to 100 words per frame of video.
#' 
#' Each element of the array includes the detected text, the precentage
#' confidence in the acuracy of the detected text, the time the text was
#' detected, bounding box information for where the text was located, and
#' unique identifiers for words and their lines.
#' 
#' Use MaxResults parameter to limit the number of text detections
#' returned. If there are more results than specified in `MaxResults`, the
#' value of `NextToken` in the operation response contains a pagination
#' token for getting the next set of results. To get the next page of
#' results, call [`get_text_detection`][rekognition_get_text_detection] and
#' populate the `NextToken` request parameter with the token value returned
#' from the previous call to
#' [`get_text_detection`][rekognition_get_text_detection].
#'
#' @usage
#' rekognition_get_text_detection(JobId, MaxResults, NextToken)
#'
#' @param JobId &#91;required&#93; Job identifier for the text detection operation for which you want
#' results returned. You get the job identifer from an initial call to
#' [`start_text_detection`][rekognition_start_text_detection].
#' @param MaxResults Maximum number of results to return per paginated call. The largest
#' value you can specify is 1000.
#' @param NextToken If the previous response was incomplete (because there are more labels
#' to retrieve), Amazon Rekognition Video returns a pagination token in the
#' response. You can use this pagination token to retrieve the next set of
#' text.
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   JobStatus = "IN_PROGRESS"|"SUCCEEDED"|"FAILED",
#'   StatusMessage = "string",
#'   VideoMetadata = list(
#'     Codec = "string",
#'     DurationMillis = 123,
#'     Format = "string",
#'     FrameRate = 123.0,
#'     FrameHeight = 123,
#'     FrameWidth = 123,
#'     ColorRange = "FULL"|"LIMITED"
#'   ),
#'   TextDetections = list(
#'     list(
#'       Timestamp = 123,
#'       TextDetection = list(
#'         DetectedText = "string",
#'         Type = "LINE"|"WORD",
#'         Id = 123,
#'         ParentId = 123,
#'         Confidence = 123.0,
#'         Geometry = list(
#'           BoundingBox = list(
#'             Width = 123.0,
#'             Height = 123.0,
#'             Left = 123.0,
#'             Top = 123.0
#'           ),
#'           Polygon = list(
#'             list(
#'               X = 123.0,
#'               Y = 123.0
#'             )
#'           )
#'         )
#'       )
#'     )
#'   ),
#'   NextToken = "string",
#'   TextModelVersion = "string",
#'   JobId = "string",
#'   Video = list(
#'     S3Object = list(
#'       Bucket = "string",
#'       Name = "string",
#'       Version = "string"
#'     )
#'   ),
#'   JobTag = "string"
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$get_text_detection(
#'   JobId = "string",
#'   MaxResults = 123,
#'   NextToken = "string"
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname rekognition_get_text_detection
#'
#' @aliases rekognition_get_text_detection
rekognition_get_text_detection <- function(JobId, MaxResults = NULL, NextToken = NULL) {
  op <- new_operation(
    name = "GetTextDetection",
    http_method = "POST",
    http_path = "/",
    host_prefix = "",
    paginator = list(input_token = "NextToken", limit_key = "MaxResults", output_token = "NextToken"),
    stream_api = FALSE
  )
  input <- .rekognition$get_text_detection_input(JobId = JobId, MaxResults = MaxResults, NextToken = NextToken)
  output <- .rekognition$get_text_detection_output()
  config <- get_config()
  svc <- .rekognition$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.rekognition$operations$get_text_detection <- rekognition_get_text_detection

#' Detects faces in the input image and adds them to the specified
#' collection
#'
#' @description
#' Detects faces in the input image and adds them to the specified
#' collection.
#' 
#' Amazon Rekognition doesn't save the actual faces that are detected.
#' Instead, the underlying detection algorithm first detects the faces in
#' the input image. For each face, the algorithm extracts facial features
#' into a feature vector, and stores it in the backend database. Amazon
#' Rekognition uses feature vectors when it performs face match and search
#' operations using the [`search_faces`][rekognition_search_faces] and
#' [`search_faces_by_image`][rekognition_search_faces_by_image] operations.
#' 
#' For more information, see Adding faces to a collection in the Amazon
#' Rekognition Developer Guide.
#' 
#' To get the number of faces in a collection, call
#' [`describe_collection`][rekognition_describe_collection].
#' 
#' If you're using version 1.0 of the face detection model,
#' [`index_faces`][rekognition_index_faces] indexes the 15 largest faces in
#' the input image. Later versions of the face detection model index the
#' 100 largest faces in the input image.
#' 
#' If you're using version 4 or later of the face model, image orientation
#' information is not returned in the `OrientationCorrection` field.
#' 
#' To determine which version of the model you're using, call
#' [`describe_collection`][rekognition_describe_collection] and supply the
#' collection ID. You can also get the model version from the value of
#' `FaceModelVersion` in the response from
#' [`index_faces`][rekognition_index_faces]
#' 
#' For more information, see Model Versioning in the Amazon Rekognition
#' Developer Guide.
#' 
#' If you provide the optional `ExternalImageId` for the input image you
#' provided, Amazon Rekognition associates this ID with all faces that it
#' detects. When you call the [`list_faces`][rekognition_list_faces]
#' operation, the response returns the external ID. You can use this
#' external image ID to create a client-side index to associate the faces
#' with each image. You can then use the index to find all faces in an
#' image.
#' 
#' You can specify the maximum number of faces to index with the `MaxFaces`
#' input parameter. This is useful when you want to index the largest faces
#' in an image and don't want to index smaller faces, such as those
#' belonging to people standing in the background.
#' 
#' The `QualityFilter` input parameter allows you to filter out detected
#' faces that don’t meet a required quality bar. The quality bar is based
#' on a variety of common use cases. By default,
#' [`index_faces`][rekognition_index_faces] chooses the quality bar that's
#' used to filter faces. You can also explicitly choose the quality bar.
#' Use `QualityFilter`, to set the quality bar by specifying `LOW`,
#' `MEDIUM`, or `HIGH`. If you do not want to filter detected faces,
#' specify `NONE`.
#' 
#' To use quality filtering, you need a collection associated with version
#' 3 of the face model or higher. To get the version of the face model
#' associated with a collection, call
#' [`describe_collection`][rekognition_describe_collection].
#' 
#' Information about faces detected in an image, but not indexed, is
#' returned in an array of UnindexedFace objects, `UnindexedFaces`. Faces
#' aren't indexed for reasons such as:
#' 
#' -   The number of faces detected exceeds the value of the `MaxFaces`
#'     request parameter.
#' 
#' -   The face is too small compared to the image dimensions.
#' 
#' -   The face is too blurry.
#' 
#' -   The image is too dark.
#' 
#' -   The face has an extreme pose.
#' 
#' -   The face doesn’t have enough detail to be suitable for face search.
#' 
#' In response, the [`index_faces`][rekognition_index_faces] operation
#' returns an array of metadata for all detected faces, `FaceRecords`. This
#' includes:
#' 
#' -   The bounding box, `BoundingBox`, of the detected face.
#' 
#' -   A confidence value, `Confidence`, which indicates the confidence
#'     that the bounding box contains a face.
#' 
#' -   A face ID, `FaceId`, assigned by the service for each face that's
#'     detected and stored.
#' 
#' -   An image ID, `ImageId`, assigned by the service for the input image.
#' 
#' If you request `ALL` or specific facial attributes (e.g.,
#' `FACE_OCCLUDED`) by using the detectionAttributes parameter, Amazon
#' Rekognition returns detailed facial attributes, such as facial landmarks
#' (for example, location of eye and mouth), facial occlusion, and other
#' facial attributes.
#' 
#' If you provide the same image, specify the same collection, and use the
#' same external ID in the [`index_faces`][rekognition_index_faces]
#' operation, Amazon Rekognition doesn't save duplicate face metadata.
#' 
#' The input image is passed either as base64-encoded image bytes, or as a
#' reference to an image in an Amazon S3 bucket. If you use the AWS CLI to
#' call Amazon Rekognition operations, passing image bytes isn't supported.
#' The image must be formatted as a PNG or JPEG file.
#' 
#' This operation requires permissions to perform the
#' `rekognition:IndexFaces` action.
#'
#' @usage
#' rekognition_index_faces(CollectionId, Image, ExternalImageId,
#'   DetectionAttributes, MaxFaces, QualityFilter)
#'
#' @param CollectionId &#91;required&#93; The ID of an existing collection to which you want to add the faces that
#' are detected in the input images.
#' @param Image &#91;required&#93; The input image as base64-encoded bytes or an S3 object. If you use the
#' AWS CLI to call Amazon Rekognition operations, passing base64-encoded
#' image bytes isn't supported.
#' 
#' If you are using an AWS SDK to call Amazon Rekognition, you might not
#' need to base64-encode image bytes passed using the `Bytes` field. For
#' more information, see Images in the Amazon Rekognition developer guide.
#' @param ExternalImageId The ID you want to assign to all the faces detected in the image.
#' @param DetectionAttributes An array of facial attributes you want to be returned. A `DEFAULT`
#' subset of facial attributes - `BoundingBox`, `Confidence`, `Pose`,
#' `Quality`, and `Landmarks` - will always be returned. You can request
#' for specific facial attributes (in addition to the default list) - by
#' using `["DEFAULT", "FACE_OCCLUDED"]` or just `["FACE_OCCLUDED"]`. You
#' can request for all facial attributes by using `["ALL"]`. Requesting
#' more attributes may increase response time.
#' 
#' If you provide both, `["ALL", "DEFAULT"]`, the service uses a logical
#' AND operator to determine which attributes to return (in this case, all
#' attributes).
#' @param MaxFaces The maximum number of faces to index. The value of `MaxFaces` must be
#' greater than or equal to 1. [`index_faces`][rekognition_index_faces]
#' returns no more than 100 detected faces in an image, even if you specify
#' a larger value for `MaxFaces`.
#' 
#' If [`index_faces`][rekognition_index_faces] detects more faces than the
#' value of `MaxFaces`, the faces with the lowest quality are filtered out
#' first. If there are still more faces than the value of `MaxFaces`, the
#' faces with the smallest bounding boxes are filtered out (up to the
#' number that's needed to satisfy the value of `MaxFaces`). Information
#' about the unindexed faces is available in the `UnindexedFaces` array.
#' 
#' The faces that are returned by [`index_faces`][rekognition_index_faces]
#' are sorted by the largest face bounding box size to the smallest size,
#' in descending order.
#' 
#' `MaxFaces` can be used with a collection associated with any version of
#' the face model.
#' @param QualityFilter A filter that specifies a quality bar for how much filtering is done to
#' identify faces. Filtered faces aren't indexed. If you specify `AUTO`,
#' Amazon Rekognition chooses the quality bar. If you specify `LOW`,
#' `MEDIUM`, or `HIGH`, filtering removes all faces that don’t meet the
#' chosen quality bar. The default value is `AUTO`. The quality bar is
#' based on a variety of common use cases. Low-quality detections can occur
#' for a number of reasons. Some examples are an object that's
#' misidentified as a face, a face that's too blurry, or a face with a pose
#' that's too extreme to use. If you specify `NONE`, no filtering is
#' performed.
#' 
#' To use quality filtering, the collection you are using must be
#' associated with version 3 of the face model or higher.
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   FaceRecords = list(
#'     list(
#'       Face = list(
#'         FaceId = "string",
#'         BoundingBox = list(
#'           Width = 123.0,
#'           Height = 123.0,
#'           Left = 123.0,
#'           Top = 123.0
#'         ),
#'         ImageId = "string",
#'         ExternalImageId = "string",
#'         Confidence = 123.0,
#'         IndexFacesModelVersion = "string",
#'         UserId = "string"
#'       ),
#'       FaceDetail = list(
#'         BoundingBox = list(
#'           Width = 123.0,
#'           Height = 123.0,
#'           Left = 123.0,
#'           Top = 123.0
#'         ),
#'         AgeRange = list(
#'           Low = 123,
#'           High = 123
#'         ),
#'         Smile = list(
#'           Value = TRUE|FALSE,
#'           Confidence = 123.0
#'         ),
#'         Eyeglasses = list(
#'           Value = TRUE|FALSE,
#'           Confidence = 123.0
#'         ),
#'         Sunglasses = list(
#'           Value = TRUE|FALSE,
#'           Confidence = 123.0
#'         ),
#'         Gender = list(
#'           Value = "Male"|"Female",
#'           Confidence = 123.0
#'         ),
#'         Beard = list(
#'           Value = TRUE|FALSE,
#'           Confidence = 123.0
#'         ),
#'         Mustache = list(
#'           Value = TRUE|FALSE,
#'           Confidence = 123.0
#'         ),
#'         EyesOpen = list(
#'           Value = TRUE|FALSE,
#'           Confidence = 123.0
#'         ),
#'         MouthOpen = list(
#'           Value = TRUE|FALSE,
#'           Confidence = 123.0
#'         ),
#'         Emotions = list(
#'           list(
#'             Type = "HAPPY"|"SAD"|"ANGRY"|"CONFUSED"|"DISGUSTED"|"SURPRISED"|"CALM"|"UNKNOWN"|"FEAR",
#'             Confidence = 123.0
#'           )
#'         ),
#'         Landmarks = list(
#'           list(
#'             Type = "eyeLeft"|"eyeRight"|"nose"|"mouthLeft"|"mouthRight"|"leftEyeBrowLeft"|"leftEyeBrowRight"|"leftEyeBrowUp"|"rightEyeBrowLeft"|"rightEyeBrowRight"|"rightEyeBrowUp"|"leftEyeLeft"|"leftEyeRight"|"leftEyeUp"|"leftEyeDown"|"rightEyeLeft"|"rightEyeRight"|"rightEyeUp"|"rightEyeDown"|"noseLeft"|"noseRight"|"mouthUp"|"mouthDown"|"leftPupil"|"rightPupil"|"upperJawlineLeft"|"midJawlineLeft"|"chinBottom"|"midJawlineRight"|"upperJawlineRight",
#'             X = 123.0,
#'             Y = 123.0
#'           )
#'         ),
#'         Pose = list(
#'           Roll = 123.0,
#'           Yaw = 123.0,
#'           Pitch = 123.0
#'         ),
#'         Quality = list(
#'           Brightness = 123.0,
#'           Sharpness = 123.0
#'         ),
#'         Confidence = 123.0,
#'         FaceOccluded = list(
#'           Value = TRUE|FALSE,
#'           Confidence = 123.0
#'         ),
#'         EyeDirection = list(
#'           Yaw = 123.0,
#'           Pitch = 123.0,
#'           Confidence = 123.0
#'         )
#'       )
#'     )
#'   ),
#'   OrientationCorrection = "ROTATE_0"|"ROTATE_90"|"ROTATE_180"|"ROTATE_270",
#'   FaceModelVersion = "string",
#'   UnindexedFaces = list(
#'     list(
#'       Reasons = list(
#'         "EXCEEDS_MAX_FACES"|"EXTREME_POSE"|"LOW_BRIGHTNESS"|"LOW_SHARPNESS"|"LOW_CONFIDENCE"|"SMALL_BOUNDING_BOX"|"LOW_FACE_QUALITY"
#'       ),
#'       FaceDetail = list(
#'         BoundingBox = list(
#'           Width = 123.0,
#'           Height = 123.0,
#'           Left = 123.0,
#'           Top = 123.0
#'         ),
#'         AgeRange = list(
#'           Low = 123,
#'           High = 123
#'         ),
#'         Smile = list(
#'           Value = TRUE|FALSE,
#'           Confidence = 123.0
#'         ),
#'         Eyeglasses = list(
#'           Value = TRUE|FALSE,
#'           Confidence = 123.0
#'         ),
#'         Sunglasses = list(
#'           Value = TRUE|FALSE,
#'           Confidence = 123.0
#'         ),
#'         Gender = list(
#'           Value = "Male"|"Female",
#'           Confidence = 123.0
#'         ),
#'         Beard = list(
#'           Value = TRUE|FALSE,
#'           Confidence = 123.0
#'         ),
#'         Mustache = list(
#'           Value = TRUE|FALSE,
#'           Confidence = 123.0
#'         ),
#'         EyesOpen = list(
#'           Value = TRUE|FALSE,
#'           Confidence = 123.0
#'         ),
#'         MouthOpen = list(
#'           Value = TRUE|FALSE,
#'           Confidence = 123.0
#'         ),
#'         Emotions = list(
#'           list(
#'             Type = "HAPPY"|"SAD"|"ANGRY"|"CONFUSED"|"DISGUSTED"|"SURPRISED"|"CALM"|"UNKNOWN"|"FEAR",
#'             Confidence = 123.0
#'           )
#'         ),
#'         Landmarks = list(
#'           list(
#'             Type = "eyeLeft"|"eyeRight"|"nose"|"mouthLeft"|"mouthRight"|"leftEyeBrowLeft"|"leftEyeBrowRight"|"leftEyeBrowUp"|"rightEyeBrowLeft"|"rightEyeBrowRight"|"rightEyeBrowUp"|"leftEyeLeft"|"leftEyeRight"|"leftEyeUp"|"leftEyeDown"|"rightEyeLeft"|"rightEyeRight"|"rightEyeUp"|"rightEyeDown"|"noseLeft"|"noseRight"|"mouthUp"|"mouthDown"|"leftPupil"|"rightPupil"|"upperJawlineLeft"|"midJawlineLeft"|"chinBottom"|"midJawlineRight"|"upperJawlineRight",
#'             X = 123.0,
#'             Y = 123.0
#'           )
#'         ),
#'         Pose = list(
#'           Roll = 123.0,
#'           Yaw = 123.0,
#'           Pitch = 123.0
#'         ),
#'         Quality = list(
#'           Brightness = 123.0,
#'           Sharpness = 123.0
#'         ),
#'         Confidence = 123.0,
#'         FaceOccluded = list(
#'           Value = TRUE|FALSE,
#'           Confidence = 123.0
#'         ),
#'         EyeDirection = list(
#'           Yaw = 123.0,
#'           Pitch = 123.0,
#'           Confidence = 123.0
#'         )
#'       )
#'     )
#'   )
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$index_faces(
#'   CollectionId = "string",
#'   Image = list(
#'     Bytes = raw,
#'     S3Object = list(
#'       Bucket = "string",
#'       Name = "string",
#'       Version = "string"
#'     )
#'   ),
#'   ExternalImageId = "string",
#'   DetectionAttributes = list(
#'     "DEFAULT"|"ALL"|"AGE_RANGE"|"BEARD"|"EMOTIONS"|"EYE_DIRECTION"|"EYEGLASSES"|"EYES_OPEN"|"GENDER"|"MOUTH_OPEN"|"MUSTACHE"|"FACE_OCCLUDED"|"SMILE"|"SUNGLASSES"
#'   ),
#'   MaxFaces = 123,
#'   QualityFilter = "NONE"|"AUTO"|"LOW"|"MEDIUM"|"HIGH"
#' )
#' ```
#'
#' @examples
#' \dontrun{
#' # This operation detects faces in an image and adds them to the specified
#' # Rekognition collection.
#' svc$index_faces(
#'   CollectionId = "myphotos",
#'   DetectionAttributes = list(),
#'   ExternalImageId = "myphotoid",
#'   Image = list(
#'     S3Object = list(
#'       Bucket = "mybucket",
#'       Name = "myphoto"
#'     )
#'   )
#' )
#' }
#'
#' @keywords internal
#'
#' @rdname rekognition_index_faces
#'
#' @aliases rekognition_index_faces
rekognition_index_faces <- function(CollectionId, Image, ExternalImageId = NULL, DetectionAttributes = NULL, MaxFaces = NULL, QualityFilter = NULL) {
  op <- new_operation(
    name = "IndexFaces",
    http_method = "POST",
    http_path = "/",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .rekognition$index_faces_input(CollectionId = CollectionId, Image = Image, ExternalImageId = ExternalImageId, DetectionAttributes = DetectionAttributes, MaxFaces = MaxFaces, QualityFilter = QualityFilter)
  output <- .rekognition$index_faces_output()
  config <- get_config()
  svc <- .rekognition$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.rekognition$operations$index_faces <- rekognition_index_faces

#' Returns list of collection IDs in your account
#'
#' @description
#' Returns list of collection IDs in your account. If the result is
#' truncated, the response also provides a `NextToken` that you can use in
#' the subsequent request to fetch the next set of collection IDs.
#' 
#' For an example, see Listing collections in the Amazon Rekognition
#' Developer Guide.
#' 
#' This operation requires permissions to perform the
#' `rekognition:ListCollections` action.
#'
#' @usage
#' rekognition_list_collections(NextToken, MaxResults)
#'
#' @param NextToken Pagination token from the previous response.
#' @param MaxResults Maximum number of collection IDs to return.
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   CollectionIds = list(
#'     "string"
#'   ),
#'   NextToken = "string",
#'   FaceModelVersions = list(
#'     "string"
#'   )
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$list_collections(
#'   NextToken = "string",
#'   MaxResults = 123
#' )
#' ```
#'
#' @examples
#' \dontrun{
#' # This operation returns a list of Rekognition collections.
#' svc$list_collections()
#' }
#'
#' @keywords internal
#'
#' @rdname rekognition_list_collections
#'
#' @aliases rekognition_list_collections
rekognition_list_collections <- function(NextToken = NULL, MaxResults = NULL) {
  op <- new_operation(
    name = "ListCollections",
    http_method = "POST",
    http_path = "/",
    host_prefix = "",
    paginator = list(input_token = "NextToken", limit_key = "MaxResults", output_token = "NextToken", result_key = list("CollectionIds", "FaceModelVersions")),
    stream_api = FALSE
  )
  input <- .rekognition$list_collections_input(NextToken = NextToken, MaxResults = MaxResults)
  output <- .rekognition$list_collections_output()
  config <- get_config()
  svc <- .rekognition$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.rekognition$operations$list_collections <- rekognition_list_collections

#' This operation applies only to Amazon Rekognition Custom Labels
#'
#' @description
#' This operation applies only to Amazon Rekognition Custom Labels.
#' 
#' Lists the entries (images) within a dataset. An entry is a JSON Line
#' that contains the information for a single image, including the image
#' location, assigned labels, and object location bounding boxes. For more
#' information, see [Creating a manifest
#' file](https://docs.aws.amazon.com/rekognition/latest/customlabels-dg/).
#' 
#' JSON Lines in the response include information about non-terminal errors
#' found in the dataset. Non terminal errors are reported in `errors` lists
#' within each JSON Line. The same information is reported in the training
#' and testing validation result manifests that Amazon Rekognition Custom
#' Labels creates during model training.
#' 
#' You can filter the response in variety of ways, such as choosing which
#' labels to return and returning JSON Lines created after a specific date.
#' 
#' This operation requires permissions to perform the
#' `rekognition:ListDatasetEntries` action.
#'
#' @usage
#' rekognition_list_dataset_entries(DatasetArn, ContainsLabels, Labeled,
#'   SourceRefContains, HasErrors, NextToken, MaxResults)
#'
#' @param DatasetArn &#91;required&#93; The Amazon Resource Name (ARN) for the dataset that you want to use.
#' @param ContainsLabels Specifies a label filter for the response. The response includes an
#' entry only if one or more of the labels in `ContainsLabels` exist in the
#' entry.
#' @param Labeled Specify `true` to get only the JSON Lines where the image is labeled.
#' Specify `false` to get only the JSON Lines where the image isn't
#' labeled. If you don't specify `Labeled`,
#' [`list_dataset_entries`][rekognition_list_dataset_entries] returns JSON
#' Lines for labeled and unlabeled images.
#' @param SourceRefContains If specified, [`list_dataset_entries`][rekognition_list_dataset_entries]
#' only returns JSON Lines where the value of `SourceRefContains` is part
#' of the `source-ref` field. The `source-ref` field contains the Amazon S3
#' location of the image. You can use `SouceRefContains` for tasks such as
#' getting the JSON Line for a single image, or gettting JSON Lines for all
#' images within a specific folder.
#' @param HasErrors Specifies an error filter for the response. Specify `True` to only
#' include entries that have errors.
#' @param NextToken If the previous response was incomplete (because there is more results
#' to retrieve), Amazon Rekognition Custom Labels returns a pagination
#' token in the response. You can use this pagination token to retrieve the
#' next set of results.
#' @param MaxResults The maximum number of results to return per paginated call. The largest
#' value you can specify is 100. If you specify a value greater than 100, a
#' ValidationException error occurs. The default value is 100.
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   DatasetEntries = list(
#'     "string"
#'   ),
#'   NextToken = "string"
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$list_dataset_entries(
#'   DatasetArn = "string",
#'   ContainsLabels = list(
#'     "string"
#'   ),
#'   Labeled = TRUE|FALSE,
#'   SourceRefContains = "string",
#'   HasErrors = TRUE|FALSE,
#'   NextToken = "string",
#'   MaxResults = 123
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname rekognition_list_dataset_entries
#'
#' @aliases rekognition_list_dataset_entries
rekognition_list_dataset_entries <- function(DatasetArn, ContainsLabels = NULL, Labeled = NULL, SourceRefContains = NULL, HasErrors = NULL, NextToken = NULL, MaxResults = NULL) {
  op <- new_operation(
    name = "ListDatasetEntries",
    http_method = "POST",
    http_path = "/",
    host_prefix = "",
    paginator = list(input_token = "NextToken", limit_key = "MaxResults", output_token = "NextToken", result_key = "DatasetEntries"),
    stream_api = FALSE
  )
  input <- .rekognition$list_dataset_entries_input(DatasetArn = DatasetArn, ContainsLabels = ContainsLabels, Labeled = Labeled, SourceRefContains = SourceRefContains, HasErrors = HasErrors, NextToken = NextToken, MaxResults = MaxResults)
  output <- .rekognition$list_dataset_entries_output()
  config <- get_config()
  svc <- .rekognition$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.rekognition$operations$list_dataset_entries <- rekognition_list_dataset_entries

#' This operation applies only to Amazon Rekognition Custom Labels
#'
#' @description
#' This operation applies only to Amazon Rekognition Custom Labels.
#' 
#' Lists the labels in a dataset. Amazon Rekognition Custom Labels uses
#' labels to describe images. For more information, see [Labeling
#' images](https://docs.aws.amazon.com/rekognition/latest/customlabels-dg/md-labeling-images.html).
#' 
#' Lists the labels in a dataset. Amazon Rekognition Custom Labels uses
#' labels to describe images. For more information, see Labeling images in
#' the *Amazon Rekognition Custom Labels Developer Guide*.
#'
#' @usage
#' rekognition_list_dataset_labels(DatasetArn, NextToken, MaxResults)
#'
#' @param DatasetArn &#91;required&#93; The Amazon Resource Name (ARN) of the dataset that you want to use.
#' @param NextToken If the previous response was incomplete (because there is more results
#' to retrieve), Amazon Rekognition Custom Labels returns a pagination
#' token in the response. You can use this pagination token to retrieve the
#' next set of results.
#' @param MaxResults The maximum number of results to return per paginated call. The largest
#' value you can specify is 100. If you specify a value greater than 100, a
#' ValidationException error occurs. The default value is 100.
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   DatasetLabelDescriptions = list(
#'     list(
#'       LabelName = "string",
#'       LabelStats = list(
#'         EntryCount = 123,
#'         BoundingBoxCount = 123
#'       )
#'     )
#'   ),
#'   NextToken = "string"
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$list_dataset_labels(
#'   DatasetArn = "string",
#'   NextToken = "string",
#'   MaxResults = 123
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname rekognition_list_dataset_labels
#'
#' @aliases rekognition_list_dataset_labels
rekognition_list_dataset_labels <- function(DatasetArn, NextToken = NULL, MaxResults = NULL) {
  op <- new_operation(
    name = "ListDatasetLabels",
    http_method = "POST",
    http_path = "/",
    host_prefix = "",
    paginator = list(input_token = "NextToken", limit_key = "MaxResults", output_token = "NextToken", result_key = "DatasetLabelDescriptions"),
    stream_api = FALSE
  )
  input <- .rekognition$list_dataset_labels_input(DatasetArn = DatasetArn, NextToken = NextToken, MaxResults = MaxResults)
  output <- .rekognition$list_dataset_labels_output()
  config <- get_config()
  svc <- .rekognition$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.rekognition$operations$list_dataset_labels <- rekognition_list_dataset_labels

#' Returns metadata for faces in the specified collection
#'
#' @description
#' Returns metadata for faces in the specified collection. This metadata
#' includes information such as the bounding box coordinates, the
#' confidence (that the bounding box contains a face), and face ID. For an
#' example, see Listing Faces in a Collection in the Amazon Rekognition
#' Developer Guide.
#' 
#' This operation requires permissions to perform the
#' `rekognition:ListFaces` action.
#'
#' @usage
#' rekognition_list_faces(CollectionId, NextToken, MaxResults, UserId,
#'   FaceIds)
#'
#' @param CollectionId &#91;required&#93; ID of the collection from which to list the faces.
#' @param NextToken If the previous response was incomplete (because there is more data to
#' retrieve), Amazon Rekognition returns a pagination token in the
#' response. You can use this pagination token to retrieve the next set of
#' faces.
#' @param MaxResults Maximum number of faces to return.
#' @param UserId An array of user IDs to filter results with when listing faces in a
#' collection.
#' @param FaceIds An array of face IDs to filter results with when listing faces in a
#' collection.
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   Faces = list(
#'     list(
#'       FaceId = "string",
#'       BoundingBox = list(
#'         Width = 123.0,
#'         Height = 123.0,
#'         Left = 123.0,
#'         Top = 123.0
#'       ),
#'       ImageId = "string",
#'       ExternalImageId = "string",
#'       Confidence = 123.0,
#'       IndexFacesModelVersion = "string",
#'       UserId = "string"
#'     )
#'   ),
#'   NextToken = "string",
#'   FaceModelVersion = "string"
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$list_faces(
#'   CollectionId = "string",
#'   NextToken = "string",
#'   MaxResults = 123,
#'   UserId = "string",
#'   FaceIds = list(
#'     "string"
#'   )
#' )
#' ```
#'
#' @examples
#' \dontrun{
#' # This operation lists the faces in a Rekognition collection.
#' svc$list_faces(
#'   CollectionId = "myphotos",
#'   MaxResults = 20L
#' )
#' }
#'
#' @keywords internal
#'
#' @rdname rekognition_list_faces
#'
#' @aliases rekognition_list_faces
rekognition_list_faces <- function(CollectionId, NextToken = NULL, MaxResults = NULL, UserId = NULL, FaceIds = NULL) {
  op <- new_operation(
    name = "ListFaces",
    http_method = "POST",
    http_path = "/",
    host_prefix = "",
    paginator = list(input_token = "NextToken", limit_key = "MaxResults", output_token = "NextToken", result_key = "Faces", non_aggregate_keys = list("FaceModelVersion")),
    stream_api = FALSE
  )
  input <- .rekognition$list_faces_input(CollectionId = CollectionId, NextToken = NextToken, MaxResults = MaxResults, UserId = UserId, FaceIds = FaceIds)
  output <- .rekognition$list_faces_output()
  config <- get_config()
  svc <- .rekognition$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.rekognition$operations$list_faces <- rekognition_list_faces

#' Returns a list of media analysis jobs
#'
#' @description
#' Returns a list of media analysis jobs. Results are sorted by
#' `CreationTimestamp` in descending order.
#'
#' @usage
#' rekognition_list_media_analysis_jobs(NextToken, MaxResults)
#'
#' @param NextToken Pagination token, if the previous response was incomplete.
#' @param MaxResults The maximum number of results to return per paginated call. The largest
#' value user can specify is 100. If user specifies a value greater than
#' 100, an `InvalidParameterException` error occurs. The default value is
#' 100.
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   NextToken = "string",
#'   MediaAnalysisJobs = list(
#'     list(
#'       JobId = "string",
#'       JobName = "string",
#'       OperationsConfig = list(
#'         DetectModerationLabels = list(
#'           MinConfidence = 123.0,
#'           ProjectVersion = "string"
#'         )
#'       ),
#'       Status = "CREATED"|"QUEUED"|"IN_PROGRESS"|"SUCCEEDED"|"FAILED",
#'       FailureDetails = list(
#'         Code = "INTERNAL_ERROR"|"INVALID_S3_OBJECT"|"INVALID_MANIFEST"|"INVALID_OUTPUT_CONFIG"|"INVALID_KMS_KEY"|"ACCESS_DENIED"|"RESOURCE_NOT_FOUND"|"RESOURCE_NOT_READY"|"THROTTLED",
#'         Message = "string"
#'       ),
#'       CreationTimestamp = as.POSIXct(
#'         "2015-01-01"
#'       ),
#'       CompletionTimestamp = as.POSIXct(
#'         "2015-01-01"
#'       ),
#'       Input = list(
#'         S3Object = list(
#'           Bucket = "string",
#'           Name = "string",
#'           Version = "string"
#'         )
#'       ),
#'       OutputConfig = list(
#'         S3Bucket = "string",
#'         S3KeyPrefix = "string"
#'       ),
#'       KmsKeyId = "string",
#'       Results = list(
#'         S3Object = list(
#'           Bucket = "string",
#'           Name = "string",
#'           Version = "string"
#'         ),
#'         ModelVersions = list(
#'           Moderation = "string"
#'         )
#'       ),
#'       ManifestSummary = list(
#'         S3Object = list(
#'           Bucket = "string",
#'           Name = "string",
#'           Version = "string"
#'         )
#'       )
#'     )
#'   )
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$list_media_analysis_jobs(
#'   NextToken = "string",
#'   MaxResults = 123
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname rekognition_list_media_analysis_jobs
#'
#' @aliases rekognition_list_media_analysis_jobs
rekognition_list_media_analysis_jobs <- function(NextToken = NULL, MaxResults = NULL) {
  op <- new_operation(
    name = "ListMediaAnalysisJobs",
    http_method = "POST",
    http_path = "/",
    host_prefix = "",
    paginator = list(input_token = "NextToken", limit_key = "MaxResults", output_token = "NextToken"),
    stream_api = FALSE
  )
  input <- .rekognition$list_media_analysis_jobs_input(NextToken = NextToken, MaxResults = MaxResults)
  output <- .rekognition$list_media_analysis_jobs_output()
  config <- get_config()
  svc <- .rekognition$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.rekognition$operations$list_media_analysis_jobs <- rekognition_list_media_analysis_jobs

#' This operation applies only to Amazon Rekognition Custom Labels
#'
#' @description
#' This operation applies only to Amazon Rekognition Custom Labels.
#' 
#' Gets a list of the project policies attached to a project.
#' 
#' To attach a project policy to a project, call
#' [`put_project_policy`][rekognition_put_project_policy]. To remove a
#' project policy from a project, call
#' [`delete_project_policy`][rekognition_delete_project_policy].
#' 
#' This operation requires permissions to perform the
#' `rekognition:ListProjectPolicies` action.
#'
#' @usage
#' rekognition_list_project_policies(ProjectArn, NextToken, MaxResults)
#'
#' @param ProjectArn &#91;required&#93; The ARN of the project for which you want to list the project policies.
#' @param NextToken If the previous response was incomplete (because there is more results
#' to retrieve), Amazon Rekognition Custom Labels returns a pagination
#' token in the response. You can use this pagination token to retrieve the
#' next set of results.
#' @param MaxResults The maximum number of results to return per paginated call. The largest
#' value you can specify is 5. If you specify a value greater than 5, a
#' ValidationException error occurs. The default value is 5.
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   ProjectPolicies = list(
#'     list(
#'       ProjectArn = "string",
#'       PolicyName = "string",
#'       PolicyRevisionId = "string",
#'       PolicyDocument = "string",
#'       CreationTimestamp = as.POSIXct(
#'         "2015-01-01"
#'       ),
#'       LastUpdatedTimestamp = as.POSIXct(
#'         "2015-01-01"
#'       )
#'     )
#'   ),
#'   NextToken = "string"
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$list_project_policies(
#'   ProjectArn = "string",
#'   NextToken = "string",
#'   MaxResults = 123
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname rekognition_list_project_policies
#'
#' @aliases rekognition_list_project_policies
rekognition_list_project_policies <- function(ProjectArn, NextToken = NULL, MaxResults = NULL) {
  op <- new_operation(
    name = "ListProjectPolicies",
    http_method = "POST",
    http_path = "/",
    host_prefix = "",
    paginator = list(input_token = "NextToken", limit_key = "MaxResults", output_token = "NextToken", result_key = "ProjectPolicies"),
    stream_api = FALSE
  )
  input <- .rekognition$list_project_policies_input(ProjectArn = ProjectArn, NextToken = NextToken, MaxResults = MaxResults)
  output <- .rekognition$list_project_policies_output()
  config <- get_config()
  svc <- .rekognition$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.rekognition$operations$list_project_policies <- rekognition_list_project_policies

#' Gets a list of stream processors that you have created with
#' CreateStreamProcessor
#'
#' @description
#' Gets a list of stream processors that you have created with
#' [`create_stream_processor`][rekognition_create_stream_processor].
#'
#' @usage
#' rekognition_list_stream_processors(NextToken, MaxResults)
#'
#' @param NextToken If the previous response was incomplete (because there are more stream
#' processors to retrieve), Amazon Rekognition Video returns a pagination
#' token in the response. You can use this pagination token to retrieve the
#' next set of stream processors.
#' @param MaxResults Maximum number of stream processors you want Amazon Rekognition Video to
#' return in the response. The default is 1000.
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   NextToken = "string",
#'   StreamProcessors = list(
#'     list(
#'       Name = "string",
#'       Status = "STOPPED"|"STARTING"|"RUNNING"|"FAILED"|"STOPPING"|"UPDATING"
#'     )
#'   )
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$list_stream_processors(
#'   NextToken = "string",
#'   MaxResults = 123
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname rekognition_list_stream_processors
#'
#' @aliases rekognition_list_stream_processors
rekognition_list_stream_processors <- function(NextToken = NULL, MaxResults = NULL) {
  op <- new_operation(
    name = "ListStreamProcessors",
    http_method = "POST",
    http_path = "/",
    host_prefix = "",
    paginator = list(result_key = "StreamProcessors", output_token = "NextToken", input_token = "NextToken", limit_key = "MaxResults"),
    stream_api = FALSE
  )
  input <- .rekognition$list_stream_processors_input(NextToken = NextToken, MaxResults = MaxResults)
  output <- .rekognition$list_stream_processors_output()
  config <- get_config()
  svc <- .rekognition$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.rekognition$operations$list_stream_processors <- rekognition_list_stream_processors

#' Returns a list of tags in an Amazon Rekognition collection, stream
#' processor, or Custom Labels model
#'
#' @description
#' Returns a list of tags in an Amazon Rekognition collection, stream
#' processor, or Custom Labels model.
#' 
#' This operation requires permissions to perform the
#' `rekognition:ListTagsForResource` action.
#'
#' @usage
#' rekognition_list_tags_for_resource(ResourceArn)
#'
#' @param ResourceArn &#91;required&#93; Amazon Resource Name (ARN) of the model, collection, or stream processor
#' that contains the tags that you want a list of.
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   Tags = list(
#'     "string"
#'   )
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$list_tags_for_resource(
#'   ResourceArn = "string"
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname rekognition_list_tags_for_resource
#'
#' @aliases rekognition_list_tags_for_resource
rekognition_list_tags_for_resource <- function(ResourceArn) {
  op <- new_operation(
    name = "ListTagsForResource",
    http_method = "POST",
    http_path = "/",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .rekognition$list_tags_for_resource_input(ResourceArn = ResourceArn)
  output <- .rekognition$list_tags_for_resource_output()
  config <- get_config()
  svc <- .rekognition$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.rekognition$operations$list_tags_for_resource <- rekognition_list_tags_for_resource

#' Returns metadata of the User such as UserID in the specified collection
#'
#' @description
#' Returns metadata of the User such as `UserID` in the specified
#' collection. Anonymous User (to reserve faces without any identity) is
#' not returned as part of this request. The results are sorted by system
#' generated primary key ID. If the response is truncated, `NextToken` is
#' returned in the response that can be used in the subsequent request to
#' retrieve the next set of identities.
#'
#' @usage
#' rekognition_list_users(CollectionId, MaxResults, NextToken)
#'
#' @param CollectionId &#91;required&#93; The ID of an existing collection.
#' @param MaxResults Maximum number of UsersID to return.
#' @param NextToken Pagingation token to receive the next set of UsersID.
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   Users = list(
#'     list(
#'       UserId = "string",
#'       UserStatus = "ACTIVE"|"UPDATING"|"CREATING"|"CREATED"
#'     )
#'   ),
#'   NextToken = "string"
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$list_users(
#'   CollectionId = "string",
#'   MaxResults = 123,
#'   NextToken = "string"
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname rekognition_list_users
#'
#' @aliases rekognition_list_users
rekognition_list_users <- function(CollectionId, MaxResults = NULL, NextToken = NULL) {
  op <- new_operation(
    name = "ListUsers",
    http_method = "POST",
    http_path = "/",
    host_prefix = "",
    paginator = list(input_token = "NextToken", limit_key = "MaxResults", output_token = "NextToken", result_key = "Users"),
    stream_api = FALSE
  )
  input <- .rekognition$list_users_input(CollectionId = CollectionId, MaxResults = MaxResults, NextToken = NextToken)
  output <- .rekognition$list_users_output()
  config <- get_config()
  svc <- .rekognition$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.rekognition$operations$list_users <- rekognition_list_users

#' This operation applies only to Amazon Rekognition Custom Labels
#'
#' @description
#' This operation applies only to Amazon Rekognition Custom Labels.
#' 
#' Attaches a project policy to a Amazon Rekognition Custom Labels project
#' in a trusting AWS account. A project policy specifies that a trusted AWS
#' account can copy a model version from a trusting AWS account to a
#' project in the trusted AWS account. To copy a model version you use the
#' [`copy_project_version`][rekognition_copy_project_version] operation.
#' Only applies to Custom Labels projects.
#' 
#' For more information about the format of a project policy document, see
#' Attaching a project policy (SDK) in the *Amazon Rekognition Custom
#' Labels Developer Guide*.
#' 
#' The response from [`put_project_policy`][rekognition_put_project_policy]
#' is a revision ID for the project policy. You can attach multiple project
#' policies to a project. You can also update an existing project policy by
#' specifying the policy revision ID of the existing policy.
#' 
#' To remove a project policy from a project, call
#' [`delete_project_policy`][rekognition_delete_project_policy]. To get a
#' list of project policies attached to a project, call
#' [`list_project_policies`][rekognition_list_project_policies].
#' 
#' You copy a model version by calling
#' [`copy_project_version`][rekognition_copy_project_version].
#' 
#' This operation requires permissions to perform the
#' `rekognition:PutProjectPolicy` action.
#'
#' @usage
#' rekognition_put_project_policy(ProjectArn, PolicyName, PolicyRevisionId,
#'   PolicyDocument)
#'
#' @param ProjectArn &#91;required&#93; The Amazon Resource Name (ARN) of the project that the project policy is
#' attached to.
#' @param PolicyName &#91;required&#93; A name for the policy.
#' @param PolicyRevisionId The revision ID for the Project Policy. Each time you modify a policy,
#' Amazon Rekognition Custom Labels generates and assigns a new
#' `PolicyRevisionId` and then deletes the previous version of the policy.
#' @param PolicyDocument &#91;required&#93; A resource policy to add to the model. The policy is a JSON structure
#' that contains one or more statements that define the policy. The policy
#' must follow the IAM syntax. For more information about the contents of a
#' JSON policy document, see [IAM JSON policy
#' reference](https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies.html).
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   PolicyRevisionId = "string"
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$put_project_policy(
#'   ProjectArn = "string",
#'   PolicyName = "string",
#'   PolicyRevisionId = "string",
#'   PolicyDocument = "string"
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname rekognition_put_project_policy
#'
#' @aliases rekognition_put_project_policy
rekognition_put_project_policy <- function(ProjectArn, PolicyName, PolicyRevisionId = NULL, PolicyDocument) {
  op <- new_operation(
    name = "PutProjectPolicy",
    http_method = "POST",
    http_path = "/",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .rekognition$put_project_policy_input(ProjectArn = ProjectArn, PolicyName = PolicyName, PolicyRevisionId = PolicyRevisionId, PolicyDocument = PolicyDocument)
  output <- .rekognition$put_project_policy_output()
  config <- get_config()
  svc <- .rekognition$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.rekognition$operations$put_project_policy <- rekognition_put_project_policy

#' Returns an array of celebrities recognized in the input image
#'
#' @description
#' Returns an array of celebrities recognized in the input image. For more
#' information, see Recognizing celebrities in the Amazon Rekognition
#' Developer Guide.
#' 
#' [`recognize_celebrities`][rekognition_recognize_celebrities] returns the
#' 64 largest faces in the image. It lists the recognized celebrities in
#' the `CelebrityFaces` array and any unrecognized faces in the
#' `UnrecognizedFaces` array.
#' [`recognize_celebrities`][rekognition_recognize_celebrities] doesn't
#' return celebrities whose faces aren't among the largest 64 faces in the
#' image.
#' 
#' For each celebrity recognized,
#' [`recognize_celebrities`][rekognition_recognize_celebrities] returns a
#' `Celebrity` object. The `Celebrity` object contains the celebrity name,
#' ID, URL links to additional information, match confidence, and a
#' `ComparedFace` object that you can use to locate the celebrity's face on
#' the image.
#' 
#' Amazon Rekognition doesn't retain information about which images a
#' celebrity has been recognized in. Your application must store this
#' information and use the `Celebrity` ID property as a unique identifier
#' for the celebrity. If you don't store the celebrity name or additional
#' information URLs returned by
#' [`recognize_celebrities`][rekognition_recognize_celebrities], you will
#' need the ID to identify the celebrity in a call to the
#' [`get_celebrity_info`][rekognition_get_celebrity_info] operation.
#' 
#' You pass the input image either as base64-encoded image bytes or as a
#' reference to an image in an Amazon S3 bucket. If you use the AWS CLI to
#' call Amazon Rekognition operations, passing image bytes is not
#' supported. The image must be either a PNG or JPEG formatted file.
#' 
#' For an example, see Recognizing celebrities in an image in the Amazon
#' Rekognition Developer Guide.
#' 
#' This operation requires permissions to perform the
#' `rekognition:RecognizeCelebrities` operation.
#'
#' @usage
#' rekognition_recognize_celebrities(Image)
#'
#' @param Image &#91;required&#93; The input image as base64-encoded bytes or an S3 object. If you use the
#' AWS CLI to call Amazon Rekognition operations, passing base64-encoded
#' image bytes is not supported.
#' 
#' If you are using an AWS SDK to call Amazon Rekognition, you might not
#' need to base64-encode image bytes passed using the `Bytes` field. For
#' more information, see Images in the Amazon Rekognition developer guide.
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   CelebrityFaces = list(
#'     list(
#'       Urls = list(
#'         "string"
#'       ),
#'       Name = "string",
#'       Id = "string",
#'       Face = list(
#'         BoundingBox = list(
#'           Width = 123.0,
#'           Height = 123.0,
#'           Left = 123.0,
#'           Top = 123.0
#'         ),
#'         Confidence = 123.0,
#'         Landmarks = list(
#'           list(
#'             Type = "eyeLeft"|"eyeRight"|"nose"|"mouthLeft"|"mouthRight"|"leftEyeBrowLeft"|"leftEyeBrowRight"|"leftEyeBrowUp"|"rightEyeBrowLeft"|"rightEyeBrowRight"|"rightEyeBrowUp"|"leftEyeLeft"|"leftEyeRight"|"leftEyeUp"|"leftEyeDown"|"rightEyeLeft"|"rightEyeRight"|"rightEyeUp"|"rightEyeDown"|"noseLeft"|"noseRight"|"mouthUp"|"mouthDown"|"leftPupil"|"rightPupil"|"upperJawlineLeft"|"midJawlineLeft"|"chinBottom"|"midJawlineRight"|"upperJawlineRight",
#'             X = 123.0,
#'             Y = 123.0
#'           )
#'         ),
#'         Pose = list(
#'           Roll = 123.0,
#'           Yaw = 123.0,
#'           Pitch = 123.0
#'         ),
#'         Quality = list(
#'           Brightness = 123.0,
#'           Sharpness = 123.0
#'         ),
#'         Emotions = list(
#'           list(
#'             Type = "HAPPY"|"SAD"|"ANGRY"|"CONFUSED"|"DISGUSTED"|"SURPRISED"|"CALM"|"UNKNOWN"|"FEAR",
#'             Confidence = 123.0
#'           )
#'         ),
#'         Smile = list(
#'           Value = TRUE|FALSE,
#'           Confidence = 123.0
#'         )
#'       ),
#'       MatchConfidence = 123.0,
#'       KnownGender = list(
#'         Type = "Male"|"Female"|"Nonbinary"|"Unlisted"
#'       )
#'     )
#'   ),
#'   UnrecognizedFaces = list(
#'     list(
#'       BoundingBox = list(
#'         Width = 123.0,
#'         Height = 123.0,
#'         Left = 123.0,
#'         Top = 123.0
#'       ),
#'       Confidence = 123.0,
#'       Landmarks = list(
#'         list(
#'           Type = "eyeLeft"|"eyeRight"|"nose"|"mouthLeft"|"mouthRight"|"leftEyeBrowLeft"|"leftEyeBrowRight"|"leftEyeBrowUp"|"rightEyeBrowLeft"|"rightEyeBrowRight"|"rightEyeBrowUp"|"leftEyeLeft"|"leftEyeRight"|"leftEyeUp"|"leftEyeDown"|"rightEyeLeft"|"rightEyeRight"|"rightEyeUp"|"rightEyeDown"|"noseLeft"|"noseRight"|"mouthUp"|"mouthDown"|"leftPupil"|"rightPupil"|"upperJawlineLeft"|"midJawlineLeft"|"chinBottom"|"midJawlineRight"|"upperJawlineRight",
#'           X = 123.0,
#'           Y = 123.0
#'         )
#'       ),
#'       Pose = list(
#'         Roll = 123.0,
#'         Yaw = 123.0,
#'         Pitch = 123.0
#'       ),
#'       Quality = list(
#'         Brightness = 123.0,
#'         Sharpness = 123.0
#'       ),
#'       Emotions = list(
#'         list(
#'           Type = "HAPPY"|"SAD"|"ANGRY"|"CONFUSED"|"DISGUSTED"|"SURPRISED"|"CALM"|"UNKNOWN"|"FEAR",
#'           Confidence = 123.0
#'         )
#'       ),
#'       Smile = list(
#'         Value = TRUE|FALSE,
#'         Confidence = 123.0
#'       )
#'     )
#'   ),
#'   OrientationCorrection = "ROTATE_0"|"ROTATE_90"|"ROTATE_180"|"ROTATE_270"
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$recognize_celebrities(
#'   Image = list(
#'     Bytes = raw,
#'     S3Object = list(
#'       Bucket = "string",
#'       Name = "string",
#'       Version = "string"
#'     )
#'   )
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname rekognition_recognize_celebrities
#'
#' @aliases rekognition_recognize_celebrities
rekognition_recognize_celebrities <- function(Image) {
  op <- new_operation(
    name = "RecognizeCelebrities",
    http_method = "POST",
    http_path = "/",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .rekognition$recognize_celebrities_input(Image = Image)
  output <- .rekognition$recognize_celebrities_output()
  config <- get_config()
  svc <- .rekognition$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.rekognition$operations$recognize_celebrities <- rekognition_recognize_celebrities

#' For a given input face ID, searches for matching faces in the collection
#' the face belongs to
#'
#' @description
#' For a given input face ID, searches for matching faces in the collection
#' the face belongs to. You get a face ID when you add a face to the
#' collection using the [`index_faces`][rekognition_index_faces] operation.
#' The operation compares the features of the input face with faces in the
#' specified collection.
#' 
#' You can also search faces without indexing faces by using the
#' [`search_faces_by_image`][rekognition_search_faces_by_image] operation.
#' 
#' The operation response returns an array of faces that match, ordered by
#' similarity score with the highest similarity first. More specifically,
#' it is an array of metadata for each face match that is found. Along with
#' the metadata, the response also includes a `confidence` value for each
#' face match, indicating the confidence that the specific face matches the
#' input face.
#' 
#' For an example, see Searching for a face using its face ID in the Amazon
#' Rekognition Developer Guide.
#' 
#' This operation requires permissions to perform the
#' `rekognition:SearchFaces` action.
#'
#' @usage
#' rekognition_search_faces(CollectionId, FaceId, MaxFaces,
#'   FaceMatchThreshold)
#'
#' @param CollectionId &#91;required&#93; ID of the collection the face belongs to.
#' @param FaceId &#91;required&#93; ID of a face to find matches for in the collection.
#' @param MaxFaces Maximum number of faces to return. The operation returns the maximum
#' number of faces with the highest confidence in the match.
#' @param FaceMatchThreshold Optional value specifying the minimum confidence in the face match to
#' return. For example, don't return any matches where confidence in
#' matches is less than 70%. The default value is 80%.
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   SearchedFaceId = "string",
#'   FaceMatches = list(
#'     list(
#'       Similarity = 123.0,
#'       Face = list(
#'         FaceId = "string",
#'         BoundingBox = list(
#'           Width = 123.0,
#'           Height = 123.0,
#'           Left = 123.0,
#'           Top = 123.0
#'         ),
#'         ImageId = "string",
#'         ExternalImageId = "string",
#'         Confidence = 123.0,
#'         IndexFacesModelVersion = "string",
#'         UserId = "string"
#'       )
#'     )
#'   ),
#'   FaceModelVersion = "string"
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$search_faces(
#'   CollectionId = "string",
#'   FaceId = "string",
#'   MaxFaces = 123,
#'   FaceMatchThreshold = 123.0
#' )
#' ```
#'
#' @examples
#' \dontrun{
#' # This operation searches for matching faces in the collection the
#' # supplied face belongs to.
#' svc$search_faces(
#'   CollectionId = "myphotos",
#'   FaceId = "70008e50-75e4-55d0-8e80-363fb73b3a14",
#'   FaceMatchThreshold = 90L,
#'   MaxFaces = 10L
#' )
#' }
#'
#' @keywords internal
#'
#' @rdname rekognition_search_faces
#'
#' @aliases rekognition_search_faces
rekognition_search_faces <- function(CollectionId, FaceId, MaxFaces = NULL, FaceMatchThreshold = NULL) {
  op <- new_operation(
    name = "SearchFaces",
    http_method = "POST",
    http_path = "/",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .rekognition$search_faces_input(CollectionId = CollectionId, FaceId = FaceId, MaxFaces = MaxFaces, FaceMatchThreshold = FaceMatchThreshold)
  output <- .rekognition$search_faces_output()
  config <- get_config()
  svc <- .rekognition$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.rekognition$operations$search_faces <- rekognition_search_faces

#' For a given input image, first detects the largest face in the image,
#' and then searches the specified collection for matching faces
#'
#' @description
#' For a given input image, first detects the largest face in the image,
#' and then searches the specified collection for matching faces. The
#' operation compares the features of the input face with faces in the
#' specified collection.
#' 
#' To search for all faces in an input image, you might first call the
#' [`index_faces`][rekognition_index_faces] operation, and then use the
#' face IDs returned in subsequent calls to the
#' [`search_faces`][rekognition_search_faces] operation.
#' 
#' You can also call the [`detect_faces`][rekognition_detect_faces]
#' operation and use the bounding boxes in the response to make face crops,
#' which then you can pass in to the
#' [`search_faces_by_image`][rekognition_search_faces_by_image] operation.
#' 
#' You pass the input image either as base64-encoded image bytes or as a
#' reference to an image in an Amazon S3 bucket. If you use the AWS CLI to
#' call Amazon Rekognition operations, passing image bytes is not
#' supported. The image must be either a PNG or JPEG formatted file.
#' 
#' The response returns an array of faces that match, ordered by similarity
#' score with the highest similarity first. More specifically, it is an
#' array of metadata for each face match found. Along with the metadata,
#' the response also includes a `similarity` indicating how similar the
#' face is to the input face. In the response, the operation also returns
#' the bounding box (and a confidence level that the bounding box contains
#' a face) of the face that Amazon Rekognition used for the input image.
#' 
#' If no faces are detected in the input image,
#' [`search_faces_by_image`][rekognition_search_faces_by_image] returns an
#' `InvalidParameterException` error.
#' 
#' For an example, Searching for a Face Using an Image in the Amazon
#' Rekognition Developer Guide.
#' 
#' The `QualityFilter` input parameter allows you to filter out detected
#' faces that don’t meet a required quality bar. The quality bar is based
#' on a variety of common use cases. Use `QualityFilter` to set the quality
#' bar for filtering by specifying `LOW`, `MEDIUM`, or `HIGH`. If you do
#' not want to filter detected faces, specify `NONE`. The default value is
#' `NONE`.
#' 
#' To use quality filtering, you need a collection associated with version
#' 3 of the face model or higher. To get the version of the face model
#' associated with a collection, call
#' [`describe_collection`][rekognition_describe_collection].
#' 
#' This operation requires permissions to perform the
#' `rekognition:SearchFacesByImage` action.
#'
#' @usage
#' rekognition_search_faces_by_image(CollectionId, Image, MaxFaces,
#'   FaceMatchThreshold, QualityFilter)
#'
#' @param CollectionId &#91;required&#93; ID of the collection to search.
#' @param Image &#91;required&#93; The input image as base64-encoded bytes or an S3 object. If you use the
#' AWS CLI to call Amazon Rekognition operations, passing base64-encoded
#' image bytes is not supported.
#' 
#' If you are using an AWS SDK to call Amazon Rekognition, you might not
#' need to base64-encode image bytes passed using the `Bytes` field. For
#' more information, see Images in the Amazon Rekognition developer guide.
#' @param MaxFaces Maximum number of faces to return. The operation returns the maximum
#' number of faces with the highest confidence in the match.
#' @param FaceMatchThreshold (Optional) Specifies the minimum confidence in the face match to return.
#' For example, don't return any matches where confidence in matches is
#' less than 70%. The default value is 80%.
#' @param QualityFilter A filter that specifies a quality bar for how much filtering is done to
#' identify faces. Filtered faces aren't searched for in the collection. If
#' you specify `AUTO`, Amazon Rekognition chooses the quality bar. If you
#' specify `LOW`, `MEDIUM`, or `HIGH`, filtering removes all faces that
#' don’t meet the chosen quality bar. The quality bar is based on a variety
#' of common use cases. Low-quality detections can occur for a number of
#' reasons. Some examples are an object that's misidentified as a face, a
#' face that's too blurry, or a face with a pose that's too extreme to use.
#' If you specify `NONE`, no filtering is performed. The default value is
#' `NONE`.
#' 
#' To use quality filtering, the collection you are using must be
#' associated with version 3 of the face model or higher.
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   SearchedFaceBoundingBox = list(
#'     Width = 123.0,
#'     Height = 123.0,
#'     Left = 123.0,
#'     Top = 123.0
#'   ),
#'   SearchedFaceConfidence = 123.0,
#'   FaceMatches = list(
#'     list(
#'       Similarity = 123.0,
#'       Face = list(
#'         FaceId = "string",
#'         BoundingBox = list(
#'           Width = 123.0,
#'           Height = 123.0,
#'           Left = 123.0,
#'           Top = 123.0
#'         ),
#'         ImageId = "string",
#'         ExternalImageId = "string",
#'         Confidence = 123.0,
#'         IndexFacesModelVersion = "string",
#'         UserId = "string"
#'       )
#'     )
#'   ),
#'   FaceModelVersion = "string"
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$search_faces_by_image(
#'   CollectionId = "string",
#'   Image = list(
#'     Bytes = raw,
#'     S3Object = list(
#'       Bucket = "string",
#'       Name = "string",
#'       Version = "string"
#'     )
#'   ),
#'   MaxFaces = 123,
#'   FaceMatchThreshold = 123.0,
#'   QualityFilter = "NONE"|"AUTO"|"LOW"|"MEDIUM"|"HIGH"
#' )
#' ```
#'
#' @examples
#' \dontrun{
#' # This operation searches for faces in a Rekognition collection that match
#' # the largest face in an S3 bucket stored image.
#' svc$search_faces_by_image(
#'   CollectionId = "myphotos",
#'   FaceMatchThreshold = 95L,
#'   Image = list(
#'     S3Object = list(
#'       Bucket = "mybucket",
#'       Name = "myphoto"
#'     )
#'   ),
#'   MaxFaces = 5L
#' )
#' }
#'
#' @keywords internal
#'
#' @rdname rekognition_search_faces_by_image
#'
#' @aliases rekognition_search_faces_by_image
rekognition_search_faces_by_image <- function(CollectionId, Image, MaxFaces = NULL, FaceMatchThreshold = NULL, QualityFilter = NULL) {
  op <- new_operation(
    name = "SearchFacesByImage",
    http_method = "POST",
    http_path = "/",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .rekognition$search_faces_by_image_input(CollectionId = CollectionId, Image = Image, MaxFaces = MaxFaces, FaceMatchThreshold = FaceMatchThreshold, QualityFilter = QualityFilter)
  output <- .rekognition$search_faces_by_image_output()
  config <- get_config()
  svc <- .rekognition$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.rekognition$operations$search_faces_by_image <- rekognition_search_faces_by_image

#' Searches for UserIDs within a collection based on a FaceId or UserId
#'
#' @description
#' Searches for UserIDs within a collection based on a `FaceId` or
#' `UserId`. This API can be used to find the closest UserID (with a
#' highest similarity) to associate a face. The request must be provided
#' with either `FaceId` or `UserId`. The operation returns an array of
#' UserID that match the `FaceId` or `UserId`, ordered by similarity score
#' with the highest similarity first.
#'
#' @usage
#' rekognition_search_users(CollectionId, UserId, FaceId,
#'   UserMatchThreshold, MaxUsers)
#'
#' @param CollectionId &#91;required&#93; The ID of an existing collection containing the UserID, used with a
#' UserId or FaceId. If a FaceId is provided, UserId isn’t required to be
#' present in the Collection.
#' @param UserId ID for the existing User.
#' @param FaceId ID for the existing face.
#' @param UserMatchThreshold Optional value that specifies the minimum confidence in the matched
#' UserID to return. Default value of 80.
#' @param MaxUsers Maximum number of identities to return.
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   UserMatches = list(
#'     list(
#'       Similarity = 123.0,
#'       User = list(
#'         UserId = "string",
#'         UserStatus = "ACTIVE"|"UPDATING"|"CREATING"|"CREATED"
#'       )
#'     )
#'   ),
#'   FaceModelVersion = "string",
#'   SearchedFace = list(
#'     FaceId = "string"
#'   ),
#'   SearchedUser = list(
#'     UserId = "string"
#'   )
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$search_users(
#'   CollectionId = "string",
#'   UserId = "string",
#'   FaceId = "string",
#'   UserMatchThreshold = 123.0,
#'   MaxUsers = 123
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname rekognition_search_users
#'
#' @aliases rekognition_search_users
rekognition_search_users <- function(CollectionId, UserId = NULL, FaceId = NULL, UserMatchThreshold = NULL, MaxUsers = NULL) {
  op <- new_operation(
    name = "SearchUsers",
    http_method = "POST",
    http_path = "/",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .rekognition$search_users_input(CollectionId = CollectionId, UserId = UserId, FaceId = FaceId, UserMatchThreshold = UserMatchThreshold, MaxUsers = MaxUsers)
  output <- .rekognition$search_users_output()
  config <- get_config()
  svc <- .rekognition$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.rekognition$operations$search_users <- rekognition_search_users

#' Searches for UserIDs using a supplied image
#'
#' @description
#' Searches for UserIDs using a supplied image. It first detects the
#' largest face in the image, and then searches a specified collection for
#' matching UserIDs.
#' 
#' The operation returns an array of UserIDs that match the face in the
#' supplied image, ordered by similarity score with the highest similarity
#' first. It also returns a bounding box for the face found in the input
#' image.
#' 
#' Information about faces detected in the supplied image, but not used for
#' the search, is returned in an array of `UnsearchedFace` objects. If no
#' valid face is detected in the image, the response will contain an empty
#' `UserMatches` list and no `SearchedFace` object.
#'
#' @usage
#' rekognition_search_users_by_image(CollectionId, Image,
#'   UserMatchThreshold, MaxUsers, QualityFilter)
#'
#' @param CollectionId &#91;required&#93; The ID of an existing collection containing the UserID.
#' @param Image &#91;required&#93; 
#' @param UserMatchThreshold Specifies the minimum confidence in the UserID match to return. Default
#' value is 80.
#' @param MaxUsers Maximum number of UserIDs to return.
#' @param QualityFilter A filter that specifies a quality bar for how much filtering is done to
#' identify faces. Filtered faces aren't searched for in the collection.
#' The default value is NONE.
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   UserMatches = list(
#'     list(
#'       Similarity = 123.0,
#'       User = list(
#'         UserId = "string",
#'         UserStatus = "ACTIVE"|"UPDATING"|"CREATING"|"CREATED"
#'       )
#'     )
#'   ),
#'   FaceModelVersion = "string",
#'   SearchedFace = list(
#'     FaceDetail = list(
#'       BoundingBox = list(
#'         Width = 123.0,
#'         Height = 123.0,
#'         Left = 123.0,
#'         Top = 123.0
#'       ),
#'       AgeRange = list(
#'         Low = 123,
#'         High = 123
#'       ),
#'       Smile = list(
#'         Value = TRUE|FALSE,
#'         Confidence = 123.0
#'       ),
#'       Eyeglasses = list(
#'         Value = TRUE|FALSE,
#'         Confidence = 123.0
#'       ),
#'       Sunglasses = list(
#'         Value = TRUE|FALSE,
#'         Confidence = 123.0
#'       ),
#'       Gender = list(
#'         Value = "Male"|"Female",
#'         Confidence = 123.0
#'       ),
#'       Beard = list(
#'         Value = TRUE|FALSE,
#'         Confidence = 123.0
#'       ),
#'       Mustache = list(
#'         Value = TRUE|FALSE,
#'         Confidence = 123.0
#'       ),
#'       EyesOpen = list(
#'         Value = TRUE|FALSE,
#'         Confidence = 123.0
#'       ),
#'       MouthOpen = list(
#'         Value = TRUE|FALSE,
#'         Confidence = 123.0
#'       ),
#'       Emotions = list(
#'         list(
#'           Type = "HAPPY"|"SAD"|"ANGRY"|"CONFUSED"|"DISGUSTED"|"SURPRISED"|"CALM"|"UNKNOWN"|"FEAR",
#'           Confidence = 123.0
#'         )
#'       ),
#'       Landmarks = list(
#'         list(
#'           Type = "eyeLeft"|"eyeRight"|"nose"|"mouthLeft"|"mouthRight"|"leftEyeBrowLeft"|"leftEyeBrowRight"|"leftEyeBrowUp"|"rightEyeBrowLeft"|"rightEyeBrowRight"|"rightEyeBrowUp"|"leftEyeLeft"|"leftEyeRight"|"leftEyeUp"|"leftEyeDown"|"rightEyeLeft"|"rightEyeRight"|"rightEyeUp"|"rightEyeDown"|"noseLeft"|"noseRight"|"mouthUp"|"mouthDown"|"leftPupil"|"rightPupil"|"upperJawlineLeft"|"midJawlineLeft"|"chinBottom"|"midJawlineRight"|"upperJawlineRight",
#'           X = 123.0,
#'           Y = 123.0
#'         )
#'       ),
#'       Pose = list(
#'         Roll = 123.0,
#'         Yaw = 123.0,
#'         Pitch = 123.0
#'       ),
#'       Quality = list(
#'         Brightness = 123.0,
#'         Sharpness = 123.0
#'       ),
#'       Confidence = 123.0,
#'       FaceOccluded = list(
#'         Value = TRUE|FALSE,
#'         Confidence = 123.0
#'       ),
#'       EyeDirection = list(
#'         Yaw = 123.0,
#'         Pitch = 123.0,
#'         Confidence = 123.0
#'       )
#'     )
#'   ),
#'   UnsearchedFaces = list(
#'     list(
#'       FaceDetails = list(
#'         BoundingBox = list(
#'           Width = 123.0,
#'           Height = 123.0,
#'           Left = 123.0,
#'           Top = 123.0
#'         ),
#'         AgeRange = list(
#'           Low = 123,
#'           High = 123
#'         ),
#'         Smile = list(
#'           Value = TRUE|FALSE,
#'           Confidence = 123.0
#'         ),
#'         Eyeglasses = list(
#'           Value = TRUE|FALSE,
#'           Confidence = 123.0
#'         ),
#'         Sunglasses = list(
#'           Value = TRUE|FALSE,
#'           Confidence = 123.0
#'         ),
#'         Gender = list(
#'           Value = "Male"|"Female",
#'           Confidence = 123.0
#'         ),
#'         Beard = list(
#'           Value = TRUE|FALSE,
#'           Confidence = 123.0
#'         ),
#'         Mustache = list(
#'           Value = TRUE|FALSE,
#'           Confidence = 123.0
#'         ),
#'         EyesOpen = list(
#'           Value = TRUE|FALSE,
#'           Confidence = 123.0
#'         ),
#'         MouthOpen = list(
#'           Value = TRUE|FALSE,
#'           Confidence = 123.0
#'         ),
#'         Emotions = list(
#'           list(
#'             Type = "HAPPY"|"SAD"|"ANGRY"|"CONFUSED"|"DISGUSTED"|"SURPRISED"|"CALM"|"UNKNOWN"|"FEAR",
#'             Confidence = 123.0
#'           )
#'         ),
#'         Landmarks = list(
#'           list(
#'             Type = "eyeLeft"|"eyeRight"|"nose"|"mouthLeft"|"mouthRight"|"leftEyeBrowLeft"|"leftEyeBrowRight"|"leftEyeBrowUp"|"rightEyeBrowLeft"|"rightEyeBrowRight"|"rightEyeBrowUp"|"leftEyeLeft"|"leftEyeRight"|"leftEyeUp"|"leftEyeDown"|"rightEyeLeft"|"rightEyeRight"|"rightEyeUp"|"rightEyeDown"|"noseLeft"|"noseRight"|"mouthUp"|"mouthDown"|"leftPupil"|"rightPupil"|"upperJawlineLeft"|"midJawlineLeft"|"chinBottom"|"midJawlineRight"|"upperJawlineRight",
#'             X = 123.0,
#'             Y = 123.0
#'           )
#'         ),
#'         Pose = list(
#'           Roll = 123.0,
#'           Yaw = 123.0,
#'           Pitch = 123.0
#'         ),
#'         Quality = list(
#'           Brightness = 123.0,
#'           Sharpness = 123.0
#'         ),
#'         Confidence = 123.0,
#'         FaceOccluded = list(
#'           Value = TRUE|FALSE,
#'           Confidence = 123.0
#'         ),
#'         EyeDirection = list(
#'           Yaw = 123.0,
#'           Pitch = 123.0,
#'           Confidence = 123.0
#'         )
#'       ),
#'       Reasons = list(
#'         "FACE_NOT_LARGEST"|"EXCEEDS_MAX_FACES"|"EXTREME_POSE"|"LOW_BRIGHTNESS"|"LOW_SHARPNESS"|"LOW_CONFIDENCE"|"SMALL_BOUNDING_BOX"|"LOW_FACE_QUALITY"
#'       )
#'     )
#'   )
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$search_users_by_image(
#'   CollectionId = "string",
#'   Image = list(
#'     Bytes = raw,
#'     S3Object = list(
#'       Bucket = "string",
#'       Name = "string",
#'       Version = "string"
#'     )
#'   ),
#'   UserMatchThreshold = 123.0,
#'   MaxUsers = 123,
#'   QualityFilter = "NONE"|"AUTO"|"LOW"|"MEDIUM"|"HIGH"
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname rekognition_search_users_by_image
#'
#' @aliases rekognition_search_users_by_image
rekognition_search_users_by_image <- function(CollectionId, Image, UserMatchThreshold = NULL, MaxUsers = NULL, QualityFilter = NULL) {
  op <- new_operation(
    name = "SearchUsersByImage",
    http_method = "POST",
    http_path = "/",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .rekognition$search_users_by_image_input(CollectionId = CollectionId, Image = Image, UserMatchThreshold = UserMatchThreshold, MaxUsers = MaxUsers, QualityFilter = QualityFilter)
  output <- .rekognition$search_users_by_image_output()
  config <- get_config()
  svc <- .rekognition$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.rekognition$operations$search_users_by_image <- rekognition_search_users_by_image

#' Starts asynchronous recognition of celebrities in a stored video
#'
#' @description
#' Starts asynchronous recognition of celebrities in a stored video.
#' 
#' Amazon Rekognition Video can detect celebrities in a video must be
#' stored in an Amazon S3 bucket. Use Video to specify the bucket name and
#' the filename of the video.
#' [`start_celebrity_recognition`][rekognition_start_celebrity_recognition]
#' returns a job identifier (`JobId`) which you use to get the results of
#' the analysis. When celebrity recognition analysis is finished, Amazon
#' Rekognition Video publishes a completion status to the Amazon Simple
#' Notification Service topic that you specify in `NotificationChannel`. To
#' get the results of the celebrity recognition analysis, first check that
#' the status value published to the Amazon SNS topic is `SUCCEEDED`. If
#' so, call
#' [`get_celebrity_recognition`][rekognition_get_celebrity_recognition] and
#' pass the job identifier (`JobId`) from the initial call to
#' [`start_celebrity_recognition`][rekognition_start_celebrity_recognition].
#' 
#' For more information, see Recognizing celebrities in the Amazon
#' Rekognition Developer Guide.
#'
#' @usage
#' rekognition_start_celebrity_recognition(Video, ClientRequestToken,
#'   NotificationChannel, JobTag)
#'
#' @param Video &#91;required&#93; The video in which you want to recognize celebrities. The video must be
#' stored in an Amazon S3 bucket.
#' @param ClientRequestToken Idempotent token used to identify the start request. If you use the same
#' token with multiple
#' [`start_celebrity_recognition`][rekognition_start_celebrity_recognition]
#' requests, the same `JobId` is returned. Use `ClientRequestToken` to
#' prevent the same job from being accidently started more than once.
#' @param NotificationChannel The Amazon SNS topic ARN that you want Amazon Rekognition Video to
#' publish the completion status of the celebrity recognition analysis to.
#' The Amazon SNS topic must have a topic name that begins with
#' *AmazonRekognition* if you are using the AmazonRekognitionServiceRole
#' permissions policy.
#' @param JobTag An identifier you specify that's returned in the completion notification
#' that's published to your Amazon Simple Notification Service topic. For
#' example, you can use `JobTag` to group related jobs and identify them in
#' the completion notification.
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   JobId = "string"
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$start_celebrity_recognition(
#'   Video = list(
#'     S3Object = list(
#'       Bucket = "string",
#'       Name = "string",
#'       Version = "string"
#'     )
#'   ),
#'   ClientRequestToken = "string",
#'   NotificationChannel = list(
#'     SNSTopicArn = "string",
#'     RoleArn = "string"
#'   ),
#'   JobTag = "string"
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname rekognition_start_celebrity_recognition
#'
#' @aliases rekognition_start_celebrity_recognition
rekognition_start_celebrity_recognition <- function(Video, ClientRequestToken = NULL, NotificationChannel = NULL, JobTag = NULL) {
  op <- new_operation(
    name = "StartCelebrityRecognition",
    http_method = "POST",
    http_path = "/",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .rekognition$start_celebrity_recognition_input(Video = Video, ClientRequestToken = ClientRequestToken, NotificationChannel = NotificationChannel, JobTag = JobTag)
  output <- .rekognition$start_celebrity_recognition_output()
  config <- get_config()
  svc <- .rekognition$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.rekognition$operations$start_celebrity_recognition <- rekognition_start_celebrity_recognition

#' Starts asynchronous detection of inappropriate, unwanted, or offensive
#' content in a stored video
#'
#' @description
#' Starts asynchronous detection of inappropriate, unwanted, or offensive
#' content in a stored video. For a list of moderation labels in Amazon
#' Rekognition, see [Using the image and video moderation
#' APIs](https://docs.aws.amazon.com/rekognition/latest/dg/moderation.html#moderation-api).
#' 
#' Amazon Rekognition Video can moderate content in a video stored in an
#' Amazon S3 bucket. Use Video to specify the bucket name and the filename
#' of the video.
#' [`start_content_moderation`][rekognition_start_content_moderation]
#' returns a job identifier (`JobId`) which you use to get the results of
#' the analysis. When content analysis is finished, Amazon Rekognition
#' Video publishes a completion status to the Amazon Simple Notification
#' Service topic that you specify in `NotificationChannel`.
#' 
#' To get the results of the content analysis, first check that the status
#' value published to the Amazon SNS topic is `SUCCEEDED`. If so, call
#' [`get_content_moderation`][rekognition_get_content_moderation] and pass
#' the job identifier (`JobId`) from the initial call to
#' [`start_content_moderation`][rekognition_start_content_moderation].
#' 
#' For more information, see Moderating content in the Amazon Rekognition
#' Developer Guide.
#'
#' @usage
#' rekognition_start_content_moderation(Video, MinConfidence,
#'   ClientRequestToken, NotificationChannel, JobTag)
#'
#' @param Video &#91;required&#93; The video in which you want to detect inappropriate, unwanted, or
#' offensive content. The video must be stored in an Amazon S3 bucket.
#' @param MinConfidence Specifies the minimum confidence that Amazon Rekognition must have in
#' order to return a moderated content label. Confidence represents how
#' certain Amazon Rekognition is that the moderated content is correctly
#' identified. 0 is the lowest confidence. 100 is the highest confidence.
#' Amazon Rekognition doesn't return any moderated content labels with a
#' confidence level lower than this specified value. If you don't specify
#' `MinConfidence`,
#' [`get_content_moderation`][rekognition_get_content_moderation] returns
#' labels with confidence values greater than or equal to 50 percent.
#' @param ClientRequestToken Idempotent token used to identify the start request. If you use the same
#' token with multiple
#' [`start_content_moderation`][rekognition_start_content_moderation]
#' requests, the same `JobId` is returned. Use `ClientRequestToken` to
#' prevent the same job from being accidently started more than once.
#' @param NotificationChannel The Amazon SNS topic ARN that you want Amazon Rekognition Video to
#' publish the completion status of the content analysis to. The Amazon SNS
#' topic must have a topic name that begins with *AmazonRekognition* if you
#' are using the AmazonRekognitionServiceRole permissions policy to access
#' the topic.
#' @param JobTag An identifier you specify that's returned in the completion notification
#' that's published to your Amazon Simple Notification Service topic. For
#' example, you can use `JobTag` to group related jobs and identify them in
#' the completion notification.
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   JobId = "string"
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$start_content_moderation(
#'   Video = list(
#'     S3Object = list(
#'       Bucket = "string",
#'       Name = "string",
#'       Version = "string"
#'     )
#'   ),
#'   MinConfidence = 123.0,
#'   ClientRequestToken = "string",
#'   NotificationChannel = list(
#'     SNSTopicArn = "string",
#'     RoleArn = "string"
#'   ),
#'   JobTag = "string"
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname rekognition_start_content_moderation
#'
#' @aliases rekognition_start_content_moderation
rekognition_start_content_moderation <- function(Video, MinConfidence = NULL, ClientRequestToken = NULL, NotificationChannel = NULL, JobTag = NULL) {
  op <- new_operation(
    name = "StartContentModeration",
    http_method = "POST",
    http_path = "/",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .rekognition$start_content_moderation_input(Video = Video, MinConfidence = MinConfidence, ClientRequestToken = ClientRequestToken, NotificationChannel = NotificationChannel, JobTag = JobTag)
  output <- .rekognition$start_content_moderation_output()
  config <- get_config()
  svc <- .rekognition$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.rekognition$operations$start_content_moderation <- rekognition_start_content_moderation

#' Starts asynchronous detection of faces in a stored video
#'
#' @description
#' Starts asynchronous detection of faces in a stored video.
#' 
#' Amazon Rekognition Video can detect faces in a video stored in an Amazon
#' S3 bucket. Use Video to specify the bucket name and the filename of the
#' video. [`start_face_detection`][rekognition_start_face_detection]
#' returns a job identifier (`JobId`) that you use to get the results of
#' the operation. When face detection is finished, Amazon Rekognition Video
#' publishes a completion status to the Amazon Simple Notification Service
#' topic that you specify in `NotificationChannel`. To get the results of
#' the face detection operation, first check that the status value
#' published to the Amazon SNS topic is `SUCCEEDED`. If so, call
#' [`get_face_detection`][rekognition_get_face_detection] and pass the job
#' identifier (`JobId`) from the initial call to
#' [`start_face_detection`][rekognition_start_face_detection].
#' 
#' For more information, see Detecting faces in a stored video in the
#' Amazon Rekognition Developer Guide.
#'
#' @usage
#' rekognition_start_face_detection(Video, ClientRequestToken,
#'   NotificationChannel, FaceAttributes, JobTag)
#'
#' @param Video &#91;required&#93; The video in which you want to detect faces. The video must be stored in
#' an Amazon S3 bucket.
#' @param ClientRequestToken Idempotent token used to identify the start request. If you use the same
#' token with multiple
#' [`start_face_detection`][rekognition_start_face_detection] requests, the
#' same `JobId` is returned. Use `ClientRequestToken` to prevent the same
#' job from being accidently started more than once.
#' @param NotificationChannel The ARN of the Amazon SNS topic to which you want Amazon Rekognition
#' Video to publish the completion status of the face detection operation.
#' The Amazon SNS topic must have a topic name that begins with
#' *AmazonRekognition* if you are using the AmazonRekognitionServiceRole
#' permissions policy.
#' @param FaceAttributes The face attributes you want returned.
#' 
#' `DEFAULT` - The following subset of facial attributes are returned:
#' BoundingBox, Confidence, Pose, Quality and Landmarks.
#' 
#' `ALL` - All facial attributes are returned.
#' @param JobTag An identifier you specify that's returned in the completion notification
#' that's published to your Amazon Simple Notification Service topic. For
#' example, you can use `JobTag` to group related jobs and identify them in
#' the completion notification.
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   JobId = "string"
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$start_face_detection(
#'   Video = list(
#'     S3Object = list(
#'       Bucket = "string",
#'       Name = "string",
#'       Version = "string"
#'     )
#'   ),
#'   ClientRequestToken = "string",
#'   NotificationChannel = list(
#'     SNSTopicArn = "string",
#'     RoleArn = "string"
#'   ),
#'   FaceAttributes = "DEFAULT"|"ALL",
#'   JobTag = "string"
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname rekognition_start_face_detection
#'
#' @aliases rekognition_start_face_detection
rekognition_start_face_detection <- function(Video, ClientRequestToken = NULL, NotificationChannel = NULL, FaceAttributes = NULL, JobTag = NULL) {
  op <- new_operation(
    name = "StartFaceDetection",
    http_method = "POST",
    http_path = "/",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .rekognition$start_face_detection_input(Video = Video, ClientRequestToken = ClientRequestToken, NotificationChannel = NotificationChannel, FaceAttributes = FaceAttributes, JobTag = JobTag)
  output <- .rekognition$start_face_detection_output()
  config <- get_config()
  svc <- .rekognition$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.rekognition$operations$start_face_detection <- rekognition_start_face_detection

#' Starts the asynchronous search for faces in a collection that match the
#' faces of persons detected in a stored video
#'
#' @description
#' Starts the asynchronous search for faces in a collection that match the
#' faces of persons detected in a stored video.
#' 
#' The video must be stored in an Amazon S3 bucket. Use Video to specify
#' the bucket name and the filename of the video.
#' [`start_face_search`][rekognition_start_face_search] returns a job
#' identifier (`JobId`) which you use to get the search results once the
#' search has completed. When searching is finished, Amazon Rekognition
#' Video publishes a completion status to the Amazon Simple Notification
#' Service topic that you specify in `NotificationChannel`. To get the
#' search results, first check that the status value published to the
#' Amazon SNS topic is `SUCCEEDED`. If so, call
#' [`get_face_search`][rekognition_get_face_search] and pass the job
#' identifier (`JobId`) from the initial call to
#' [`start_face_search`][rekognition_start_face_search]. For more
#' information, see [Searching stored videos for
#' faces](https://docs.aws.amazon.com/rekognition/latest/dg/procedure-person-search-videos.html).
#'
#' @usage
#' rekognition_start_face_search(Video, ClientRequestToken,
#'   FaceMatchThreshold, CollectionId, NotificationChannel, JobTag)
#'
#' @param Video &#91;required&#93; The video you want to search. The video must be stored in an Amazon S3
#' bucket.
#' @param ClientRequestToken Idempotent token used to identify the start request. If you use the same
#' token with multiple [`start_face_search`][rekognition_start_face_search]
#' requests, the same `JobId` is returned. Use `ClientRequestToken` to
#' prevent the same job from being accidently started more than once.
#' @param FaceMatchThreshold The minimum confidence in the person match to return. For example, don't
#' return any matches where confidence in matches is less than 70%. The
#' default value is 80%.
#' @param CollectionId &#91;required&#93; ID of the collection that contains the faces you want to search for.
#' @param NotificationChannel The ARN of the Amazon SNS topic to which you want Amazon Rekognition
#' Video to publish the completion status of the search. The Amazon SNS
#' topic must have a topic name that begins with *AmazonRekognition* if you
#' are using the AmazonRekognitionServiceRole permissions policy to access
#' the topic.
#' @param JobTag An identifier you specify that's returned in the completion notification
#' that's published to your Amazon Simple Notification Service topic. For
#' example, you can use `JobTag` to group related jobs and identify them in
#' the completion notification.
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   JobId = "string"
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$start_face_search(
#'   Video = list(
#'     S3Object = list(
#'       Bucket = "string",
#'       Name = "string",
#'       Version = "string"
#'     )
#'   ),
#'   ClientRequestToken = "string",
#'   FaceMatchThreshold = 123.0,
#'   CollectionId = "string",
#'   NotificationChannel = list(
#'     SNSTopicArn = "string",
#'     RoleArn = "string"
#'   ),
#'   JobTag = "string"
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname rekognition_start_face_search
#'
#' @aliases rekognition_start_face_search
rekognition_start_face_search <- function(Video, ClientRequestToken = NULL, FaceMatchThreshold = NULL, CollectionId, NotificationChannel = NULL, JobTag = NULL) {
  op <- new_operation(
    name = "StartFaceSearch",
    http_method = "POST",
    http_path = "/",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .rekognition$start_face_search_input(Video = Video, ClientRequestToken = ClientRequestToken, FaceMatchThreshold = FaceMatchThreshold, CollectionId = CollectionId, NotificationChannel = NotificationChannel, JobTag = JobTag)
  output <- .rekognition$start_face_search_output()
  config <- get_config()
  svc <- .rekognition$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.rekognition$operations$start_face_search <- rekognition_start_face_search

#' Starts asynchronous detection of labels in a stored video
#'
#' @description
#' Starts asynchronous detection of labels in a stored video.
#' 
#' Amazon Rekognition Video can detect labels in a video. Labels are
#' instances of real-world entities. This includes objects like flower,
#' tree, and table; events like wedding, graduation, and birthday party;
#' concepts like landscape, evening, and nature; and activities like a
#' person getting out of a car or a person skiing.
#' 
#' The video must be stored in an Amazon S3 bucket. Use Video to specify
#' the bucket name and the filename of the video.
#' [`start_label_detection`][rekognition_start_label_detection] returns a
#' job identifier (`JobId`) which you use to get the results of the
#' operation. When label detection is finished, Amazon Rekognition Video
#' publishes a completion status to the Amazon Simple Notification Service
#' topic that you specify in `NotificationChannel`.
#' 
#' To get the results of the label detection operation, first check that
#' the status value published to the Amazon SNS topic is `SUCCEEDED`. If
#' so, call [`get_label_detection`][rekognition_get_label_detection] and
#' pass the job identifier (`JobId`) from the initial call to
#' [`start_label_detection`][rekognition_start_label_detection].
#' 
#' *Optional Parameters*
#' 
#' [`start_label_detection`][rekognition_start_label_detection] has the
#' `GENERAL_LABELS` Feature applied by default. This feature allows you to
#' provide filtering criteria to the `Settings` parameter. You can filter
#' with sets of individual labels or with label categories. You can specify
#' inclusive filters, exclusive filters, or a combination of inclusive and
#' exclusive filters. For more information on filtering, see [Detecting
#' labels in a
#' video](https://docs.aws.amazon.com/rekognition/latest/dg/labels-detecting-labels-video.html).
#' 
#' You can specify `MinConfidence` to control the confidence threshold for
#' the labels returned. The default is 50.
#'
#' @usage
#' rekognition_start_label_detection(Video, ClientRequestToken,
#'   MinConfidence, NotificationChannel, JobTag, Features, Settings)
#'
#' @param Video &#91;required&#93; The video in which you want to detect labels. The video must be stored
#' in an Amazon S3 bucket.
#' @param ClientRequestToken Idempotent token used to identify the start request. If you use the same
#' token with multiple
#' [`start_label_detection`][rekognition_start_label_detection] requests,
#' the same `JobId` is returned. Use `ClientRequestToken` to prevent the
#' same job from being accidently started more than once.
#' @param MinConfidence Specifies the minimum confidence that Amazon Rekognition Video must have
#' in order to return a detected label. Confidence represents how certain
#' Amazon Rekognition is that a label is correctly identified.0 is the
#' lowest confidence. 100 is the highest confidence. Amazon Rekognition
#' Video doesn't return any labels with a confidence level lower than this
#' specified value.
#' 
#' If you don't specify `MinConfidence`, the operation returns labels and
#' bounding boxes (if detected) with confidence values greater than or
#' equal to 50 percent.
#' @param NotificationChannel The Amazon SNS topic ARN you want Amazon Rekognition Video to publish
#' the completion status of the label detection operation to. The Amazon
#' SNS topic must have a topic name that begins with *AmazonRekognition* if
#' you are using the AmazonRekognitionServiceRole permissions policy.
#' @param JobTag An identifier you specify that's returned in the completion notification
#' that's published to your Amazon Simple Notification Service topic. For
#' example, you can use `JobTag` to group related jobs and identify them in
#' the completion notification.
#' @param Features The features to return after video analysis. You can specify that
#' GENERAL_LABELS are returned.
#' @param Settings The settings for a StartLabelDetection request.Contains the specified
#' parameters for the label detection request of an asynchronous label
#' analysis operation. Settings can include filters for GENERAL_LABELS.
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   JobId = "string"
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$start_label_detection(
#'   Video = list(
#'     S3Object = list(
#'       Bucket = "string",
#'       Name = "string",
#'       Version = "string"
#'     )
#'   ),
#'   ClientRequestToken = "string",
#'   MinConfidence = 123.0,
#'   NotificationChannel = list(
#'     SNSTopicArn = "string",
#'     RoleArn = "string"
#'   ),
#'   JobTag = "string",
#'   Features = list(
#'     "GENERAL_LABELS"
#'   ),
#'   Settings = list(
#'     GeneralLabels = list(
#'       LabelInclusionFilters = list(
#'         "string"
#'       ),
#'       LabelExclusionFilters = list(
#'         "string"
#'       ),
#'       LabelCategoryInclusionFilters = list(
#'         "string"
#'       ),
#'       LabelCategoryExclusionFilters = list(
#'         "string"
#'       )
#'     )
#'   )
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname rekognition_start_label_detection
#'
#' @aliases rekognition_start_label_detection
rekognition_start_label_detection <- function(Video, ClientRequestToken = NULL, MinConfidence = NULL, NotificationChannel = NULL, JobTag = NULL, Features = NULL, Settings = NULL) {
  op <- new_operation(
    name = "StartLabelDetection",
    http_method = "POST",
    http_path = "/",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .rekognition$start_label_detection_input(Video = Video, ClientRequestToken = ClientRequestToken, MinConfidence = MinConfidence, NotificationChannel = NotificationChannel, JobTag = JobTag, Features = Features, Settings = Settings)
  output <- .rekognition$start_label_detection_output()
  config <- get_config()
  svc <- .rekognition$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.rekognition$operations$start_label_detection <- rekognition_start_label_detection

#' Initiates a new media analysis job
#'
#' @description
#' Initiates a new media analysis job. Accepts a manifest file in an Amazon
#' S3 bucket. The output is a manifest file and a summary of the manifest
#' stored in the Amazon S3 bucket.
#'
#' @usage
#' rekognition_start_media_analysis_job(ClientRequestToken, JobName,
#'   OperationsConfig, Input, OutputConfig, KmsKeyId)
#'
#' @param ClientRequestToken Idempotency token used to prevent the accidental creation of duplicate
#' versions. If you use the same token with multiple
#' `StartMediaAnalysisJobRequest` requests, the same response is returned.
#' Use `ClientRequestToken` to prevent the same request from being
#' processed more than once.
#' @param JobName The name of the job. Does not have to be unique.
#' @param OperationsConfig &#91;required&#93; Configuration options for the media analysis job to be created.
#' @param Input &#91;required&#93; Input data to be analyzed by the job.
#' @param OutputConfig &#91;required&#93; The Amazon S3 bucket location to store the results.
#' @param KmsKeyId The identifier of customer managed AWS KMS key (name or ARN). The key is
#' used to encrypt images copied into the service. The key is also used to
#' encrypt results and manifest files written to the output Amazon S3
#' bucket.
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   JobId = "string"
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$start_media_analysis_job(
#'   ClientRequestToken = "string",
#'   JobName = "string",
#'   OperationsConfig = list(
#'     DetectModerationLabels = list(
#'       MinConfidence = 123.0,
#'       ProjectVersion = "string"
#'     )
#'   ),
#'   Input = list(
#'     S3Object = list(
#'       Bucket = "string",
#'       Name = "string",
#'       Version = "string"
#'     )
#'   ),
#'   OutputConfig = list(
#'     S3Bucket = "string",
#'     S3KeyPrefix = "string"
#'   ),
#'   KmsKeyId = "string"
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname rekognition_start_media_analysis_job
#'
#' @aliases rekognition_start_media_analysis_job
rekognition_start_media_analysis_job <- function(ClientRequestToken = NULL, JobName = NULL, OperationsConfig, Input, OutputConfig, KmsKeyId = NULL) {
  op <- new_operation(
    name = "StartMediaAnalysisJob",
    http_method = "POST",
    http_path = "/",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .rekognition$start_media_analysis_job_input(ClientRequestToken = ClientRequestToken, JobName = JobName, OperationsConfig = OperationsConfig, Input = Input, OutputConfig = OutputConfig, KmsKeyId = KmsKeyId)
  output <- .rekognition$start_media_analysis_job_output()
  config <- get_config()
  svc <- .rekognition$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.rekognition$operations$start_media_analysis_job <- rekognition_start_media_analysis_job

#' End of support notice: On October 31, 2025, AWS will discontinue support
#' for Amazon Rekognition People Pathing
#'
#' @description
#' *End of support notice:* On October 31, 2025, AWS will discontinue
#' support for Amazon Rekognition People Pathing. After October 31, 2025,
#' you will no longer be able to use the Rekognition People Pathing
#' capability. For more information, visit this [blog
#' post](https://aws.amazon.com/blogs/machine-learning/transitioning-from-amazon-rekognition-people-pathing-exploring-other-alternatives/).
#' 
#' Starts the asynchronous tracking of a person's path in a stored video.
#' 
#' Amazon Rekognition Video can track the path of people in a video stored
#' in an Amazon S3 bucket. Use Video to specify the bucket name and the
#' filename of the video.
#' [`start_person_tracking`][rekognition_start_person_tracking] returns a
#' job identifier (`JobId`) which you use to get the results of the
#' operation. When label detection is finished, Amazon Rekognition
#' publishes a completion status to the Amazon Simple Notification Service
#' topic that you specify in `NotificationChannel`.
#' 
#' To get the results of the person detection operation, first check that
#' the status value published to the Amazon SNS topic is `SUCCEEDED`. If
#' so, call [`get_person_tracking`][rekognition_get_person_tracking] and
#' pass the job identifier (`JobId`) from the initial call to
#' [`start_person_tracking`][rekognition_start_person_tracking].
#'
#' @usage
#' rekognition_start_person_tracking(Video, ClientRequestToken,
#'   NotificationChannel, JobTag)
#'
#' @param Video &#91;required&#93; The video in which you want to detect people. The video must be stored
#' in an Amazon S3 bucket.
#' @param ClientRequestToken Idempotent token used to identify the start request. If you use the same
#' token with multiple
#' [`start_person_tracking`][rekognition_start_person_tracking] requests,
#' the same `JobId` is returned. Use `ClientRequestToken` to prevent the
#' same job from being accidently started more than once.
#' @param NotificationChannel The Amazon SNS topic ARN you want Amazon Rekognition Video to publish
#' the completion status of the people detection operation to. The Amazon
#' SNS topic must have a topic name that begins with *AmazonRekognition* if
#' you are using the AmazonRekognitionServiceRole permissions policy.
#' @param JobTag An identifier you specify that's returned in the completion notification
#' that's published to your Amazon Simple Notification Service topic. For
#' example, you can use `JobTag` to group related jobs and identify them in
#' the completion notification.
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   JobId = "string"
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$start_person_tracking(
#'   Video = list(
#'     S3Object = list(
#'       Bucket = "string",
#'       Name = "string",
#'       Version = "string"
#'     )
#'   ),
#'   ClientRequestToken = "string",
#'   NotificationChannel = list(
#'     SNSTopicArn = "string",
#'     RoleArn = "string"
#'   ),
#'   JobTag = "string"
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname rekognition_start_person_tracking
#'
#' @aliases rekognition_start_person_tracking
rekognition_start_person_tracking <- function(Video, ClientRequestToken = NULL, NotificationChannel = NULL, JobTag = NULL) {
  op <- new_operation(
    name = "StartPersonTracking",
    http_method = "POST",
    http_path = "/",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .rekognition$start_person_tracking_input(Video = Video, ClientRequestToken = ClientRequestToken, NotificationChannel = NotificationChannel, JobTag = JobTag)
  output <- .rekognition$start_person_tracking_output()
  config <- get_config()
  svc <- .rekognition$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.rekognition$operations$start_person_tracking <- rekognition_start_person_tracking

#' This operation applies only to Amazon Rekognition Custom Labels
#'
#' @description
#' This operation applies only to Amazon Rekognition Custom Labels.
#' 
#' Starts the running of the version of a model. Starting a model takes a
#' while to complete. To check the current state of the model, use
#' [`describe_project_versions`][rekognition_describe_project_versions].
#' 
#' Once the model is running, you can detect custom labels in new images by
#' calling [`detect_custom_labels`][rekognition_detect_custom_labels].
#' 
#' You are charged for the amount of time that the model is running. To
#' stop a running model, call
#' [`stop_project_version`][rekognition_stop_project_version].
#' 
#' This operation requires permissions to perform the
#' `rekognition:StartProjectVersion` action.
#'
#' @usage
#' rekognition_start_project_version(ProjectVersionArn, MinInferenceUnits,
#'   MaxInferenceUnits)
#'
#' @param ProjectVersionArn &#91;required&#93; The Amazon Resource Name(ARN) of the model version that you want to
#' start.
#' @param MinInferenceUnits &#91;required&#93; The minimum number of inference units to use. A single inference unit
#' represents 1 hour of processing.
#' 
#' Use a higher number to increase the TPS throughput of your model. You
#' are charged for the number of inference units that you use.
#' @param MaxInferenceUnits The maximum number of inference units to use for auto-scaling the model.
#' If you don't specify a value, Amazon Rekognition Custom Labels doesn't
#' auto-scale the model.
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   Status = "TRAINING_IN_PROGRESS"|"TRAINING_COMPLETED"|"TRAINING_FAILED"|"STARTING"|"RUNNING"|"FAILED"|"STOPPING"|"STOPPED"|"DELETING"|"COPYING_IN_PROGRESS"|"COPYING_COMPLETED"|"COPYING_FAILED"|"DEPRECATED"|"EXPIRED"
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$start_project_version(
#'   ProjectVersionArn = "string",
#'   MinInferenceUnits = 123,
#'   MaxInferenceUnits = 123
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname rekognition_start_project_version
#'
#' @aliases rekognition_start_project_version
rekognition_start_project_version <- function(ProjectVersionArn, MinInferenceUnits, MaxInferenceUnits = NULL) {
  op <- new_operation(
    name = "StartProjectVersion",
    http_method = "POST",
    http_path = "/",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .rekognition$start_project_version_input(ProjectVersionArn = ProjectVersionArn, MinInferenceUnits = MinInferenceUnits, MaxInferenceUnits = MaxInferenceUnits)
  output <- .rekognition$start_project_version_output()
  config <- get_config()
  svc <- .rekognition$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.rekognition$operations$start_project_version <- rekognition_start_project_version

#' Starts asynchronous detection of segment detection in a stored video
#'
#' @description
#' Starts asynchronous detection of segment detection in a stored video.
#' 
#' Amazon Rekognition Video can detect segments in a video stored in an
#' Amazon S3 bucket. Use Video to specify the bucket name and the filename
#' of the video.
#' [`start_segment_detection`][rekognition_start_segment_detection] returns
#' a job identifier (`JobId`) which you use to get the results of the
#' operation. When segment detection is finished, Amazon Rekognition Video
#' publishes a completion status to the Amazon Simple Notification Service
#' topic that you specify in `NotificationChannel`.
#' 
#' You can use the `Filters` (StartSegmentDetectionFilters) input parameter
#' to specify the minimum detection confidence returned in the response.
#' Within `Filters`, use `ShotFilter` (StartShotDetectionFilter) to filter
#' detected shots. Use `TechnicalCueFilter`
#' (StartTechnicalCueDetectionFilter) to filter technical cues.
#' 
#' To get the results of the segment detection operation, first check that
#' the status value published to the Amazon SNS topic is `SUCCEEDED`. if
#' so, call [`get_segment_detection`][rekognition_get_segment_detection]
#' and pass the job identifier (`JobId`) from the initial call to
#' [`start_segment_detection`][rekognition_start_segment_detection].
#' 
#' For more information, see Detecting video segments in stored video in
#' the Amazon Rekognition Developer Guide.
#'
#' @usage
#' rekognition_start_segment_detection(Video, ClientRequestToken,
#'   NotificationChannel, JobTag, Filters, SegmentTypes)
#'
#' @param Video &#91;required&#93; 
#' @param ClientRequestToken Idempotent token used to identify the start request. If you use the same
#' token with multiple
#' [`start_segment_detection`][rekognition_start_segment_detection]
#' requests, the same `JobId` is returned. Use `ClientRequestToken` to
#' prevent the same job from being accidently started more than once.
#' @param NotificationChannel The ARN of the Amazon SNS topic to which you want Amazon Rekognition
#' Video to publish the completion status of the segment detection
#' operation. Note that the Amazon SNS topic must have a topic name that
#' begins with *AmazonRekognition* if you are using the
#' AmazonRekognitionServiceRole permissions policy to access the topic.
#' @param JobTag An identifier you specify that's returned in the completion notification
#' that's published to your Amazon Simple Notification Service topic. For
#' example, you can use `JobTag` to group related jobs and identify them in
#' the completion notification.
#' @param Filters Filters for technical cue or shot detection.
#' @param SegmentTypes &#91;required&#93; An array of segment types to detect in the video. Valid values are
#' TECHNICAL_CUE and SHOT.
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   JobId = "string"
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$start_segment_detection(
#'   Video = list(
#'     S3Object = list(
#'       Bucket = "string",
#'       Name = "string",
#'       Version = "string"
#'     )
#'   ),
#'   ClientRequestToken = "string",
#'   NotificationChannel = list(
#'     SNSTopicArn = "string",
#'     RoleArn = "string"
#'   ),
#'   JobTag = "string",
#'   Filters = list(
#'     TechnicalCueFilter = list(
#'       MinSegmentConfidence = 123.0,
#'       BlackFrame = list(
#'         MaxPixelThreshold = 123.0,
#'         MinCoveragePercentage = 123.0
#'       )
#'     ),
#'     ShotFilter = list(
#'       MinSegmentConfidence = 123.0
#'     )
#'   ),
#'   SegmentTypes = list(
#'     "TECHNICAL_CUE"|"SHOT"
#'   )
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname rekognition_start_segment_detection
#'
#' @aliases rekognition_start_segment_detection
rekognition_start_segment_detection <- function(Video, ClientRequestToken = NULL, NotificationChannel = NULL, JobTag = NULL, Filters = NULL, SegmentTypes) {
  op <- new_operation(
    name = "StartSegmentDetection",
    http_method = "POST",
    http_path = "/",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .rekognition$start_segment_detection_input(Video = Video, ClientRequestToken = ClientRequestToken, NotificationChannel = NotificationChannel, JobTag = JobTag, Filters = Filters, SegmentTypes = SegmentTypes)
  output <- .rekognition$start_segment_detection_output()
  config <- get_config()
  svc <- .rekognition$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.rekognition$operations$start_segment_detection <- rekognition_start_segment_detection

#' Starts processing a stream processor
#'
#' @description
#' Starts processing a stream processor. You create a stream processor by
#' calling
#' [`create_stream_processor`][rekognition_create_stream_processor]. To
#' tell [`start_stream_processor`][rekognition_start_stream_processor]
#' which stream processor to start, use the value of the `Name` field
#' specified in the call to
#' [`create_stream_processor`][rekognition_create_stream_processor].
#' 
#' If you are using a label detection stream processor to detect labels,
#' you need to provide a `Start selector` and a `Stop selector` to
#' determine the length of the stream processing time.
#'
#' @usage
#' rekognition_start_stream_processor(Name, StartSelector, StopSelector)
#'
#' @param Name &#91;required&#93; The name of the stream processor to start processing.
#' @param StartSelector Specifies the starting point in the Kinesis stream to start processing.
#' You can use the producer timestamp or the fragment number. If you use
#' the producer timestamp, you must put the time in milliseconds. For more
#' information about fragment numbers, see
#' [Fragment](https://docs.aws.amazon.com/kinesisvideostreams/latest/dg/API_reader_Fragment.html).
#' 
#' This is a required parameter for label detection stream processors and
#' should not be used to start a face search stream processor.
#' @param StopSelector Specifies when to stop processing the stream. You can specify a maximum
#' amount of time to process the video.
#' 
#' This is a required parameter for label detection stream processors and
#' should not be used to start a face search stream processor.
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   SessionId = "string"
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$start_stream_processor(
#'   Name = "string",
#'   StartSelector = list(
#'     KVSStreamStartSelector = list(
#'       ProducerTimestamp = 123,
#'       FragmentNumber = "string"
#'     )
#'   ),
#'   StopSelector = list(
#'     MaxDurationInSeconds = 123
#'   )
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname rekognition_start_stream_processor
#'
#' @aliases rekognition_start_stream_processor
rekognition_start_stream_processor <- function(Name, StartSelector = NULL, StopSelector = NULL) {
  op <- new_operation(
    name = "StartStreamProcessor",
    http_method = "POST",
    http_path = "/",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .rekognition$start_stream_processor_input(Name = Name, StartSelector = StartSelector, StopSelector = StopSelector)
  output <- .rekognition$start_stream_processor_output()
  config <- get_config()
  svc <- .rekognition$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.rekognition$operations$start_stream_processor <- rekognition_start_stream_processor

#' Starts asynchronous detection of text in a stored video
#'
#' @description
#' Starts asynchronous detection of text in a stored video.
#' 
#' Amazon Rekognition Video can detect text in a video stored in an Amazon
#' S3 bucket. Use Video to specify the bucket name and the filename of the
#' video. [`start_text_detection`][rekognition_start_text_detection]
#' returns a job identifier (`JobId`) which you use to get the results of
#' the operation. When text detection is finished, Amazon Rekognition Video
#' publishes a completion status to the Amazon Simple Notification Service
#' topic that you specify in `NotificationChannel`.
#' 
#' To get the results of the text detection operation, first check that the
#' status value published to the Amazon SNS topic is `SUCCEEDED`. if so,
#' call [`get_text_detection`][rekognition_get_text_detection] and pass the
#' job identifier (`JobId`) from the initial call to
#' [`start_text_detection`][rekognition_start_text_detection].
#'
#' @usage
#' rekognition_start_text_detection(Video, ClientRequestToken,
#'   NotificationChannel, JobTag, Filters)
#'
#' @param Video &#91;required&#93; 
#' @param ClientRequestToken Idempotent token used to identify the start request. If you use the same
#' token with multiple
#' [`start_text_detection`][rekognition_start_text_detection] requests, the
#' same `JobId` is returned. Use `ClientRequestToken` to prevent the same
#' job from being accidentaly started more than once.
#' @param NotificationChannel 
#' @param JobTag An identifier returned in the completion status published by your Amazon
#' Simple Notification Service topic. For example, you can use `JobTag` to
#' group related jobs and identify them in the completion notification.
#' @param Filters Optional parameters that let you set criteria the text must meet to be
#' included in your response.
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   JobId = "string"
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$start_text_detection(
#'   Video = list(
#'     S3Object = list(
#'       Bucket = "string",
#'       Name = "string",
#'       Version = "string"
#'     )
#'   ),
#'   ClientRequestToken = "string",
#'   NotificationChannel = list(
#'     SNSTopicArn = "string",
#'     RoleArn = "string"
#'   ),
#'   JobTag = "string",
#'   Filters = list(
#'     WordFilter = list(
#'       MinConfidence = 123.0,
#'       MinBoundingBoxHeight = 123.0,
#'       MinBoundingBoxWidth = 123.0
#'     ),
#'     RegionsOfInterest = list(
#'       list(
#'         BoundingBox = list(
#'           Width = 123.0,
#'           Height = 123.0,
#'           Left = 123.0,
#'           Top = 123.0
#'         ),
#'         Polygon = list(
#'           list(
#'             X = 123.0,
#'             Y = 123.0
#'           )
#'         )
#'       )
#'     )
#'   )
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname rekognition_start_text_detection
#'
#' @aliases rekognition_start_text_detection
rekognition_start_text_detection <- function(Video, ClientRequestToken = NULL, NotificationChannel = NULL, JobTag = NULL, Filters = NULL) {
  op <- new_operation(
    name = "StartTextDetection",
    http_method = "POST",
    http_path = "/",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .rekognition$start_text_detection_input(Video = Video, ClientRequestToken = ClientRequestToken, NotificationChannel = NotificationChannel, JobTag = JobTag, Filters = Filters)
  output <- .rekognition$start_text_detection_output()
  config <- get_config()
  svc <- .rekognition$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.rekognition$operations$start_text_detection <- rekognition_start_text_detection

#' This operation applies only to Amazon Rekognition Custom Labels
#'
#' @description
#' This operation applies only to Amazon Rekognition Custom Labels.
#' 
#' Stops a running model. The operation might take a while to complete. To
#' check the current status, call
#' [`describe_project_versions`][rekognition_describe_project_versions].
#' Only applies to Custom Labels projects.
#' 
#' This operation requires permissions to perform the
#' `rekognition:StopProjectVersion` action.
#'
#' @usage
#' rekognition_stop_project_version(ProjectVersionArn)
#'
#' @param ProjectVersionArn &#91;required&#93; The Amazon Resource Name (ARN) of the model version that you want to
#' stop.
#' 
#' This operation requires permissions to perform the
#' `rekognition:StopProjectVersion` action.
#'
#' @return
#' A list with the following syntax:
#' ```
#' list(
#'   Status = "TRAINING_IN_PROGRESS"|"TRAINING_COMPLETED"|"TRAINING_FAILED"|"STARTING"|"RUNNING"|"FAILED"|"STOPPING"|"STOPPED"|"DELETING"|"COPYING_IN_PROGRESS"|"COPYING_COMPLETED"|"COPYING_FAILED"|"DEPRECATED"|"EXPIRED"
#' )
#' ```
#'
#' @section Request syntax:
#' ```
#' svc$stop_project_version(
#'   ProjectVersionArn = "string"
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname rekognition_stop_project_version
#'
#' @aliases rekognition_stop_project_version
rekognition_stop_project_version <- function(ProjectVersionArn) {
  op <- new_operation(
    name = "StopProjectVersion",
    http_method = "POST",
    http_path = "/",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .rekognition$stop_project_version_input(ProjectVersionArn = ProjectVersionArn)
  output <- .rekognition$stop_project_version_output()
  config <- get_config()
  svc <- .rekognition$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.rekognition$operations$stop_project_version <- rekognition_stop_project_version

#' Stops a running stream processor that was created by
#' CreateStreamProcessor
#'
#' @description
#' Stops a running stream processor that was created by
#' [`create_stream_processor`][rekognition_create_stream_processor].
#'
#' @usage
#' rekognition_stop_stream_processor(Name)
#'
#' @param Name &#91;required&#93; The name of a stream processor created by
#' [`create_stream_processor`][rekognition_create_stream_processor].
#'
#' @return
#' An empty list.
#'
#' @section Request syntax:
#' ```
#' svc$stop_stream_processor(
#'   Name = "string"
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname rekognition_stop_stream_processor
#'
#' @aliases rekognition_stop_stream_processor
rekognition_stop_stream_processor <- function(Name) {
  op <- new_operation(
    name = "StopStreamProcessor",
    http_method = "POST",
    http_path = "/",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .rekognition$stop_stream_processor_input(Name = Name)
  output <- .rekognition$stop_stream_processor_output()
  config <- get_config()
  svc <- .rekognition$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.rekognition$operations$stop_stream_processor <- rekognition_stop_stream_processor

#' Adds one or more key-value tags to an Amazon Rekognition collection,
#' stream processor, or Custom Labels model
#'
#' @description
#' Adds one or more key-value tags to an Amazon Rekognition collection,
#' stream processor, or Custom Labels model. For more information, see
#' [Tagging AWS
#' Resources](https://docs.aws.amazon.com/tag-editor/latest/userguide/tagging.html).
#' 
#' This operation requires permissions to perform the
#' `rekognition:TagResource` action.
#'
#' @usage
#' rekognition_tag_resource(ResourceArn, Tags)
#'
#' @param ResourceArn &#91;required&#93; Amazon Resource Name (ARN) of the model, collection, or stream processor
#' that you want to assign the tags to.
#' @param Tags &#91;required&#93; The key-value tags to assign to the resource.
#'
#' @return
#' An empty list.
#'
#' @section Request syntax:
#' ```
#' svc$tag_resource(
#'   ResourceArn = "string",
#'   Tags = list(
#'     "string"
#'   )
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname rekognition_tag_resource
#'
#' @aliases rekognition_tag_resource
rekognition_tag_resource <- function(ResourceArn, Tags) {
  op <- new_operation(
    name = "TagResource",
    http_method = "POST",
    http_path = "/",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .rekognition$tag_resource_input(ResourceArn = ResourceArn, Tags = Tags)
  output <- .rekognition$tag_resource_output()
  config <- get_config()
  svc <- .rekognition$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.rekognition$operations$tag_resource <- rekognition_tag_resource

#' Removes one or more tags from an Amazon Rekognition collection, stream
#' processor, or Custom Labels model
#'
#' @description
#' Removes one or more tags from an Amazon Rekognition collection, stream
#' processor, or Custom Labels model.
#' 
#' This operation requires permissions to perform the
#' `rekognition:UntagResource` action.
#'
#' @usage
#' rekognition_untag_resource(ResourceArn, TagKeys)
#'
#' @param ResourceArn &#91;required&#93; Amazon Resource Name (ARN) of the model, collection, or stream processor
#' that you want to remove the tags from.
#' @param TagKeys &#91;required&#93; A list of the tags that you want to remove.
#'
#' @return
#' An empty list.
#'
#' @section Request syntax:
#' ```
#' svc$untag_resource(
#'   ResourceArn = "string",
#'   TagKeys = list(
#'     "string"
#'   )
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname rekognition_untag_resource
#'
#' @aliases rekognition_untag_resource
rekognition_untag_resource <- function(ResourceArn, TagKeys) {
  op <- new_operation(
    name = "UntagResource",
    http_method = "POST",
    http_path = "/",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .rekognition$untag_resource_input(ResourceArn = ResourceArn, TagKeys = TagKeys)
  output <- .rekognition$untag_resource_output()
  config <- get_config()
  svc <- .rekognition$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.rekognition$operations$untag_resource <- rekognition_untag_resource

#' This operation applies only to Amazon Rekognition Custom Labels
#'
#' @description
#' This operation applies only to Amazon Rekognition Custom Labels.
#' 
#' Adds or updates one or more entries (images) in a dataset. An entry is a
#' JSON Line which contains the information for a single image, including
#' the image location, assigned labels, and object location bounding boxes.
#' For more information, see Image-Level labels in manifest files and
#' Object localization in manifest files in the *Amazon Rekognition Custom
#' Labels Developer Guide*.
#' 
#' If the `source-ref` field in the JSON line references an existing image,
#' the existing image in the dataset is updated. If `source-ref` field
#' doesn't reference an existing image, the image is added as a new image
#' to the dataset.
#' 
#' You specify the changes that you want to make in the `Changes` input
#' parameter. There isn't a limit to the number JSON Lines that you can
#' change, but the size of `Changes` must be less than 5MB.
#' 
#' [`update_dataset_entries`][rekognition_update_dataset_entries] returns
#' immediatly, but the dataset update might take a while to complete. Use
#' [`describe_dataset`][rekognition_describe_dataset] to check the current
#' status. The dataset updated successfully if the value of `Status` is
#' `UPDATE_COMPLETE`.
#' 
#' To check if any non-terminal errors occured, call
#' [`list_dataset_entries`][rekognition_list_dataset_entries] and check for
#' the presence of `errors` lists in the JSON Lines.
#' 
#' Dataset update fails if a terminal error occurs (`Status` =
#' `UPDATE_FAILED`). Currently, you can't access the terminal error
#' information from the Amazon Rekognition Custom Labels SDK.
#' 
#' This operation requires permissions to perform the
#' `rekognition:UpdateDatasetEntries` action.
#'
#' @usage
#' rekognition_update_dataset_entries(DatasetArn, Changes)
#'
#' @param DatasetArn &#91;required&#93; The Amazon Resource Name (ARN) of the dataset that you want to update.
#' @param Changes &#91;required&#93; The changes that you want to make to the dataset.
#'
#' @return
#' An empty list.
#'
#' @section Request syntax:
#' ```
#' svc$update_dataset_entries(
#'   DatasetArn = "string",
#'   Changes = list(
#'     GroundTruth = raw
#'   )
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname rekognition_update_dataset_entries
#'
#' @aliases rekognition_update_dataset_entries
rekognition_update_dataset_entries <- function(DatasetArn, Changes) {
  op <- new_operation(
    name = "UpdateDatasetEntries",
    http_method = "POST",
    http_path = "/",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .rekognition$update_dataset_entries_input(DatasetArn = DatasetArn, Changes = Changes)
  output <- .rekognition$update_dataset_entries_output()
  config <- get_config()
  svc <- .rekognition$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.rekognition$operations$update_dataset_entries <- rekognition_update_dataset_entries

#' Allows you to update a stream processor
#'
#' @description
#' Allows you to update a stream processor. You can change some settings
#' and regions of interest and delete certain parameters.
#'
#' @usage
#' rekognition_update_stream_processor(Name, SettingsForUpdate,
#'   RegionsOfInterestForUpdate, DataSharingPreferenceForUpdate,
#'   ParametersToDelete)
#'
#' @param Name &#91;required&#93; Name of the stream processor that you want to update.
#' @param SettingsForUpdate The stream processor settings that you want to update. Label detection
#' settings can be updated to detect different labels with a different
#' minimum confidence.
#' @param RegionsOfInterestForUpdate Specifies locations in the frames where Amazon Rekognition checks for
#' objects or people. This is an optional parameter for label detection
#' stream processors.
#' @param DataSharingPreferenceForUpdate Shows whether you are sharing data with Rekognition to improve model
#' performance. You can choose this option at the account level or on a
#' per-stream basis. Note that if you opt out at the account level this
#' setting is ignored on individual streams.
#' @param ParametersToDelete A list of parameters you want to delete from the stream processor.
#'
#' @return
#' An empty list.
#'
#' @section Request syntax:
#' ```
#' svc$update_stream_processor(
#'   Name = "string",
#'   SettingsForUpdate = list(
#'     ConnectedHomeForUpdate = list(
#'       Labels = list(
#'         "string"
#'       ),
#'       MinConfidence = 123.0
#'     )
#'   ),
#'   RegionsOfInterestForUpdate = list(
#'     list(
#'       BoundingBox = list(
#'         Width = 123.0,
#'         Height = 123.0,
#'         Left = 123.0,
#'         Top = 123.0
#'       ),
#'       Polygon = list(
#'         list(
#'           X = 123.0,
#'           Y = 123.0
#'         )
#'       )
#'     )
#'   ),
#'   DataSharingPreferenceForUpdate = list(
#'     OptIn = TRUE|FALSE
#'   ),
#'   ParametersToDelete = list(
#'     "ConnectedHomeMinConfidence"|"RegionsOfInterest"
#'   )
#' )
#' ```
#'
#' @keywords internal
#'
#' @rdname rekognition_update_stream_processor
#'
#' @aliases rekognition_update_stream_processor
rekognition_update_stream_processor <- function(Name, SettingsForUpdate = NULL, RegionsOfInterestForUpdate = NULL, DataSharingPreferenceForUpdate = NULL, ParametersToDelete = NULL) {
  op <- new_operation(
    name = "UpdateStreamProcessor",
    http_method = "POST",
    http_path = "/",
    host_prefix = "",
    paginator = list(),
    stream_api = FALSE
  )
  input <- .rekognition$update_stream_processor_input(Name = Name, SettingsForUpdate = SettingsForUpdate, RegionsOfInterestForUpdate = RegionsOfInterestForUpdate, DataSharingPreferenceForUpdate = DataSharingPreferenceForUpdate, ParametersToDelete = ParametersToDelete)
  output <- .rekognition$update_stream_processor_output()
  config <- get_config()
  svc <- .rekognition$service(config, op)
  request <- new_request(svc, op, input, output)
  response <- send_request(request)
  return(response)
}
.rekognition$operations$update_stream_processor <- rekognition_update_stream_processor
