% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/databasemigrationservice_operations.R
\name{databasemigrationservice_modify_endpoint}
\alias{databasemigrationservice_modify_endpoint}
\title{Modifies the specified endpoint}
\usage{
databasemigrationservice_modify_endpoint(EndpointArn,
  EndpointIdentifier, EndpointType, EngineName, Username, Password,
  ServerName, Port, DatabaseName, ExtraConnectionAttributes,
  CertificateArn, SslMode, ServiceAccessRoleArn, ExternalTableDefinition,
  DynamoDbSettings, S3Settings, DmsTransferSettings, MongoDbSettings,
  KinesisSettings, KafkaSettings, ElasticsearchSettings, NeptuneSettings,
  RedshiftSettings)
}
\arguments{
\item{EndpointArn}{[required] The Amazon Resource Name (ARN) string that uniquely identifies the
endpoint.}

\item{EndpointIdentifier}{The database endpoint identifier. Identifiers must begin with a letter
and must contain only ASCII letters, digits, and hyphens. They can't end
with a hyphen or contain two consecutive hyphens.}

\item{EndpointType}{The type of endpoint. Valid values are \code{source} and \code{target}.}

\item{EngineName}{The type of engine for the endpoint. Valid values, depending on the
EndpointType, include \code{"mysql"}, \code{"oracle"}, \code{"postgres"}, \code{"mariadb"},
\code{"aurora"}, \code{"aurora-postgresql"}, \code{"redshift"}, \code{"s3"}, \code{"db2"},
\code{"azuredb"}, \code{"sybase"}, \code{"dynamodb"}, \code{"mongodb"}, \code{"kinesis"},
\code{"kafka"}, \code{"elasticsearch"}, \code{"documentdb"}, \code{"sqlserver"}, and
\code{"neptune"}.}

\item{Username}{The user name to be used to login to the endpoint database.}

\item{Password}{The password to be used to login to the endpoint database.}

\item{ServerName}{The name of the server where the endpoint database resides.}

\item{Port}{The port used by the endpoint database.}

\item{DatabaseName}{The name of the endpoint database.}

\item{ExtraConnectionAttributes}{Additional attributes associated with the connection. To reset this
parameter, pass the empty string ("") as an argument.}

\item{CertificateArn}{The Amazon Resource Name (ARN) of the certificate used for SSL
connection.}

\item{SslMode}{The SSL mode used to connect to the endpoint. The default value is
\code{none}.}

\item{ServiceAccessRoleArn}{The Amazon Resource Name (ARN) for the service access role you want to
use to modify the endpoint.}

\item{ExternalTableDefinition}{The external table definition.}

\item{DynamoDbSettings}{Settings in JSON format for the target Amazon DynamoDB endpoint. For
information about other available settings, see \href{https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.DynamoDB.html}{Using Object Mapping to Migrate Data to DynamoDB}
in the \emph{AWS Database Migration Service User Guide.}}

\item{S3Settings}{Settings in JSON format for the target Amazon S3 endpoint. For more
information about the available settings, see \href{https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.S3.html#CHAP_Target.S3.Configuring}{Extra Connection Attributes When Using Amazon S3 as a Target for AWS DMS}
in the \emph{AWS Database Migration Service User Guide.}}

\item{DmsTransferSettings}{The settings in JSON format for the DMS transfer type of source
endpoint.

Attributes include the following:
\itemize{
\item serviceAccessRoleArn - The AWS Identity and Access Management (IAM)
role that has permission to access the Amazon S3 bucket.
\item BucketName - The name of the S3 bucket to use.
\item compressionType - An optional parameter to use GZIP to compress the
target files. Either set this parameter to NONE (the default) or
don't use it to leave the files uncompressed.
}

Shorthand syntax for these settings is as follows:
\verb{ServiceAccessRoleArn=string ,BucketName=string,CompressionType=string}

JSON syntax for these settings is as follows:
\verb{\\\{ "ServiceAccessRoleArn": "string", "BucketName": "string", "CompressionType": "none"|"gzip" \\\} }}

\item{MongoDbSettings}{Settings in JSON format for the source MongoDB endpoint. For more
information about the available settings, see the configuration
properties section in \href{https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Source.MongoDB.html}{Using MongoDB as a Target for AWS Database Migration Service}
in the \emph{AWS Database Migration Service User Guide.}}

\item{KinesisSettings}{Settings in JSON format for the target endpoint for Amazon Kinesis Data
Streams. For more information about the available settings, see \href{https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.Kinesis.html}{Using Amazon Kinesis Data Streams as a Target for AWS Database Migration Service}
in the \emph{AWS Database Migration Service User Guide.}}

\item{KafkaSettings}{Settings in JSON format for the target Apache Kafka endpoint. For more
information about the available settings, see \href{https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.Kafka.html}{Using Apache Kafka as a Target for AWS Database Migration Service}
in the \emph{AWS Database Migration Service User Guide.}}

\item{ElasticsearchSettings}{Settings in JSON format for the target Elasticsearch endpoint. For more
information about the available settings, see \href{https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.Elasticsearch.html#CHAP_Target.Elasticsearch.Configuration}{Extra Connection Attributes When Using Elasticsearch as a Target for AWS DMS}
in the \emph{AWS Database Migration Service User Guide.}}

\item{NeptuneSettings}{Settings in JSON format for the target Amazon Neptune endpoint. For more
information about the available settings, see \href{https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.Neptune.html#CHAP_Target.Neptune.EndpointSettings}{Specifying Endpoint Settings for Amazon Neptune as a Target}
in the \emph{AWS Database Migration Service User Guide.}}

\item{RedshiftSettings}{}
}
\description{
Modifies the specified endpoint.
}
\section{Request syntax}{
\preformatted{svc$modify_endpoint(
  EndpointArn = "string",
  EndpointIdentifier = "string",
  EndpointType = "source"|"target",
  EngineName = "string",
  Username = "string",
  Password = "string",
  ServerName = "string",
  Port = 123,
  DatabaseName = "string",
  ExtraConnectionAttributes = "string",
  CertificateArn = "string",
  SslMode = "none"|"require"|"verify-ca"|"verify-full",
  ServiceAccessRoleArn = "string",
  ExternalTableDefinition = "string",
  DynamoDbSettings = list(
    ServiceAccessRoleArn = "string"
  ),
  S3Settings = list(
    ServiceAccessRoleArn = "string",
    ExternalTableDefinition = "string",
    CsvRowDelimiter = "string",
    CsvDelimiter = "string",
    BucketFolder = "string",
    BucketName = "string",
    CompressionType = "none"|"gzip",
    EncryptionMode = "sse-s3"|"sse-kms",
    ServerSideEncryptionKmsKeyId = "string",
    DataFormat = "csv"|"parquet",
    EncodingType = "plain"|"plain-dictionary"|"rle-dictionary",
    DictPageSizeLimit = 123,
    RowGroupLength = 123,
    DataPageSize = 123,
    ParquetVersion = "parquet-1-0"|"parquet-2-0",
    EnableStatistics = TRUE|FALSE,
    IncludeOpForFullLoad = TRUE|FALSE,
    CdcInsertsOnly = TRUE|FALSE,
    TimestampColumnName = "string",
    ParquetTimestampInMillisecond = TRUE|FALSE,
    CdcInsertsAndUpdates = TRUE|FALSE
  ),
  DmsTransferSettings = list(
    ServiceAccessRoleArn = "string",
    BucketName = "string"
  ),
  MongoDbSettings = list(
    Username = "string",
    Password = "string",
    ServerName = "string",
    Port = 123,
    DatabaseName = "string",
    AuthType = "no"|"password",
    AuthMechanism = "default"|"mongodb_cr"|"scram_sha_1",
    NestingLevel = "none"|"one",
    ExtractDocId = "string",
    DocsToInvestigate = "string",
    AuthSource = "string",
    KmsKeyId = "string"
  ),
  KinesisSettings = list(
    StreamArn = "string",
    MessageFormat = "json"|"json-unformatted",
    ServiceAccessRoleArn = "string",
    IncludeTransactionDetails = TRUE|FALSE,
    IncludePartitionValue = TRUE|FALSE,
    PartitionIncludeSchemaTable = TRUE|FALSE,
    IncludeTableAlterOperations = TRUE|FALSE,
    IncludeControlDetails = TRUE|FALSE
  ),
  KafkaSettings = list(
    Broker = "string",
    Topic = "string"
  ),
  ElasticsearchSettings = list(
    ServiceAccessRoleArn = "string",
    EndpointUri = "string",
    FullLoadErrorPercentage = 123,
    ErrorRetryDuration = 123
  ),
  NeptuneSettings = list(
    ServiceAccessRoleArn = "string",
    S3BucketName = "string",
    S3BucketFolder = "string",
    ErrorRetryDuration = 123,
    MaxFileSize = 123,
    MaxRetryCount = 123,
    IamAuthEnabled = TRUE|FALSE
  ),
  RedshiftSettings = list(
    AcceptAnyDate = TRUE|FALSE,
    AfterConnectScript = "string",
    BucketFolder = "string",
    BucketName = "string",
    ConnectionTimeout = 123,
    DatabaseName = "string",
    DateFormat = "string",
    EmptyAsNull = TRUE|FALSE,
    EncryptionMode = "sse-s3"|"sse-kms",
    FileTransferUploadStreams = 123,
    LoadTimeout = 123,
    MaxFileSize = 123,
    Password = "string",
    Port = 123,
    RemoveQuotes = TRUE|FALSE,
    ReplaceInvalidChars = "string",
    ReplaceChars = "string",
    ServerName = "string",
    ServiceAccessRoleArn = "string",
    ServerSideEncryptionKmsKeyId = "string",
    TimeFormat = "string",
    TrimBlanks = TRUE|FALSE,
    TruncateColumns = TRUE|FALSE,
    Username = "string",
    WriteBufferSize = 123
  )
)
}
}

\examples{
\dontrun{
# Modifies the specified endpoint.
svc$modify_endpoint(
  CertificateArn = "",
  DatabaseName = "",
  EndpointArn = "",
  EndpointIdentifier = "",
  EndpointType = "source",
  EngineName = "",
  ExtraConnectionAttributes = "",
  Password = "",
  Port = 123L,
  ServerName = "",
  SslMode = "require",
  Username = ""
)
}

}
\keyword{internal}
